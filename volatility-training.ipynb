{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f771c68a",
   "metadata": {
    "_cell_guid": "493ca3fa-128d-428d-8674-40eb9b9057ab",
    "_uuid": "304e8aa6-deb1-4e4b-89cc-9f4e8b96df8c",
    "papermill": {
     "duration": 0.027494,
     "end_time": "2021-09-27T12:09:34.332437",
     "exception": false,
     "start_time": "2021-09-27T12:09:34.304943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Change Log\n",
    "* Version 40: LGBM: stock clust + time clust; NN: stock clust; TabNet: time clust\n",
    "* Version 41: LGBM: time clust; NN: stock clust; TabNet: time clust\n",
    "* Version 42: Clipping min=0\n",
    "* Version 44: Ensembling - Random Forest\n",
    "* Version 45: Ensembling - ElasticNet with correlated stocks\n",
    "* Version 46: FE with enhanced error handling\n",
    "* Version 47: Ensembling - ElasticNet with correlated stocks (not forcing positive weights)\n",
    "* Version 48: Ensembling - Random Forest with correlated stocks\n",
    "* Version 49: Ensembling - ElasticNet with correlated stocks (not forcing positive weights)\n",
    "* Version 50: Testing of new FE dataset with syn testing (only run LGBM)\n",
    "* Version 51: Removing >0.99 correlated features\n",
    "* Version 53:\n",
    "    - Optimized LGBM + TabNet params\n",
    "    - Remove correlated feats for TabNet\n",
    "    - Ensembling - Random Forest with correlated stocks\n",
    "* Version 54: ElasticNet\n",
    "\n",
    "Feature:\n",
    "* Stock / Time clustering - direclty put clustering centroid as feature\n",
    "* Correlation between time series as a feature\n",
    "* Remove highly correlated features\n",
    "* Quarticity https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/267096\n",
    "\n",
    "Model:\n",
    "* RF meta model\n",
    "* Add correlated stock prediction to meta model\n",
    "* Final HP Tuning of all models\n",
    "* AutoEncoder+1dCNN from public\n",
    "* 1D CNN\n",
    "* 2D CNN https://towardsdatascience.com/how-to-encode-time-series-into-images-for-financial-forecasting-using-convolutional-neural-networks-5683eb5c53d9\n",
    "    - GAF for time series to image transformation\n",
    "    - 11 features corresponding to 11 channels\n",
    "* LSTM + Dense layer regression\n",
    "\n",
    "Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce8d9f0f",
   "metadata": {
    "_cell_guid": "0ded3417-46b2-4556-80f5-6df738bd2767",
    "_uuid": "2c83d654-9cb2-4c59-a44b-92ac60d62d60",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:09:34.397590Z",
     "iopub.status.busy": "2021-09-27T12:09:34.396318Z",
     "iopub.status.idle": "2021-09-27T12:09:34.399166Z",
     "shell.execute_reply": "2021-09-27T12:09:34.398683Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.470291Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.040621,
     "end_time": "2021-09-27T12:09:34.399287",
     "exception": false,
     "start_time": "2021-09-27T12:09:34.358666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE = False\n",
    "TEST_MODE = 'test'\n",
    "\n",
    "FE_PATH = '/kaggle/input/volatility-fe-output-version-15'\n",
    "BASE_MODEL_PATH = '/kaggle/input/volatility-model-training-output-version-22' # this is for fixing the pre-trained base models (for submission only)\n",
    "\n",
    "SEED = 1111\n",
    "N_FOLD = 5\n",
    "\n",
    "LGBM_NUM_BOOST = 3000\n",
    "NN_EPOCH = 1000\n",
    "TABNET_EPOCH = 1000\n",
    "# LGBM_NUM_BOOST = 1\n",
    "# NN_EPOCH = 1\n",
    "# TABNET_EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d06c083",
   "metadata": {
    "_cell_guid": "74de9426-a64c-4057-9419-263624f51aee",
    "_uuid": "68e0db8d-1ce4-446c-b4cc-336f1f9a66a8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:09:34.458659Z",
     "iopub.status.busy": "2021-09-27T12:09:34.457995Z",
     "iopub.status.idle": "2021-09-27T12:09:40.802462Z",
     "shell.execute_reply": "2021-09-27T12:09:40.801602Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.47842Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.377318,
     "end_time": "2021-09-27T12:09:40.802601",
     "exception": false,
     "start_time": "2021-09-27T12:09:34.425283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "import random\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# set seed\n",
    "def seed_everything(seed=SEED):\n",
    "    import torch\n",
    "    import random\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    tf.random.set_seed(SEED)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395d8dc6",
   "metadata": {
    "_cell_guid": "d9aaa250-23dd-4fb9-854f-c2e5fc279e9a",
    "_uuid": "e378ebbf-68a5-4da6-ae08-aeafadda4d2e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:09:40.862766Z",
     "iopub.status.busy": "2021-09-27T12:09:40.861943Z",
     "iopub.status.idle": "2021-09-27T12:09:40.864498Z",
     "shell.execute_reply": "2021-09-27T12:09:40.864099Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.503171Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.033498,
     "end_time": "2021-09-27T12:09:40.864603",
     "exception": false,
     "start_time": "2021-09-27T12:09:40.831105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'a':[1,2,3,4,5], 'b':['a','a','b','b','c']})\n",
    "# df.groupby('b')['a'].agg(lambda s:s.max()-s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee5905c5",
   "metadata": {
    "_cell_guid": "fdd5bf99-dd93-40b7-aaf5-4b1cb68ff19b",
    "_uuid": "f4807a6c-5a22-489d-8cfc-757ee9dc4eb2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:09:40.925841Z",
     "iopub.status.busy": "2021-09-27T12:09:40.925064Z",
     "iopub.status.idle": "2021-09-27T12:09:46.509791Z",
     "shell.execute_reply": "2021-09-27T12:09:46.510220Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.516523Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.616795,
     "end_time": "2021-09-27T12:09:46.510376",
     "exception": false,
     "start_time": "2021-09-27T12:09:40.893581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape is (428932, 305)\n",
      "Test data shape is (3, 304)\n"
     ]
    }
   ],
   "source": [
    "# import from FE script\n",
    "if INFERENCE==False:\n",
    "    train = pd.read_feather(os.path.join(FE_PATH, 'train.f'))\n",
    "    test = pd.read_feather(os.path.join(FE_PATH, 'test.f'))\n",
    "    print(f'Train data shape is {train.shape}')\n",
    "    print(f'Test data shape is {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fafd25",
   "metadata": {
    "_cell_guid": "0b6efafb-392a-439b-b002-63f1b36f7b31",
    "_uuid": "2e09fba9-9377-4860-97da-b7bc14f2fd6e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:09:46.581080Z",
     "iopub.status.busy": "2021-09-27T12:09:46.580008Z",
     "iopub.status.idle": "2021-09-27T12:09:46.596911Z",
     "shell.execute_reply": "2021-09-27T12:09:46.596484Z",
     "shell.execute_reply.started": "2021-09-26T10:12:28.698699Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.059398,
     "end_time": "2021-09-27T12:09:46.597043",
     "exception": false,
     "start_time": "2021-09-27T12:09:46.537645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the keys of each dataset which are preserved in the whole notebook\n",
    "train_key_cols = ['stock_id','time_id','row_id','target']\n",
    "test_key_cols = ['stock_id','time_id','row_id']\n",
    "train_keys = train[train_key_cols].reset_index(drop=True)\n",
    "test_keys = test[test_key_cols].reset_index(drop=True)\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0a5128",
   "metadata": {
    "_cell_guid": "04c7cc21-ee22-4059-9d8f-243c753a5ce3",
    "_uuid": "cc3cb1a1-8327-4934-8076-8d852fa83dcb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:09:46.656117Z",
     "iopub.status.busy": "2021-09-27T12:09:46.654927Z",
     "iopub.status.idle": "2021-09-27T12:09:46.658450Z",
     "shell.execute_reply": "2021-09-27T12:09:46.658847Z",
     "shell.execute_reply.started": "2021-09-26T10:12:28.747188Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.034778,
     "end_time": "2021-09-27T12:09:46.658962",
     "exception": false,
     "start_time": "2021-09-27T12:09:46.624184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 305)\n",
      "(428932,)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb74492",
   "metadata": {
    "_cell_guid": "d64fd62e-3886-4ae4-aa5e-c5b4a806ebb6",
    "_uuid": "58a1e556-f4ff-4886-b395-ee6cfb027bc5",
    "papermill": {
     "duration": 0.027278,
     "end_time": "2021-09-27T12:09:46.712857",
     "exception": false,
     "start_time": "2021-09-27T12:09:46.685579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8402180d",
   "metadata": {
    "_cell_guid": "683cce4b-05c8-418e-ada8-538e696b5566",
    "_uuid": "9d9f758d-258a-4a30-b684-241160197024",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:09:46.778505Z",
     "iopub.status.busy": "2021-09-27T12:09:46.777792Z",
     "iopub.status.idle": "2021-09-27T12:42:06.790491Z",
     "shell.execute_reply": "2021-09-27T12:42:06.790069Z",
     "shell.execute_reply.started": "2021-09-26T10:12:28.760595Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1940.050318,
     "end_time": "2021-09-27T12:42:06.790616",
     "exception": false,
     "start_time": "2021-09-27T12:09:46.740298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features considered for LightGBM is 270\n",
      "Training fold 0\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000460666\ttraining's RMSPE: 0.213388\tvalid_1's rmse: 0.000487132\tvalid_1's RMSPE: 0.224663\n",
      "[500]\ttraining's rmse: 0.000435272\ttraining's RMSPE: 0.201625\tvalid_1's rmse: 0.000473884\tvalid_1's RMSPE: 0.218553\n",
      "[750]\ttraining's rmse: 0.000422306\ttraining's RMSPE: 0.195619\tvalid_1's rmse: 0.000470266\tvalid_1's RMSPE: 0.216884\n",
      "[1000]\ttraining's rmse: 0.000413041\ttraining's RMSPE: 0.191327\tvalid_1's rmse: 0.000468468\tvalid_1's RMSPE: 0.216055\n",
      "[1250]\ttraining's rmse: 0.000405777\ttraining's RMSPE: 0.187962\tvalid_1's rmse: 0.000467177\tvalid_1's RMSPE: 0.21546\n",
      "[1500]\ttraining's rmse: 0.000400151\ttraining's RMSPE: 0.185357\tvalid_1's rmse: 0.0004664\tvalid_1's RMSPE: 0.215101\n",
      "[1750]\ttraining's rmse: 0.000395209\ttraining's RMSPE: 0.183067\tvalid_1's rmse: 0.000465934\tvalid_1's RMSPE: 0.214886\n",
      "[2000]\ttraining's rmse: 0.000391\ttraining's RMSPE: 0.181118\tvalid_1's rmse: 0.000465626\tvalid_1's RMSPE: 0.214744\n",
      "Early stopping, best iteration is:\n",
      "[1970]\ttraining's rmse: 0.000391542\ttraining's RMSPE: 0.181369\tvalid_1's rmse: 0.000465569\tvalid_1's RMSPE: 0.214718\n",
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000459864\ttraining's RMSPE: 0.212402\tvalid_1's rmse: 0.000506979\tvalid_1's RMSPE: 0.236521\n",
      "[500]\ttraining's rmse: 0.000435034\ttraining's RMSPE: 0.200933\tvalid_1's rmse: 0.000494186\tvalid_1's RMSPE: 0.230552\n",
      "[750]\ttraining's rmse: 0.000421954\ttraining's RMSPE: 0.194892\tvalid_1's rmse: 0.000490075\tvalid_1's RMSPE: 0.228635\n",
      "[1000]\ttraining's rmse: 0.000412646\ttraining's RMSPE: 0.190593\tvalid_1's rmse: 0.000487816\tvalid_1's RMSPE: 0.227581\n",
      "[1250]\ttraining's rmse: 0.000405692\ttraining's RMSPE: 0.187381\tvalid_1's rmse: 0.000486564\tvalid_1's RMSPE: 0.226997\n",
      "[1500]\ttraining's rmse: 0.000399896\ttraining's RMSPE: 0.184703\tvalid_1's rmse: 0.00048556\tvalid_1's RMSPE: 0.226528\n",
      "Early stopping, best iteration is:\n",
      "[1660]\ttraining's rmse: 0.000396595\ttraining's RMSPE: 0.183179\tvalid_1's rmse: 0.000485091\tvalid_1's RMSPE: 0.22631\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000464141\ttraining's RMSPE: 0.21451\tvalid_1's rmse: 0.000468258\tvalid_1's RMSPE: 0.217921\n",
      "[500]\ttraining's rmse: 0.000438273\ttraining's RMSPE: 0.202555\tvalid_1's rmse: 0.000458271\tvalid_1's RMSPE: 0.213273\n",
      "[750]\ttraining's rmse: 0.000424893\ttraining's RMSPE: 0.196371\tvalid_1's rmse: 0.000456512\tvalid_1's RMSPE: 0.212454\n",
      "[1000]\ttraining's rmse: 0.000415524\ttraining's RMSPE: 0.192042\tvalid_1's rmse: 0.000455647\tvalid_1's RMSPE: 0.212052\n",
      "[1250]\ttraining's rmse: 0.00040843\ttraining's RMSPE: 0.188763\tvalid_1's rmse: 0.000454919\tvalid_1's RMSPE: 0.211713\n",
      "Early stopping, best iteration is:\n",
      "[1300]\ttraining's rmse: 0.000407159\ttraining's RMSPE: 0.188176\tvalid_1's rmse: 0.000454772\tvalid_1's RMSPE: 0.211645\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000464126\ttraining's RMSPE: 0.214761\tvalid_1's rmse: 0.000471335\tvalid_1's RMSPE: 0.218311\n",
      "[500]\ttraining's rmse: 0.000438612\ttraining's RMSPE: 0.202955\tvalid_1's rmse: 0.000461392\tvalid_1's RMSPE: 0.213706\n",
      "[750]\ttraining's rmse: 0.000425057\ttraining's RMSPE: 0.196683\tvalid_1's rmse: 0.000458807\tvalid_1's RMSPE: 0.212509\n",
      "[1000]\ttraining's rmse: 0.000415492\ttraining's RMSPE: 0.192257\tvalid_1's rmse: 0.000457571\tvalid_1's RMSPE: 0.211937\n",
      "[1250]\ttraining's rmse: 0.000408415\ttraining's RMSPE: 0.188982\tvalid_1's rmse: 0.000456643\tvalid_1's RMSPE: 0.211507\n",
      "Early stopping, best iteration is:\n",
      "[1268]\ttraining's rmse: 0.000407984\ttraining's RMSPE: 0.188783\tvalid_1's rmse: 0.000456531\tvalid_1's RMSPE: 0.211455\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000463369\ttraining's RMSPE: 0.215039\tvalid_1's rmse: 0.000470409\tvalid_1's RMSPE: 0.215316\n",
      "[500]\ttraining's rmse: 0.000437789\ttraining's RMSPE: 0.203168\tvalid_1's rmse: 0.000460374\tvalid_1's RMSPE: 0.210723\n",
      "[750]\ttraining's rmse: 0.000424347\ttraining's RMSPE: 0.19693\tvalid_1's rmse: 0.000458261\tvalid_1's RMSPE: 0.209756\n",
      "[1000]\ttraining's rmse: 0.000414959\ttraining's RMSPE: 0.192573\tvalid_1's rmse: 0.000456713\tvalid_1's RMSPE: 0.209047\n",
      "Early stopping, best iteration is:\n",
      "[1191]\ttraining's rmse: 0.000409424\ttraining's RMSPE: 0.190004\tvalid_1's rmse: 0.000455993\tvalid_1's RMSPE: 0.208718\n",
      "LGBM averaged CV score is 0.21466\n",
      "CPU times: user 32min 10s, sys: 5.77 s, total: 32min 15s\n",
      "Wall time: 32min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_features = [col for col in train.columns if col not in [\"time_id\", \"target\", \"row_id\"] and 'ts_ae' not in col]\n",
    "\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': 60,\n",
    "    'max_bin': 250,\n",
    "    'min_data_in_leaf':422,\n",
    "    'learning_rate': 0.014565641020775516,\n",
    "    'subsample': 0.6563801192981948,\n",
    "    'subsample_freq': 1,\n",
    "    'feature_fraction': 0.525513036358404,\n",
    "    'lambda_l1': 8.177995270216595,\n",
    "    'lambda_l2': 3.8822889556906657,\n",
    "    'categorical_column': [lgb_features.index('stock_id'),\n",
    "                           lgb_features.index('clust_wap1_sma50'),\n",
    "                           lgb_features.index('clust_wap1_sms50'),\n",
    "                           lgb_features.index('clust_total_volume_sma60'),\n",
    "                           lgb_features.index('clust_volume_imbalance_sma80'),\n",
    "                           lgb_features.index('stock_clustering_label'),\n",
    "                           lgb_features.index('time_clustering_label')],\n",
    "    'seed': SEED,\n",
    "    'feature_fraction_seed': SEED,\n",
    "    'bagging_seed': SEED,\n",
    "    'drop_seed': SEED,\n",
    "    'data_random_seed': SEED,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):  \n",
    "    print(f'Number of features considered for LightGBM is {len(lgb_features)}')\n",
    "    oof_pred = np.zeros(train.shape[0])\n",
    "    test_pred = []\n",
    "    kfold = GroupKFold(n_splits=N_FOLD)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train, y, train_keys.time_id)):\n",
    "        \n",
    "        if INFERENCE==False:\n",
    "            print(f'Training fold {fold}')\n",
    "            x_train, x_val = train.iloc[trn_idx], train.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "            train_weights = 1 / np.square(y_train)\n",
    "            val_weights = 1 / np.square(y_val)\n",
    "            train_dataset = lgb.Dataset(x_train[lgb_features], y_train, weight=train_weights)\n",
    "            val_dataset = lgb.Dataset(x_val[lgb_features], y_val, weight=val_weights)\n",
    "            model = lgb.train(params=params,\n",
    "                              num_boost_round=LGBM_NUM_BOOST,\n",
    "                              train_set=train_dataset, \n",
    "                              valid_sets=[train_dataset, val_dataset], \n",
    "                              verbose_eval=250,\n",
    "                              early_stopping_rounds=50,\n",
    "                              feval=feval_rmspe)\n",
    "            # predictions\n",
    "            oof_pred[val_idx] = model.predict(x_val[lgb_features])\n",
    "            test_pred.append(model.predict(test[lgb_features]))\n",
    "            # save model\n",
    "            pickle.dump(model, open(f'lgbm_fold{fold}.p', 'wb'))\n",
    "        \n",
    "        elif INFERENCE==True:\n",
    "            print(f'Inferring fold {fold}')\n",
    "            model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'lgbm_fold{fold}.p'), 'rb'))\n",
    "            test_pred.append(model.predict(test[lgb_features]))\n",
    "    # Return test predictions\n",
    "    return oof_pred, test_pred\n",
    "\n",
    "# Traing and evaluate\n",
    "oof_pred_lgb, test_pred_lgb = train_and_evaluate_lgb(train, test, params0)\n",
    "cv_score_lgb = round(rmspe(train['target'], oof_pred_lgb), 5)\n",
    "print(f'LGBM averaged CV score is {cv_score_lgb}')\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce987d8",
   "metadata": {
    "_cell_guid": "2f145e50-2ea6-44a6-9d95-528fccece5c7",
    "_uuid": "30b864d3-db45-41d1-82e5-5dc47d6b36ce",
    "papermill": {
     "duration": 0.043145,
     "end_time": "2021-09-27T12:42:06.872528",
     "exception": false,
     "start_time": "2021-09-27T12:42:06.829383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d08f37ea",
   "metadata": {
    "_cell_guid": "d280b88f-3a26-4458-b32c-c07567873d62",
    "_uuid": "56c2ac51-a211-4ea6-9f97-a20532ba4b58",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:42:06.954163Z",
     "iopub.status.busy": "2021-09-27T12:42:06.953534Z",
     "iopub.status.idle": "2021-09-27T12:42:13.640674Z",
     "shell.execute_reply": "2021-09-27T12:42:13.639824Z",
     "shell.execute_reply.started": "2021-09-26T10:14:17.258156Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.729386,
     "end_time": "2021-09-27T12:42:13.640809",
     "exception": false,
     "start_time": "2021-09-27T12:42:06.911423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if INFERENCE==False:\n",
    "    train_nn = pd.read_feather(os.path.join(FE_PATH, 'train_nn.f'))\n",
    "    test_nn = pd.read_feather(os.path.join(FE_PATH, 'test_nn.f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "214b8aac",
   "metadata": {
    "_cell_guid": "fc7962a4-5eb9-45aa-9a93-2a0a5cb14cee",
    "_uuid": "7dcd69f1-cd17-4f68-a1d5-b0fe939d4902",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:42:13.725035Z",
     "iopub.status.busy": "2021-09-27T12:42:13.724434Z",
     "iopub.status.idle": "2021-09-27T12:42:13.813875Z",
     "shell.execute_reply": "2021-09-27T12:42:13.813472Z",
     "shell.execute_reply.started": "2021-09-26T10:14:18.527002Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.134301,
     "end_time": "2021-09-27T12:42:13.813986",
     "exception": false,
     "start_time": "2021-09-27T12:42:13.679685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9837561",
   "metadata": {
    "_cell_guid": "f0096717-5dd3-4950-90e0-4e1c1366b6b7",
    "_uuid": "32dfbc05-8acd-43bb-b553-4df874483a58",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:42:13.907803Z",
     "iopub.status.busy": "2021-09-27T12:42:13.907279Z",
     "iopub.status.idle": "2021-09-27T12:42:22.117344Z",
     "shell.execute_reply": "2021-09-27T12:42:22.116402Z",
     "shell.execute_reply.started": "2021-09-26T10:14:18.546481Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.262545,
     "end_time": "2021-09-27T12:42:22.117478",
     "exception": false,
     "start_time": "2021-09-27T12:42:13.854933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train[out_train.time_id.isin(train_nn.time_id.unique())]\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "# code to add the just the read data after first execution\n",
    "# data separation based on knn ++\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "nind = int(mat.shape[0] / N_FOLD) # number of individuals\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=N_FOLD, replace=False)\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "for n in range(N_FOLD):\n",
    "    totDist.append(np.zeros(mat.shape[0]-N_FOLD))\n",
    "# saves index\n",
    "for n in range(N_FOLD):\n",
    "    values.append([lineNumber[n]])    \n",
    "s=[]\n",
    "for n in range(N_FOLD):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "for n in range(nind-1):    \n",
    "    luck = np.random.uniform(0,1,N_FOLD)\n",
    "    for cycle in range(N_FOLD):\n",
    "         # saves the values of index           \n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(N_FOLD):\n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "for n_mod in range(N_FOLD):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a116a6d7",
   "metadata": {
    "_cell_guid": "31bf7c71-ad5c-4ab3-ab23-dc9bf16a3a2a",
    "_uuid": "134ab93e-12a5-4561-847b-ae2d25befd9d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:42:22.206324Z",
     "iopub.status.busy": "2021-09-27T12:42:22.205592Z",
     "iopub.status.idle": "2021-09-27T12:42:22.218771Z",
     "shell.execute_reply": "2021-09-27T12:42:22.218379Z",
     "shell.execute_reply.started": "2021-09-26T10:14:27.869839Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.061958,
     "end_time": "2021-09-27T12:42:22.218887",
     "exception": false,
     "start_time": "2021-09-27T12:42:22.156929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NN Model Architecture\n",
    "\n",
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "def create_nn_model(hidden_units):\n",
    "    # initialize input list (later to be concatenated)\n",
    "    raw_input_list = []\n",
    "    concat_input_list = []\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    num_input = keras.Input(shape=(len(numerical_feats),), name='num_data')\n",
    "    raw_input_list.append(num_input)\n",
    "    concat_input_list.append(num_input)\n",
    "\n",
    "    # stock_id embedding\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    stock_id_embedded = keras.layers.Embedding(stock_id_orig_dim, stock_id_emb_dim, input_length=1, name='stock_id_embedding')(stock_id_input)\n",
    "    stock_id_flattened = keras.layers.Flatten()(stock_id_embedded)\n",
    "    raw_input_list.append(stock_id_input)\n",
    "    concat_input_list.append(stock_id_flattened)\n",
    "    \n",
    "    # stock clustering embedding\n",
    "    stock_clustering_label_input = keras.Input(shape=(1,), name='stock_clustering_label')\n",
    "    stock_clustering_label_embedded = keras.layers.Embedding(stock_clustering_label_orig_dim, stock_clustering_label_emb_dim, input_length=1, name='stock_clustering_label_embedding')(stock_clustering_label_input)\n",
    "    stock_clustering_label_flattened = keras.layers.Flatten()(stock_clustering_label_embedded)\n",
    "    raw_input_list.append(stock_clustering_label_input)\n",
    "    concat_input_list.append(stock_clustering_label_flattened)\n",
    "    \n",
    "#     # time clustering embedding\n",
    "#     time_clustering_label_input = keras.Input(shape=(1,), name='time_clustering_label')\n",
    "#     time_clustering_label_embedded = keras.layers.Embedding(time_clustering_label_orig_dim, time_clustering_label_emb_dim, input_length=1, name='time_clustering_label_embedding')(time_clustering_label_input)\n",
    "#     time_clustering_label_flattened = keras.layers.Flatten()(time_clustering_label_embedded)\n",
    "#     raw_input_list.append(time_clustering_label_input)\n",
    "#     concat_input_list.append(time_clustering_label_flattened)\n",
    "    \n",
    "    # concatencate all input layers\n",
    "    x = keras.layers.Concatenate()(concat_input_list)\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "        x = keras.layers.Dense(n_hidden, activation='swish')(x)\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=raw_input_list, outputs=out)\n",
    "    model.compile(keras.optimizers.Adam(learning_rate=0.006), loss=root_mean_squared_per_error)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ef22ec",
   "metadata": {
    "_cell_guid": "32aebff2-14cb-4fdf-afaf-215762de5041",
    "_uuid": "7f376e82-8a3e-4fe6-9dc4-3d40a3798f4d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T12:42:22.299937Z",
     "iopub.status.busy": "2021-09-27T12:42:22.299146Z",
     "iopub.status.idle": "2021-09-27T13:06:34.947849Z",
     "shell.execute_reply": "2021-09-27T13:06:34.948284Z",
     "shell.execute_reply.started": "2021-09-26T10:14:27.8857Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1452.690569,
     "end_time": "2021-09-27T13:06:34.948449",
     "exception": false,
     "start_time": "2021-09-27T12:42:22.257880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 11ms/step - loss: 6.7262 - val_loss: 0.3748\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5221 - val_loss: 0.5334\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4325 - val_loss: 0.2821\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3848 - val_loss: 0.3106\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3419 - val_loss: 0.3107\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3170 - val_loss: 0.2637\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2815 - val_loss: 0.2309\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2238 - val_loss: 0.2368\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2159 - val_loss: 0.2527\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2212 - val_loss: 0.2439\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2233 - val_loss: 0.2372\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2259 - val_loss: 0.2560\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2260 - val_loss: 0.2546\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2224 - val_loss: 0.2442\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2089 - val_loss: 0.2191\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2042 - val_loss: 0.2209\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2191\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.2186\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2047 - val_loss: 0.2195\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2038 - val_loss: 0.2214\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2038 - val_loss: 0.2188\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2036 - val_loss: 0.2257\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2038 - val_loss: 0.2257\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2034 - val_loss: 0.2196\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2044 - val_loss: 0.2187\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.2180\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2177\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2007 - val_loss: 0.2181\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2006 - val_loss: 0.2166\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2006 - val_loss: 0.2172\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2001 - val_loss: 0.2171\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2001 - val_loss: 0.2164\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2171\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1999 - val_loss: 0.2167\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1999 - val_loss: 0.2176\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2172\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2177\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2175\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2169\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2163\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.2168\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2174\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1989 - val_loss: 0.2171\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1989 - val_loss: 0.2178\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1984 - val_loss: 0.2163\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1984 - val_loss: 0.2174\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2173\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2172\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2172\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2169\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1982 - val_loss: 0.2169\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1983 - val_loss: 0.2168\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2171\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2169\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1974 - val_loss: 0.2171\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2170\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2171\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2170\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2170\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2170\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2171\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2170\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1979 - val_loss: 0.2170\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2170\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1982 - val_loss: 0.2170\n",
      "Fold 0: 0.2163\n",
      "Training fold 1...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 9ms/step - loss: 6.4495 - val_loss: 0.4588\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3409 - val_loss: 0.3067\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3054 - val_loss: 0.2198\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2944 - val_loss: 0.2357\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3025 - val_loss: 0.2611\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2819 - val_loss: 0.2315\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2608 - val_loss: 0.6151\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5322 - val_loss: 0.4262\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3502 - val_loss: 0.2168\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2233 - val_loss: 0.2206\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2205 - val_loss: 0.2237\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2263 - val_loss: 0.2195\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2240 - val_loss: 0.2124\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2265 - val_loss: 0.2281\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2306 - val_loss: 0.2475\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2291 - val_loss: 0.2438\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2241 - val_loss: 0.2646\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2292 - val_loss: 0.2531\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2196 - val_loss: 0.3374\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2418 - val_loss: 0.3011\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2176 - val_loss: 0.2090\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2038 - val_loss: 0.2093\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2102\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2012 - val_loss: 0.2091\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2119\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2100\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2017 - val_loss: 0.2127\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.2094\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1994 - val_loss: 0.2090\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2092\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1986 - val_loss: 0.2089\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1991 - val_loss: 0.2098\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2096\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2088\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2091\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1990 - val_loss: 0.2094\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2094\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2096\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1992 - val_loss: 0.2088\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1990 - val_loss: 0.2092\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2096\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.2092\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.2089\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2092\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2094\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1973 - val_loss: 0.2093\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2099\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.2090\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1965 - val_loss: 0.2091\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1966 - val_loss: 0.2093\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2090\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1968 - val_loss: 0.2091\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2091\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.2093\n",
      "Fold 1: 0.20882\n",
      "Training fold 2...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 10ms/step - loss: 5.5068 - val_loss: 0.4158\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.5643 - val_loss: 0.5255\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4515 - val_loss: 0.4000\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3800 - val_loss: 0.4545\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3452 - val_loss: 0.2696\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3086 - val_loss: 0.2830\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2483 - val_loss: 0.2165\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2205 - val_loss: 0.2181\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2239 - val_loss: 0.2311\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2269 - val_loss: 0.2163\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2308 - val_loss: 0.2215\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2326 - val_loss: 0.2178\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2221 - val_loss: 0.2173\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2302 - val_loss: 0.2250\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2353 - val_loss: 0.2120\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2256 - val_loss: 0.2150\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2243 - val_loss: 0.2148\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2251 - val_loss: 0.2242\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2146 - val_loss: 0.2119\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2307 - val_loss: 0.2209\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2344 - val_loss: 0.2158\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2193 - val_loss: 0.2272\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2033 - val_loss: 0.2094\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2082\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2011 - val_loss: 0.2085\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.2087\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2077\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2102\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2012 - val_loss: 0.2086\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2009 - val_loss: 0.2110\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2011 - val_loss: 0.2103\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.2101\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.2099\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2007 - val_loss: 0.2088\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1981 - val_loss: 0.2084\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1979 - val_loss: 0.2094\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2093\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2098\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2088\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1974 - val_loss: 0.2083\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2079\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1970 - val_loss: 0.2080\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1964 - val_loss: 0.2082\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1967 - val_loss: 0.2079\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1962 - val_loss: 0.2084\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1967 - val_loss: 0.2081\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1965 - val_loss: 0.2080\n",
      "Fold 2: 0.2077\n",
      "Training fold 3...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 7.6420 - val_loss: 0.5771\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5869 - val_loss: 0.4564\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4968 - val_loss: 0.3704\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4318 - val_loss: 0.3974\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3744 - val_loss: 0.2296\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2337 - val_loss: 0.2215\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2296 - val_loss: 0.2275\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2880 - val_loss: 0.3490\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3681 - val_loss: 0.3329\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3149 - val_loss: 0.3501\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2569 - val_loss: 0.2175\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2243 - val_loss: 0.2264\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2213 - val_loss: 0.2123\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2159 - val_loss: 0.2107\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2274 - val_loss: 0.2117\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2246 - val_loss: 0.2220\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2171 - val_loss: 0.2109\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2264 - val_loss: 0.2383\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2383 - val_loss: 0.2124\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2294 - val_loss: 0.2162\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2293 - val_loss: 0.2097\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2201 - val_loss: 0.2118\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2222 - val_loss: 0.2171\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2204 - val_loss: 0.2081\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2247 - val_loss: 0.2254\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2205 - val_loss: 0.2259\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2228 - val_loss: 0.2172\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2179 - val_loss: 0.2225\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2219 - val_loss: 0.2133\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2350 - val_loss: 0.2248\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2233 - val_loss: 0.2396\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2030 - val_loss: 0.2098\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2082\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1982 - val_loss: 0.2088\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2062\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1984 - val_loss: 0.2073\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1976 - val_loss: 0.2059\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2077\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2085\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2069\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.2112\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1986 - val_loss: 0.2068\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.2075\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2099\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1949 - val_loss: 0.2078\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1945 - val_loss: 0.2077\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1947 - val_loss: 0.2080\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1947 - val_loss: 0.2096\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1950 - val_loss: 0.2081\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1941 - val_loss: 0.2079\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.1945 - val_loss: 0.2088\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1929 - val_loss: 0.2082\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1938 - val_loss: 0.2082\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1933 - val_loss: 0.2090\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1936 - val_loss: 0.2078\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1934 - val_loss: 0.2085\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1936 - val_loss: 0.2079\n",
      "Fold 3: 0.20593\n",
      "Training fold 4...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 8ms/step - loss: 6.0332 - val_loss: 0.4912\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5079 - val_loss: 0.2435\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2791 - val_loss: 0.2832\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2816 - val_loss: 0.2843\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4575 - val_loss: 0.2797\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3854 - val_loss: 0.3651\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3286 - val_loss: 0.3086\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2888 - val_loss: 0.2488\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2240 - val_loss: 0.2190\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2220 - val_loss: 0.2151\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2299 - val_loss: 0.2282\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2312 - val_loss: 0.2167\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2328 - val_loss: 0.2223\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2217 - val_loss: 0.2271\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2232 - val_loss: 0.2818\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2234 - val_loss: 0.2515\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2426 - val_loss: 0.2090\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2235 - val_loss: 0.2115\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2165 - val_loss: 0.2181\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2166 - val_loss: 0.2118\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2249 - val_loss: 0.2368\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2215 - val_loss: 0.2283\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2259 - val_loss: 0.2366\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2196 - val_loss: 0.2162\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2015 - val_loss: 0.2083\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2077\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2075\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.1992 - val_loss: 0.2090\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1989 - val_loss: 0.2093\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1995 - val_loss: 0.2099\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2106\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2094\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2082\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2077\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1973 - val_loss: 0.2081\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.2082\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1969 - val_loss: 0.2078\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1966 - val_loss: 0.2076\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1962 - val_loss: 0.2087\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1960 - val_loss: 0.2084\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1970 - val_loss: 0.2088\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.1961 - val_loss: 0.2083\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1951 - val_loss: 0.2083\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1960 - val_loss: 0.2084\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1961 - val_loss: 0.2084\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1959 - val_loss: 0.2086\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1963 - val_loss: 0.2084\n",
      "Fold 4: 0.20749\n",
      "Hidden units is  (240, 200, 160, 120, 80, 40, 20)\n",
      "Individual folds score is [0.2163, 0.20882, 0.2077, 0.20593, 0.20749]\n",
      "Training fold 0...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 9.9733 - val_loss: 0.4425\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.5829 - val_loss: 0.4642\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.5230 - val_loss: 0.3194\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4785 - val_loss: 0.3342\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4188 - val_loss: 0.3912\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3793 - val_loss: 0.3797\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3526 - val_loss: 0.2558\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2268 - val_loss: 0.2512\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2218 - val_loss: 0.2362\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2201 - val_loss: 0.2259\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2384 - val_loss: 0.3334\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2501 - val_loss: 0.2576\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2285 - val_loss: 0.2410\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2383 - val_loss: 0.2359\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2310 - val_loss: 0.2298\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2294 - val_loss: 0.2226\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2305 - val_loss: 0.2888\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2315 - val_loss: 0.2282\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2234 - val_loss: 0.2515\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2245 - val_loss: 0.2850\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2396 - val_loss: 0.2220\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2203 - val_loss: 0.2265\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2221 - val_loss: 0.2173\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2264 - val_loss: 0.2260\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2308 - val_loss: 0.2548\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2221 - val_loss: 0.2221\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2137 - val_loss: 0.2252\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2280 - val_loss: 0.2446\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2268 - val_loss: 0.2249\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2228 - val_loss: 0.3085\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2093 - val_loss: 0.2176\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1977 - val_loss: 0.2168\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2156\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2174\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2170\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1967 - val_loss: 0.2163\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1970 - val_loss: 0.2160\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1974 - val_loss: 0.2176\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1966 - val_loss: 0.2175\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1992 - val_loss: 0.2169\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1949 - val_loss: 0.2169\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1945 - val_loss: 0.2175\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1953 - val_loss: 0.2172\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.1951 - val_loss: 0.2185\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1946 - val_loss: 0.2167\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1945 - val_loss: 0.2176\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1958 - val_loss: 0.2183\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1941 - val_loss: 0.2176\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1942 - val_loss: 0.2179\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1941 - val_loss: 0.2178\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1941 - val_loss: 0.2178\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1938 - val_loss: 0.2174\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1935 - val_loss: 0.2179\n",
      "Fold 0: 0.21558\n",
      "Training fold 1...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 7.6187 - val_loss: 0.6464\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5548 - val_loss: 0.3585\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4834 - val_loss: 0.3660\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4206 - val_loss: 0.3744\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3715 - val_loss: 0.2400\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2406 - val_loss: 0.2236\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2315 - val_loss: 0.2439\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2719 - val_loss: 0.2301\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2449 - val_loss: 0.2155\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2373 - val_loss: 0.2243\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2314 - val_loss: 0.2176\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2332 - val_loss: 0.2469\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2346 - val_loss: 0.2136\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2285 - val_loss: 0.7574\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4095 - val_loss: 0.2152\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2200 - val_loss: 0.2320\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2237 - val_loss: 0.2423\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2318 - val_loss: 0.2131\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2149 - val_loss: 0.3441\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2426 - val_loss: 0.2128\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2153 - val_loss: 0.2125\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2188 - val_loss: 0.2592\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2197 - val_loss: 0.2249\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2229 - val_loss: 0.2443\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2225 - val_loss: 0.2243\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2279 - val_loss: 0.2229\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2315 - val_loss: 0.2263\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2228 - val_loss: 0.2119\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2672 - val_loss: 0.2401\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2349 - val_loss: 0.2354\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2295 - val_loss: 0.2528\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2171 - val_loss: 0.2381\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2222 - val_loss: 0.2251\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2210 - val_loss: 0.2261\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2163 - val_loss: 0.2806\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2082 - val_loss: 0.2101\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1964 - val_loss: 0.2106\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1960 - val_loss: 0.2100\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1965 - val_loss: 0.2102\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1967 - val_loss: 0.2107\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1960 - val_loss: 0.2111\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1953 - val_loss: 0.2115\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1959 - val_loss: 0.2109\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1961 - val_loss: 0.2107\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.2108\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1936 - val_loss: 0.2105\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1937 - val_loss: 0.2109\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1931 - val_loss: 0.2108\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1927 - val_loss: 0.2111\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1929 - val_loss: 0.2113\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1930 - val_loss: 0.2111\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1927 - val_loss: 0.2119\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1925 - val_loss: 0.2111\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1925 - val_loss: 0.2113\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1928 - val_loss: 0.2114\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1917 - val_loss: 0.2113\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1924 - val_loss: 0.2117\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1929 - val_loss: 0.2115\n",
      "Fold 1: 0.20999\n",
      "Training fold 2...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 7.8374 - val_loss: 0.2674\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2955 - val_loss: 0.3359\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3005 - val_loss: 0.2251\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2873 - val_loss: 0.2498\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.4157 - val_loss: 0.4467\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4164 - val_loss: 0.4098\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3598 - val_loss: 0.2654\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2938 - val_loss: 0.2252\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2297 - val_loss: 0.2190\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2246 - val_loss: 0.2179\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2301 - val_loss: 0.2932\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2391 - val_loss: 0.2235\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3130 - val_loss: 0.2360\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2279 - val_loss: 0.2229\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2330 - val_loss: 0.2135\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2222 - val_loss: 0.2283\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2220 - val_loss: 0.2123\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2197 - val_loss: 0.2277\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2174 - val_loss: 0.2594\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2194 - val_loss: 0.2309\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2315 - val_loss: 0.2118\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2247 - val_loss: 0.2137\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2225 - val_loss: 0.2121\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2231 - val_loss: 0.2279\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2160 - val_loss: 0.2207\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2254 - val_loss: 0.2143\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2219 - val_loss: 0.2307\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2198 - val_loss: 0.2112\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2232 - val_loss: 0.2371\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2220 - val_loss: 0.2100\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2216 - val_loss: 0.2103\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2169 - val_loss: 0.3892\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2402 - val_loss: 0.2323\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2234 - val_loss: 0.2387\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2176 - val_loss: 0.2167\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2225 - val_loss: 0.2099\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2170 - val_loss: 0.2147\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2187 - val_loss: 0.2445\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2250 - val_loss: 0.2346\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2202 - val_loss: 0.2128\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2159 - val_loss: 0.2380\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 25.4424 - val_loss: 1.4749\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.7944 - val_loss: 0.3745\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3286 - val_loss: 0.3216\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3182 - val_loss: 0.3109\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3086 - val_loss: 0.3061\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3047 - val_loss: 0.3033\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2966 - val_loss: 0.2956\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2909 - val_loss: 0.2934\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2843 - val_loss: 0.2889\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2776 - val_loss: 0.2820\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2771 - val_loss: 0.2801\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2780 - val_loss: 0.2804\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2758 - val_loss: 0.2778\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2735 - val_loss: 0.2773\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2721 - val_loss: 0.2746\n",
      "Fold 2: 0.2099\n",
      "Training fold 3...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 12ms/step - loss: 7.5722 - val_loss: 0.5875\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.4895 - val_loss: 0.4951\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4184 - val_loss: 0.2268\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2797 - val_loss: 0.3368\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3406 - val_loss: 0.2197\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2244 - val_loss: 0.2349\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2288 - val_loss: 0.3208\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3510 - val_loss: 0.3077\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2945 - val_loss: 0.2525\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2659 - val_loss: 0.2917\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2228 - val_loss: 0.2121\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2203 - val_loss: 0.2381\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2187 - val_loss: 0.2137\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2194 - val_loss: 0.2236\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2253 - val_loss: 0.2130\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2221 - val_loss: 0.2092\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2172 - val_loss: 0.2092\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2251 - val_loss: 0.2394\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2363 - val_loss: 0.2124\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2217 - val_loss: 0.2134\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2290 - val_loss: 0.2142\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2259 - val_loss: 0.2414\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.2372\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2065 - val_loss: 0.2090\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2071\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2065\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2071\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2015 - val_loss: 0.2081\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2083\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2006 - val_loss: 0.2071\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2000 - val_loss: 0.2071\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2003 - val_loss: 0.2093\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2008 - val_loss: 0.2152\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2073\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2073\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2069\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1978 - val_loss: 0.2065\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2060\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1979 - val_loss: 0.2070\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2065\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1984 - val_loss: 0.2076\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1979 - val_loss: 0.2068\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2070\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2068\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.2095\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1969 - val_loss: 0.2066\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1969 - val_loss: 0.2069\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1970 - val_loss: 0.2071\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1974 - val_loss: 0.2072\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1967 - val_loss: 0.2070\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1968 - val_loss: 0.2070\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1960 - val_loss: 0.2067\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2067\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1963 - val_loss: 0.2069\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1967 - val_loss: 0.2068\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1965 - val_loss: 0.2067\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1966 - val_loss: 0.2070\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2073\n",
      "Fold 3: 0.20603\n",
      "Training fold 4...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 7.2343 - val_loss: 0.4011\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5397 - val_loss: 0.6238\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.4624 - val_loss: 0.3239\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4081 - val_loss: 0.3602\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3168 - val_loss: 0.2170\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2337 - val_loss: 0.2166\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2370 - val_loss: 0.2580\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2505 - val_loss: 0.2928\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2409 - val_loss: 0.2236\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2362 - val_loss: 0.2496\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2326 - val_loss: 0.2128\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2920 - val_loss: 0.2433\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3139 - val_loss: 0.2202\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2202 - val_loss: 0.2290\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2275 - val_loss: 0.2574\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2228 - val_loss: 0.2245\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2169 - val_loss: 0.2173\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2265 - val_loss: 0.2266\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2069 - val_loss: 0.2080\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2038 - val_loss: 0.2080\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2079\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2085\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2104\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2027 - val_loss: 0.2078\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2021 - val_loss: 0.2079\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2038 - val_loss: 0.2093\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2029 - val_loss: 0.2088\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2024 - val_loss: 0.2110\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2031 - val_loss: 0.2088\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2129\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2154\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2002 - val_loss: 0.2081\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1984 - val_loss: 0.2079\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2076\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1990 - val_loss: 0.2079\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1994 - val_loss: 0.2086\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1989 - val_loss: 0.2079\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2079\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2083\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2079\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1990 - val_loss: 0.2087\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2079\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.2078\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1979 - val_loss: 0.2081\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2081\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2080\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2079\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1974 - val_loss: 0.2096\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2080\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1965 - val_loss: 0.2079\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2080\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1970 - val_loss: 0.2080\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.2081\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1973 - val_loss: 0.2081\n",
      "Fold 4: 0.20762\n",
      "Hidden units is  (220, 180, 140, 100, 50, 25)\n",
      "Individual folds score is [0.21558, 0.20999, 0.2099, 0.20603, 0.20762]\n",
      "Training fold 0...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 10.9663 - val_loss: 0.5255\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4080 - val_loss: 0.2416\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.4591 - val_loss: 0.6748\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6365 - val_loss: 0.5291\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5371 - val_loss: 0.4688\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4615 - val_loss: 0.3971\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4582 - val_loss: 0.4131\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3852 - val_loss: 0.3780\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3355 - val_loss: 0.2424\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2117 - val_loss: 0.2265\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2105 - val_loss: 0.2241\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2097 - val_loss: 0.2236\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2092 - val_loss: 0.2243\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2084 - val_loss: 0.2219\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2081 - val_loss: 0.2221\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2075 - val_loss: 0.2253\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2118 - val_loss: 0.2205\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2076 - val_loss: 0.2210\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2065 - val_loss: 0.2226\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2070 - val_loss: 0.2221\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2072 - val_loss: 0.2278\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2092 - val_loss: 0.2316\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2089 - val_loss: 0.2319\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2108 - val_loss: 0.2245\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2043 - val_loss: 0.2186\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2026 - val_loss: 0.2200\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2022 - val_loss: 0.2192\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2024 - val_loss: 0.2228\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2023 - val_loss: 0.2170\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2023 - val_loss: 0.2179\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2186\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2181\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2022 - val_loss: 0.2188\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2014 - val_loss: 0.2186\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2019 - val_loss: 0.2194\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2007 - val_loss: 0.2185\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2190\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1999 - val_loss: 0.2181\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1993 - val_loss: 0.2185\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2005 - val_loss: 0.2175\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2185\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1996 - val_loss: 0.2183\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2003 - val_loss: 0.2181\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2183\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1995 - val_loss: 0.2178\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2184\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2185\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2183\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1999 - val_loss: 0.2185\n",
      "Fold 0: 0.21697\n",
      "Training fold 1...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 13ms/step - loss: 10.7965 - val_loss: 0.3429\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.4816 - val_loss: 0.6387\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.5912 - val_loss: 0.9483\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.7162 - val_loss: 0.5006\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4857 - val_loss: 0.5309\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.4341 - val_loss: 0.3585\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3724 - val_loss: 0.3136\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3424 - val_loss: 0.3243\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3100 - val_loss: 0.2456\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2968 - val_loss: 0.2543\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2203 - val_loss: 0.2230\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2252 - val_loss: 0.2241\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2226 - val_loss: 0.2169\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2212 - val_loss: 0.2333\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2263 - val_loss: 0.2297\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2258 - val_loss: 0.2329\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2490 - val_loss: 0.2322\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2304 - val_loss: 0.2218\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2159 - val_loss: 0.3527\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2503 - val_loss: 0.2777\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2140 - val_loss: 0.2096\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2059 - val_loss: 0.2101\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2043 - val_loss: 0.2106\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2097\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2041 - val_loss: 0.2136\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2105\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2148\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2039 - val_loss: 0.2102\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2017 - val_loss: 0.2091\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2094\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2007 - val_loss: 0.2089\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2014 - val_loss: 0.2103\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2008 - val_loss: 0.2095\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2008 - val_loss: 0.2090\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2003 - val_loss: 0.2088\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2011 - val_loss: 0.2098\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2008 - val_loss: 0.2094\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2094\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2092\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2010 - val_loss: 0.2091\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2089\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2088\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1996 - val_loss: 0.2087\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2088\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2005 - val_loss: 0.2090\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1993 - val_loss: 0.2091\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1996 - val_loss: 0.2094\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1990 - val_loss: 0.2087\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1986 - val_loss: 0.2092\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1989 - val_loss: 0.2090\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1990 - val_loss: 0.2088\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2089\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1990 - val_loss: 0.2089\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2090\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1993 - val_loss: 0.2089\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.2089\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1989 - val_loss: 0.2089\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1996 - val_loss: 0.2089\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1991 - val_loss: 0.2089\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2089\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1990 - val_loss: 0.2089\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2089\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2089\n",
      "Fold 1: 0.20869\n",
      "Training fold 2...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 9.5592 - val_loss: 0.3413\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5538 - val_loss: 0.4905\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5761 - val_loss: 0.4211\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4230 - val_loss: 0.5105\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4012 - val_loss: 0.3395\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3605 - val_loss: 0.5787\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3903 - val_loss: 0.2190\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2246 - val_loss: 0.2209\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2328 - val_loss: 0.2155\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2233 - val_loss: 0.2523\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2484 - val_loss: 0.2151\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2368 - val_loss: 0.2642\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6441 - val_loss: 0.2219\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2353 - val_loss: 0.2215\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2254 - val_loss: 0.2423\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2187 - val_loss: 0.2141\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2216 - val_loss: 0.2181\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2543 - val_loss: 0.2672\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3096 - val_loss: 0.2399\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2839 - val_loss: 0.2676\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2621 - val_loss: 0.2260\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2262 - val_loss: 0.2141\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2128 - val_loss: 0.2155\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2064 - val_loss: 0.2109\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2050 - val_loss: 0.2103\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2044 - val_loss: 0.2098\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2043 - val_loss: 0.2095\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2038 - val_loss: 0.2115\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2046 - val_loss: 0.2095\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 2s 11ms/step - loss: 0.2045 - val_loss: 0.2105\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2037 - val_loss: 0.2113\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2042 - val_loss: 0.2100\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2114\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2036 - val_loss: 0.2092\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2030 - val_loss: 0.2090\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2030 - val_loss: 0.2096\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2030 - val_loss: 0.2093\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2025 - val_loss: 0.2117\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2162\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2033 - val_loss: 0.2089\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2106\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2092\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2083\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1993 - val_loss: 0.2078\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2081\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2090\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1990 - val_loss: 0.2084\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2086\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1986 - val_loss: 0.2081\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2094\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1986 - val_loss: 0.2088\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2081\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2081\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2083\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2084\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2085\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.2086\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1975 - val_loss: 0.2083\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2083\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1974 - val_loss: 0.2084\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2084\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2084\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2083\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2082\n",
      "Fold 2: 0.20784\n",
      "Training fold 3...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 9.9658 - val_loss: 0.4944\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6623 - val_loss: 0.4713\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5638 - val_loss: 0.3929\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5313 - val_loss: 0.4617\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4694 - val_loss: 0.4237\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.4084 - val_loss: 0.2168\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2320 - val_loss: 0.2358\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3783 - val_loss: 0.4832\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2769 - val_loss: 0.2141\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2280 - val_loss: 0.2219\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2581 - val_loss: 0.2164\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2437 - val_loss: 0.3149\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2400 - val_loss: 0.2164\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2366 - val_loss: 0.2490\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2460 - val_loss: 0.2538\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2674 - val_loss: 0.3512\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2236 - val_loss: 0.2112\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2065 - val_loss: 0.2092\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2055 - val_loss: 0.2097\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2053 - val_loss: 0.2082\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2054 - val_loss: 0.2087\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2046 - val_loss: 0.2090\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2049 - val_loss: 0.2095\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2041 - val_loss: 0.2079\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2044 - val_loss: 0.2143\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2078\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2060 - val_loss: 0.2110\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2045 - val_loss: 0.2078\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2074 - val_loss: 0.2117\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2065 - val_loss: 0.2080\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2048 - val_loss: 0.2126\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2015 - val_loss: 0.2081\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2087\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2089\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2002 - val_loss: 0.2083\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2007 - val_loss: 0.2079\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2071\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2068\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2075\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2072\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2074\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2001 - val_loss: 0.2074\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2070\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2071\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1993 - val_loss: 0.2077\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1984 - val_loss: 0.2072\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2075\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2082\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2073\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2075\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2073\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1974 - val_loss: 0.2070\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.2070\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2073\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2071\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.1979 - val_loss: 0.2071\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1980 - val_loss: 0.2071\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1986 - val_loss: 0.2076\n",
      "Fold 3: 0.20682\n",
      "Training fold 4...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 8.6652 - val_loss: 1.1725\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.9862 - val_loss: 0.2330\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3619 - val_loss: 0.4155\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4264 - val_loss: 0.4145\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3957 - val_loss: 0.3743\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3662 - val_loss: 0.3413\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3375 - val_loss: 0.3696\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3310 - val_loss: 0.3051\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3252 - val_loss: 0.3423\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2345 - val_loss: 0.2136\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2157 - val_loss: 0.2120\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2126 - val_loss: 0.2119\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2119 - val_loss: 0.2110\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2107 - val_loss: 0.2116\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2102 - val_loss: 0.2122\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2093 - val_loss: 0.2112\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2115 - val_loss: 0.2105\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2122 - val_loss: 0.2119\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2082 - val_loss: 0.2092\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2081 - val_loss: 0.2149\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2143 - val_loss: 0.2116\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2094 - val_loss: 0.2098\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2079 - val_loss: 0.2170\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2098 - val_loss: 0.2094\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2093 - val_loss: 0.2110\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2073 - val_loss: 0.2095\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2033 - val_loss: 0.2078\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2025 - val_loss: 0.2083\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2084\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2024 - val_loss: 0.2103\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2025 - val_loss: 0.2076\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2024 - val_loss: 0.2089\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2086\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2079\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2079\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2110\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2102\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2024 - val_loss: 0.2107\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2012 - val_loss: 0.2081\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2003 - val_loss: 0.2075\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2083\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.2079\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2080\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.2077\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2012 - val_loss: 0.2080\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2005 - val_loss: 0.2079\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2078\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2001 - val_loss: 0.2079\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2003 - val_loss: 0.2078\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1995 - val_loss: 0.2077\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2001 - val_loss: 0.2077\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2077\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2078\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2001 - val_loss: 0.2078\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2079\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.2078\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1995 - val_loss: 0.2078\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2003 - val_loss: 0.2078\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1995 - val_loss: 0.2078\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2001 - val_loss: 0.2078\n",
      "Fold 4: 0.20754\n",
      "Hidden units is  (200, 150, 100, 50, 25)\n",
      "Individual folds score is [0.21697, 0.20869, 0.20784, 0.20682, 0.20754]\n",
      "Training fold 0...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 20.4357 - val_loss: 0.7520\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6345 - val_loss: 0.7067\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6104 - val_loss: 0.5077\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5499 - val_loss: 0.5336\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.6041 - val_loss: 0.3910\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.4672 - val_loss: 0.5388\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.4843 - val_loss: 0.4613\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 2s 11ms/step - loss: 0.4206 - val_loss: 0.3817\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2543 - val_loss: 0.2344\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2292 - val_loss: 3.5353\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 2.3442 - val_loss: 0.2306\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2222 - val_loss: 0.3038\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2286 - val_loss: 0.2360\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2170 - val_loss: 0.2235\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2155 - val_loss: 0.2212\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2209 - val_loss: 0.2435\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2244 - val_loss: 0.2677\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2307 - val_loss: 0.2408\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2306 - val_loss: 0.2267\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2253 - val_loss: 0.2281\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2291 - val_loss: 0.2218\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2294 - val_loss: 0.2511\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2068 - val_loss: 0.2192\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2177\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 0.2177\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2022 - val_loss: 0.2211\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2193\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2018 - val_loss: 0.2231\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2018 - val_loss: 0.2167\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2259\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2021 - val_loss: 0.2202\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2020 - val_loss: 0.2171\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2021 - val_loss: 0.2184\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2038 - val_loss: 0.2262\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2052 - val_loss: 0.2198\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.2303\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1996 - val_loss: 0.2175\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2178\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2178\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2180\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.2171\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2187\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1987 - val_loss: 0.2175\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1979 - val_loss: 0.2178\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1974 - val_loss: 0.2168\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1974 - val_loss: 0.2173\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2177\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.2178\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2177\n",
      "Fold 0: 0.21667\n",
      "Training fold 1...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 14.8334 - val_loss: 0.7983\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6566 - val_loss: 0.5182\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5195 - val_loss: 0.2626\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2898 - val_loss: 0.2604\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2821 - val_loss: 0.2216\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.9742 - val_loss: 0.3745\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3989 - val_loss: 0.4267\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3886 - val_loss: 0.3575\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3631 - val_loss: 0.3907\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3450 - val_loss: 0.4035\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3288 - val_loss: 0.3383\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2542 - val_loss: 0.2187\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2265 - val_loss: 0.2233\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2227 - val_loss: 0.6227\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.4289 - val_loss: 0.2976\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.3156 - val_loss: 0.3338\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.3011 - val_loss: 0.2144\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 2s 11ms/step - loss: 0.2283 - val_loss: 0.2148\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2116 - val_loss: 0.2179\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2222 - val_loss: 0.2412\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2250 - val_loss: 0.2126\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2251 - val_loss: 0.2122\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2146 - val_loss: 0.2839\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2364 - val_loss: 0.2109\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2735 - val_loss: 0.2127\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2288 - val_loss: 0.2723\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2198 - val_loss: 0.2230\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2263 - val_loss: 0.2144\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2241 - val_loss: 0.2315\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2234 - val_loss: 0.2134\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2190 - val_loss: 0.2177\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2043 - val_loss: 0.2093\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2008 - val_loss: 0.2096\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.2094\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2005 - val_loss: 0.2097\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2006 - val_loss: 0.2110\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2012 - val_loss: 0.2102\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2007 - val_loss: 0.2102\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.2122\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2094\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2093\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2096\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2094\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1984 - val_loss: 0.2094\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2094\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2096\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1979 - val_loss: 0.2096\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.2095\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1969 - val_loss: 0.2095\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1969 - val_loss: 0.2096\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1974 - val_loss: 0.2094\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2096\n",
      "Fold 1: 0.20925\n",
      "Training fold 2...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 25.5433 - val_loss: 0.8070\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6984 - val_loss: 0.6680\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6763 - val_loss: 0.5754\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6246 - val_loss: 0.2877\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5895 - val_loss: 0.4849\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5400 - val_loss: 0.6679\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.0013 - val_loss: 0.4572\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.3886 - val_loss: 0.2470\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2532 - val_loss: 0.2236\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2538 - val_loss: 0.2370\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2647 - val_loss: 0.2835\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2780 - val_loss: 0.9573\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.0723 - val_loss: 0.2333\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2566 - val_loss: 0.2272\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2337 - val_loss: 0.2217\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2365 - val_loss: 0.2189\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2399 - val_loss: 0.2283\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2367 - val_loss: 0.2155\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2321 - val_loss: 0.3009\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2393 - val_loss: 0.3229\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2604 - val_loss: 0.2275\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2385 - val_loss: 0.2581\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2353 - val_loss: 0.2166\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2389 - val_loss: 0.2296\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2387 - val_loss: 0.2163\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2038 - val_loss: 0.2100\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2095\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2019 - val_loss: 0.2115\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2092\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2037 - val_loss: 0.2099\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2025 - val_loss: 0.2120\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2110\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2015 - val_loss: 0.2141\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2034 - val_loss: 0.2101\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2018 - val_loss: 0.2144\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 0.2188\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2108\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1993 - val_loss: 0.2127\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2102\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1988 - val_loss: 0.2103\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2092\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2098\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1986 - val_loss: 0.2101\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2089\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2093\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1981 - val_loss: 0.2089\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2089\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1974 - val_loss: 0.2093\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1975 - val_loss: 0.2088\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1973 - val_loss: 0.2098\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2091\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1971 - val_loss: 0.2089\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1985 - val_loss: 0.2092\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2090\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2091\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2090\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.2090\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1972 - val_loss: 0.2091\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2091\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1975 - val_loss: 0.2092\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1979 - val_loss: 0.2091\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1976 - val_loss: 0.2091\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2091\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1975 - val_loss: 0.2091\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1971 - val_loss: 0.2091\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2091\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2091\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1970 - val_loss: 0.2091\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1973 - val_loss: 0.2091\n",
      "Fold 2: 0.20883\n",
      "Training fold 3...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 14.8209 - val_loss: 0.6903\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6902 - val_loss: 0.5863\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.7242 - val_loss: 0.4418\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.5634 - val_loss: 0.5173\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5289 - val_loss: 0.4335\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4763 - val_loss: 0.3792\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4346 - val_loss: 0.3618\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4102 - val_loss: 0.3240\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2709 - val_loss: 0.2141\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2263 - val_loss: 0.2418\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2508 - val_loss: 0.2128\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2370 - val_loss: 0.2344\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.2395 - val_loss: 0.2218\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2399 - val_loss: 0.2262\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.4646 - val_loss: 0.2280\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2415 - val_loss: 0.2389\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2349 - val_loss: 0.2184\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2395 - val_loss: 0.2184\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2080 - val_loss: 0.2081\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2054 - val_loss: 0.2082\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2045 - val_loss: 0.2096\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2046 - val_loss: 0.2091\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2043 - val_loss: 0.2091\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2039 - val_loss: 0.2087\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.2097\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2038 - val_loss: 0.2081\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2025 - val_loss: 0.2081\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2018 - val_loss: 0.2073\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2015 - val_loss: 0.2072\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2015 - val_loss: 0.2070\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.2074\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2012 - val_loss: 0.2087\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2011 - val_loss: 0.2093\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.2099\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2014 - val_loss: 0.2088\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2017 - val_loss: 0.2082\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2009 - val_loss: 0.2077\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2075\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2075\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.2070\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2077\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1996 - val_loss: 0.2072\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2072\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2004 - val_loss: 0.2074\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2072\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2072\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2071\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2072\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2003 - val_loss: 0.2075\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2074\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.2076\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1989 - val_loss: 0.2073\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2004 - val_loss: 0.2072\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1996 - val_loss: 0.2071\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2001 - val_loss: 0.2071\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2071\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2000 - val_loss: 0.2072\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2005 - val_loss: 0.2073\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.2073\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2072\n",
      "Fold 3: 0.20697\n",
      "Training fold 4...\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 14.1022 - val_loss: 0.5305\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.6730 - val_loss: 0.6641\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5170 - val_loss: 0.2540\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4788 - val_loss: 0.4581\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.5633 - val_loss: 0.5598\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4954 - val_loss: 0.3789\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.4204 - val_loss: 0.4093\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3256 - val_loss: 0.2205\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 0.2480 - val_loss: 0.3291\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 0.2497 - val_loss: 0.2372\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 0.9516 - val_loss: 0.2252\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2509 - val_loss: 0.2547\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.3041 - val_loss: 0.2525\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2932 - val_loss: 0.2150\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2166 - val_loss: 0.2200\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2164 - val_loss: 0.3221\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3878 - val_loss: 0.2964\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2785 - val_loss: 0.2543\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2271 - val_loss: 0.2226\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2232 - val_loss: 0.2184\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2187 - val_loss: 0.2095\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2136 - val_loss: 0.2212\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2222 - val_loss: 0.2314\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2187 - val_loss: 0.2148\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2463 - val_loss: 0.2217\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2285 - val_loss: 0.2167\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2213 - val_loss: 0.2240\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2228 - val_loss: 0.4654\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2386 - val_loss: 0.2082\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2092\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2028 - val_loss: 0.2079\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2032 - val_loss: 0.2082\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2018 - val_loss: 0.2088\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2022 - val_loss: 0.2085\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 0.2100\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2023 - val_loss: 0.2138\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2042 - val_loss: 0.2083\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2031 - val_loss: 0.2120\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2002 - val_loss: 0.2082\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2070\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2001 - val_loss: 0.2075\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.2071\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.1989 - val_loss: 0.2071\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1997 - val_loss: 0.2072\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2075\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1994 - val_loss: 0.2074\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1998 - val_loss: 0.2072\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1986 - val_loss: 0.2074\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2070\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1978 - val_loss: 0.2072\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2071\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2071\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2070\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.2071\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1983 - val_loss: 0.2072\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1980 - val_loss: 0.2072\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1977 - val_loss: 0.2071\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1985 - val_loss: 0.2072\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1976 - val_loss: 0.2072\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1982 - val_loss: 0.2072\n",
      "Fold 4: 0.20697\n",
      "Hidden units is  (180, 120, 60, 30)\n",
      "Individual folds score is [0.21667, 0.20925, 0.20883, 0.20697, 0.20697]\n",
      "CPU times: user 28min 17s, sys: 3min 7s, total: 31min 24s\n",
      "Wall time: 24min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "hidden_units_list = [(240,200,160,120,80,40,20), (220,180,140,100,50,25), (200,150,100,50,25), (180,120,60,30)]\n",
    "# hidden_units_list = [(4,2), (4,2), (4,2), (4,2)]\n",
    "# hidden_units_list = [(128,64,32)]\n",
    "stock_id_orig_dim, stock_id_emb_dim = 112, 24\n",
    "stock_clustering_label_orig_dim, stock_clustering_label_emb_dim = 6, 4\n",
    "# time_clustering_label_orig_dim, time_clustering_label_emb_dim = 8, 4\n",
    "numerical_feats = [c for c in train_nn if c not in train_key_cols and 'clustering_label' not in c]\n",
    "train_nn_stock_id = train_nn['stock_id']\n",
    "train_nn_stock_clustering_label = train_nn['stock_clustering_label']\n",
    "# train_nn_time_clustering_label = train_nn['time_clustering_label']\n",
    "\n",
    "def train_and_eval_nn():\n",
    "    oof_pred_nn_list = []\n",
    "    test_pred_nn_list = []\n",
    "    for h in range(len(hidden_units_list)):\n",
    "        # initialize predictions and scores\n",
    "        oof_pred_nn = np.zeros(train_nn.shape[0])\n",
    "        fold_scores = []\n",
    "        test_pred_nn = []\n",
    "        for fold in range(N_FOLD):\n",
    "            if INFERENCE==False:\n",
    "                print(f'Training fold {fold}...')\n",
    "                # train-test split\n",
    "                indexes = np.arange(N_FOLD).astype(int)    \n",
    "                indexes = np.delete(indexes, obj=fold, axis=0) \n",
    "                indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "                trn_idx = train_nn[train_nn.time_id.isin(indexes)].index.tolist()\n",
    "                val_idx = train_nn[train_nn.time_id.isin(values[fold])].index.tolist()\n",
    "                X_train = train_nn.iloc[trn_idx,:][numerical_feats + ['stock_id','stock_clustering_label']]\n",
    "                y_train = train_nn.iloc[trn_idx,:]['target']\n",
    "                X_valid = train_nn.iloc[val_idx,:][numerical_feats + ['stock_id','stock_clustering_label']]\n",
    "                y_valid = train_nn.iloc[val_idx,:]['target']\n",
    "\n",
    "                # define numerical and categorical data for train set\n",
    "                X_train_num = X_train[numerical_feats].values\n",
    "                X_train_stock_id = X_train['stock_id']\n",
    "                X_train_stock_clustering_label = X_train['stock_clustering_label']\n",
    "#                 X_train_time_clustering_label = X_train['time_clustering_label']\n",
    "\n",
    "                # define numerical and categorical data for validation set\n",
    "                X_valid_num = X_valid[numerical_feats].values\n",
    "                X_valid_stock_id = X_valid['stock_id']\n",
    "                X_valid_stock_clustering_label = X_valid['stock_clustering_label']\n",
    "#                 X_valid_time_clustering_label = X_valid['time_clustering_label']\n",
    "\n",
    "                # model training\n",
    "                model = create_nn_model(hidden_units=hidden_units_list[h])\n",
    "                model.fit([X_train_num, X_train_stock_id, X_train_stock_clustering_label], \n",
    "                          y_train,               \n",
    "                          batch_size=2048,\n",
    "                          epochs=NN_EPOCH,\n",
    "                          validation_data=([X_valid_num, X_valid_stock_id, X_valid_stock_clustering_label], y_valid),\n",
    "                          callbacks=[es, plateau],\n",
    "                          validation_batch_size=len(y_valid),\n",
    "                          shuffle=True,\n",
    "                          verbose=1)\n",
    "\n",
    "                # validation result\n",
    "                val_pred = model.predict([X_valid_num, X_valid_stock_id, X_valid_stock_clustering_label]).reshape(1,-1)[0]\n",
    "                oof_pred_nn[val_idx] = val_pred\n",
    "                score = round(rmspe(y_valid, val_pred),5)\n",
    "                fold_scores.append(score)\n",
    "                print('Fold {}: {}'.format(fold, score))\n",
    "\n",
    "                # test data prediction\n",
    "                test_pred_nn.append(model.predict([test_nn[numerical_feats].values, test_nn['stock_id'], test_nn['stock_clustering_label']]).reshape(1,-1)[0].clip(0,1e10))\n",
    "                # save model\n",
    "                model.save(f'nn_layer{h}_fold{fold}')\n",
    "            \n",
    "            elif INFERENCE==True:\n",
    "                print(f'Inferring layer {h} fold {fold}')\n",
    "                path = os.path.join(BASE_MODEL_PATH, f'nn_layer{h}_fold{fold}')\n",
    "                model = tf.keras.models.load_model(path, compile=False)\n",
    "                test_pred_nn.append(model.predict([test_nn[numerical_feats].values, test_nn['stock_id'], test_nn['stock_clustering_label']]).reshape(1,-1)[0].clip(0,1e10))\n",
    "                \n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "    \n",
    "        # check OOF data quality\n",
    "        print('Hidden units is ', hidden_units_list[h])\n",
    "        print(f'Individual folds score is', fold_scores)\n",
    "        oof_pred_nn_list.append(oof_pred_nn)\n",
    "        test_pred_nn_list.append(test_pred_nn)\n",
    "    return oof_pred_nn_list, test_pred_nn_list\n",
    "\n",
    "oof_pred_nn_list, test_pred_nn_list = train_and_eval_nn()\n",
    "\n",
    "del train_nn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3505013",
   "metadata": {
    "_cell_guid": "8255d259-ed2c-4e9c-9778-f6c261277017",
    "_uuid": "25fe909f-dd89-4a3f-8689-b19ae8902fe8",
    "papermill": {
     "duration": 5.976403,
     "end_time": "2021-09-27T13:06:46.600386",
     "exception": false,
     "start_time": "2021-09-27T13:06:40.623983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76671b35",
   "metadata": {
    "_cell_guid": "31aa9f72-d402-482f-8803-5e275ea8543b",
    "_uuid": "4b83e558-6f53-43b1-b544-9837e38c73ad",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T13:06:57.789497Z",
     "iopub.status.busy": "2021-09-27T13:06:57.788793Z",
     "iopub.status.idle": "2021-09-27T13:07:07.301238Z",
     "shell.execute_reply": "2021-09-27T13:07:07.300724Z",
     "shell.execute_reply.started": "2021-09-26T10:17:50.814434Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 14.974559,
     "end_time": "2021-09-27T13:07:07.301366",
     "exception": false,
     "start_time": "2021-09-27T13:06:52.326807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if INFERENCE==False:\n",
    "    train_tbn = pd.read_feather(os.path.join(FE_PATH, 'train_tbn.f'))\n",
    "    test_tbn = pd.read_feather(os.path.join(FE_PATH, 'test_tbn.f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae205d71",
   "metadata": {
    "_cell_guid": "8fd0be32-f836-452d-9183-7dbefa5ae972",
    "_uuid": "eaad0963-7a7f-411a-83cc-ecc10d36bd3e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T13:07:18.601044Z",
     "iopub.status.busy": "2021-09-27T13:07:18.600480Z",
     "iopub.status.idle": "2021-09-27T13:07:46.344199Z",
     "shell.execute_reply": "2021-09-27T13:07:46.343584Z",
     "shell.execute_reply.started": "2021-09-26T10:17:52.064645Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 33.270525,
     "end_time": "2021-09-27T13:07:46.344369",
     "exception": false,
     "start_time": "2021-09-27T13:07:13.073844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "2\n",
      "Mon Sep 27 13:07:46 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    32W / 250W |  15425MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "import psutil\n",
    "print(psutil.cpu_count())\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99feae76",
   "metadata": {
    "_cell_guid": "64a974b5-38dd-4c96-970e-f120329fc16d",
    "_uuid": "250fda30-a230-4f79-81b9-083236120f7e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T13:07:58.024541Z",
     "iopub.status.busy": "2021-09-27T13:07:58.023459Z",
     "iopub.status.idle": "2021-09-27T13:07:58.025497Z",
     "shell.execute_reply": "2021-09-27T13:07:58.025941Z",
     "shell.execute_reply.started": "2021-09-26T10:18:18.41307Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.106894,
     "end_time": "2021-09-27T13:07:58.026091",
     "exception": false,
     "start_time": "2021-09-27T13:07:51.919197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f2c4cce",
   "metadata": {
    "_cell_guid": "a5b87ad1-f8ea-48d7-ab75-d710d3fa75fe",
    "_uuid": "4556addf-af36-42b8-aba2-10abdbe6fa86",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T13:08:09.125252Z",
     "iopub.status.busy": "2021-09-27T13:08:09.124703Z",
     "iopub.status.idle": "2021-09-27T13:08:09.137706Z",
     "shell.execute_reply": "2021-09-27T13:08:09.137269Z",
     "shell.execute_reply.started": "2021-09-26T10:18:18.422874Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.661867,
     "end_time": "2021-09-27T13:08:09.137823",
     "exception": false,
     "start_time": "2021-09-27T13:08:03.475956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# identify categorical and numerical columns\n",
    "correlated_features = pickle.load(open(os.path.join('/kaggle/input/volatility-correlated-features', f'correlated_features.p'), 'rb'))\n",
    "cat_cols = ['stock_id','time_clustering_label']\n",
    "num_cols = [c for c in train_tbn if c not in ['stock_id','time_id','row_id','target'] and 'clustering_label' not in c and c not in correlated_features]\n",
    "tabnet_feats = [c for c in train_tbn if c in cat_cols + num_cols]\n",
    "\n",
    "# define categorical features index and dimentions for Tabnet params\n",
    "cat_idxs = [tabnet_feats.index(c) for c in cat_cols]\n",
    "cat_dims = [112, 8]\n",
    "cat_emb_dim = [24, 4]\n",
    "\n",
    "tabnet_params = dict(\n",
    "    cat_idxs = cat_idxs,\n",
    "    cat_dims = cat_dims,\n",
    "    cat_emb_dim = cat_emb_dim,\n",
    "    n_d = 24,\n",
    "    n_a = 24,\n",
    "    n_steps = 1,\n",
    "    gamma = 2.0,\n",
    "    n_independent = 2,\n",
    "    n_shared = 2,\n",
    "    lambda_sparse = 2.166158737727093e-06,\n",
    "    mask_type = \"entmax\",\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    scheduler_params=dict(mode=\"min\", patience=3, min_lr=1e-5, factor=0.5),\n",
    "    seed = SEED,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7713a2fb",
   "metadata": {
    "_cell_guid": "365cd3e9-3870-4031-bad4-52c8c5111c27",
    "_uuid": "c209b915-b6b4-485a-88b2-102cd39ad21c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T13:08:20.298432Z",
     "iopub.status.busy": "2021-09-27T13:08:20.297571Z",
     "iopub.status.idle": "2021-09-27T14:05:03.403722Z",
     "shell.execute_reply": "2021-09-27T14:05:03.404217Z",
     "shell.execute_reply.started": "2021-09-26T10:18:18.437769Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3408.819442,
     "end_time": "2021-09-27T14:05:03.404449",
     "exception": false,
     "start_time": "2021-09-27T13:08:14.585007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0......\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 12.91826| val_0_rmspe: 1.67432 |  0:00:17s\n",
      "epoch 10 | loss: 0.22326 | val_0_rmspe: 0.28279 |  0:03:09s\n",
      "epoch 20 | loss: 0.20608 | val_0_rmspe: 0.21445 |  0:06:05s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_rmspe = 0.213\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at TabNet_fold0.zip\n",
      "TabNet fold 0 RMSPE is 0.213002423997988\n",
      "Training fold 1......\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 10.06206| val_0_rmspe: 0.29637 |  0:00:16s\n",
      "epoch 10 | loss: 0.22492 | val_0_rmspe: 0.23883 |  0:03:09s\n",
      "epoch 20 | loss: 0.21748 | val_0_rmspe: 0.22099 |  0:06:03s\n",
      "epoch 30 | loss: 0.21325 | val_0_rmspe: 0.22037 |  0:08:58s\n",
      "epoch 40 | loss: 0.20847 | val_0_rmspe: 0.2221  |  0:11:52s\n",
      "epoch 50 | loss: 0.2045  | val_0_rmspe: 0.21874 |  0:14:46s\n",
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_rmspe = 0.21755\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at TabNet_fold1.zip\n",
      "TabNet fold 1 RMSPE is 0.21754703325721475\n",
      "Training fold 2......\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 11.75703| val_0_rmspe: 0.31145 |  0:00:16s\n",
      "epoch 10 | loss: 0.23084 | val_0_rmspe: 0.21743 |  0:03:09s\n",
      "epoch 20 | loss: 0.21876 | val_0_rmspe: 0.2192  |  0:06:03s\n",
      "epoch 30 | loss: 0.21084 | val_0_rmspe: 0.21103 |  0:08:57s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_rmspe = 0.20903\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at TabNet_fold2.zip\n",
      "TabNet fold 2 RMSPE is 0.20903485354528237\n",
      "Training fold 3......\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 13.02034| val_0_rmspe: 0.93967 |  0:00:17s\n",
      "epoch 10 | loss: 0.2329  | val_0_rmspe: 0.21634 |  0:03:08s\n",
      "epoch 20 | loss: 0.22234 | val_0_rmspe: 0.21085 |  0:06:03s\n",
      "epoch 30 | loss: 0.2131  | val_0_rmspe: 0.20891 |  0:08:56s\n",
      "epoch 40 | loss: 0.20471 | val_0_rmspe: 0.20729 |  0:11:51s\n",
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 33 and best_val_0_rmspe = 0.20719\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at TabNet_fold3.zip\n",
      "TabNet fold 3 RMSPE is 0.20719410550171327\n",
      "Training fold 4......\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 11.11511| val_0_rmspe: 1.30014 |  0:00:16s\n",
      "epoch 10 | loss: 0.22844 | val_0_rmspe: 0.23234 |  0:03:13s\n",
      "epoch 20 | loss: 0.21515 | val_0_rmspe: 0.20691 |  0:06:07s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_rmspe = 0.20555\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at TabNet_fold4.zip\n",
      "TabNet fold 4 RMSPE is 0.20555234653463272\n",
      "OOF score across folds: 0.21051058226979563\n",
      "CPU times: user 56min 17s, sys: 8.41 s, total: 56min 25s\n",
      "Wall time: 56min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "def zip_directory(folder_path, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, mode='w') as zipf:\n",
    "        len_dir_path = len(folder_path)\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, file_path[len_dir_path:])\n",
    "    return\n",
    "                \n",
    "def train_and_eval_tabnet():\n",
    "    kfold = GroupKFold(n_splits=N_FOLD)\n",
    "    oof_pred_tbn = np.zeros(train_tbn.shape[0])\n",
    "    test_pred_tbn = []\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train_tbn, train_tbn.target, train_tbn.time_id)):\n",
    "        if INFERENCE==False:\n",
    "            print(f'Training fold {fold}......')\n",
    "            X_train, X_val = train_tbn[tabnet_feats].iloc[trn_idx].values, train_tbn[tabnet_feats].iloc[val_idx].values\n",
    "            y_train, y_val = train_tbn.target.iloc[trn_idx].values.reshape(-1,1), train_tbn.target.iloc[val_idx].values.reshape(-1,1)\n",
    "            \n",
    "            model = TabNetRegressor(**tabnet_params)\n",
    "            model.fit(\n",
    "              X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              max_epochs = TABNET_EPOCH,\n",
    "              patience = 10,\n",
    "              batch_size = 1024, \n",
    "              virtual_batch_size = 128,\n",
    "              num_workers = 0,\n",
    "              drop_last = False,\n",
    "              eval_metric=[RMSPE],\n",
    "              loss_fn=RMSPELoss\n",
    "              )\n",
    "            # saving model\n",
    "            saving_path_name = f\"TabNet_fold{fold}\"\n",
    "            saved_filepath = model.save_model(saving_path_name)\n",
    "            # predictions\n",
    "            oof_pred = model.predict(X_val).flatten()\n",
    "            oof_pred_tbn[val_idx] = oof_pred\n",
    "            val_rmspe = rmspe(y_val.flatten(), oof_pred)\n",
    "            print(f'TabNet fold {fold} RMSPE is {val_rmspe}')\n",
    "            test_pred_tbn.append(model.predict(test_tbn[tabnet_feats].values).flatten())\n",
    "\n",
    "        elif INFERENCE==True:\n",
    "            input_path = os.path.join(BASE_MODEL_PATH, f'TabNet_fold{fold}')\n",
    "            output_filename = f'TabNet_fold{fold}.zip'\n",
    "            zip_directory(input_path, output_filename)\n",
    "            model = TabNetRegressor()\n",
    "            model.load_model(output_filename)\n",
    "            test_pred_tbn.append(model.predict(test_tbn[tabnet_feats].values).flatten())\n",
    "\n",
    "    return oof_pred_tbn, test_pred_tbn\n",
    "\n",
    "oof_pred_tbn, test_pred_tbn = train_and_eval_tabnet()\n",
    "print(f'OOF score across folds: {rmspe(y, oof_pred_tbn)}')\n",
    "\n",
    "del train_tbn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c591c037",
   "metadata": {
    "_cell_guid": "73357123-24f1-4527-87e5-506e045bd394",
    "_uuid": "f0e71bad-03d9-454f-a3e8-93a076001849",
    "papermill": {
     "duration": 6.011287,
     "end_time": "2021-09-27T14:05:15.141492",
     "exception": false,
     "start_time": "2021-09-27T14:05:09.130205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d55e8adb",
   "metadata": {
    "_cell_guid": "0b0d751b-2aaf-4190-b993-ed4415e2ed9e",
    "_uuid": "a3032b13-61ff-4cc9-8d3f-cc6073c2064d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:05:26.356361Z",
     "iopub.status.busy": "2021-09-27T14:05:26.354561Z",
     "iopub.status.idle": "2021-09-27T14:05:26.401856Z",
     "shell.execute_reply": "2021-09-27T14:05:26.402658Z",
     "shell.execute_reply.started": "2021-09-26T10:21:39.505034Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.547467,
     "end_time": "2021-09-27T14:05:26.402877",
     "exception": false,
     "start_time": "2021-09-27T14:05:20.855410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of meta test is (3, 9)\n"
     ]
    }
   ],
   "source": [
    "# combining oof set\n",
    "train_meta = pd.DataFrame(np.column_stack([oof_pred_lgb] + [oof for oof in oof_pred_nn_list] + [oof_pred_tbn]),\n",
    "                                columns=['lgb'] + [f'nn_layer{i}' for i in range(len(hidden_units_list))] + ['tabnet'])\n",
    "train_meta = pd.concat([train_keys, train_meta], axis=1).reset_index(drop=True)\n",
    "    \n",
    "# combining test set    \n",
    "test_pred_lgb = pd.DataFrame(np.mean(np.column_stack(test_pred_lgb), axis=1), columns=['lgb'])\n",
    "test_pred_nn = [pd.DataFrame(np.mean(np.column_stack(test_pred_nn_list[i]), axis=1), columns=[f'nn_layer{i}']) for i in range(len(hidden_units_list))]\n",
    "test_pred_tbn = pd.DataFrame(np.mean(np.column_stack(test_pred_tbn), axis=1), columns=['tabnet'])\n",
    "test_meta = pd.concat([test_keys] + [test_pred_lgb] + test_pred_nn + [test_pred_tbn], axis=1).reset_index(drop=True)\n",
    "print(f'Shape of meta test is {test_meta.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcad489a",
   "metadata": {
    "_cell_guid": "f851920c-56eb-4eaf-9362-8767479d76ff",
    "_uuid": "9d7947f7-5488-4108-bf67-9110cf7b2eb3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:05:37.545287Z",
     "iopub.status.busy": "2021-09-27T14:05:37.544591Z",
     "iopub.status.idle": "2021-09-27T14:06:37.103184Z",
     "shell.execute_reply": "2021-09-27T14:06:37.102264Z",
     "shell.execute_reply.started": "2021-09-26T10:21:39.614845Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 65.034836,
     "end_time": "2021-09-27T14:06:37.103342",
     "exception": false,
     "start_time": "2021-09-27T14:05:32.068506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train correlated stocks mean prediciton is (428932, 8)\n",
      "Shape of test correlated stocks mean prediciton is (0, 8)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Getting predictions of correlated stocks\n",
    "'''\n",
    "# correlation based on realized volatility (prediction target)\n",
    "def get_sxt_corr_stock_mapping(data, metric, n_top, log_transform, show_distance):\n",
    "    n_top = min(len(data.stock_id.unique())-1, n_top)\n",
    "    # calculate correlations\n",
    "    if log_transform==False:\n",
    "        corr = pd.pivot_table(data, values=metric, index='time_id', columns='stock_id', aggfunc=np.sum).corr()\n",
    "    elif log_transform==True:\n",
    "        corr = np.log(pd.pivot_table(data, values=metric, index='time_id', columns='stock_id', aggfunc=np.sum)).corr()\n",
    "    # compile mapping table\n",
    "    mapping = []\n",
    "    for stock_id in corr.columns:\n",
    "        df = pd.DataFrame({'nearest_stocks':corr[stock_id].sort_values(ascending=False)[1:n_top+1].index.tolist()})\n",
    "        if show_distance==True:\n",
    "            df['nearest_stocks_corr'] = corr[stock_id].sort_values(ascending=False)[1:n_top+1].tolist()\n",
    "        df['stock_id'] = stock_id\n",
    "        mapping.append(df)\n",
    "    mapping = pd.concat(mapping, axis=0).reset_index(drop=True)\n",
    "    return mapping\n",
    "\n",
    "# generate mapping table\n",
    "target = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n",
    "corr_stock_mapping = get_sxt_corr_stock_mapping(data=target, metric='target', n_top=5, log_transform=True, show_distance=False)\n",
    "\n",
    "# cross join with time_id\n",
    "train_id_list = train_meta[['stock_id','time_id']].drop_duplicates()\n",
    "corr_stock_mapping_train = pd.merge(corr_stock_mapping, train_id_list, how='inner',on='stock_id')[['stock_id','time_id','nearest_stocks']]\n",
    "test_id_list = test_meta[['stock_id','time_id']].drop_duplicates()\n",
    "corr_stock_mapping_test = pd.merge(corr_stock_mapping, test_id_list, how='inner',on='stock_id')[['stock_id','time_id','nearest_stocks']]\n",
    "\n",
    "# calculate mean predictions of correlated stocks\n",
    "corr_stock_pred_train = pd.merge(corr_stock_mapping_train, \n",
    "                                 train_meta.drop(['row_id','target'], axis=1).rename(columns={'stock_id':'nearest_stocks'}), \n",
    "                                 how='inner', \n",
    "                                 on=['nearest_stocks','time_id']).\\\n",
    "                        groupby(['stock_id','time_id'])[['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']].\\\n",
    "                        mean().\\\n",
    "                        reset_index()\n",
    "corr_stock_pred_train.columns = ['stock_id','time_id'] + [f'{c}_corr' for c in ['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']]\n",
    "print(f'Shape of train correlated stocks mean prediciton is {corr_stock_pred_train.shape}')\n",
    "corr_stock_pred_test = pd.merge(corr_stock_mapping_test, \n",
    "                                 test_meta.drop(['row_id'], axis=1).rename(columns={'stock_id':'nearest_stocks'}), \n",
    "                                 how='inner', \n",
    "                                 on=['nearest_stocks','time_id']).\\\n",
    "                        groupby(['stock_id','time_id'])[['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']].\\\n",
    "                        mean().\\\n",
    "                        reset_index()\n",
    "corr_stock_pred_test.columns = ['stock_id','time_id'] + [f'{c}_corr' for c in ['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']]\n",
    "print(f'Shape of test correlated stocks mean prediciton is {corr_stock_pred_test.shape}')\n",
    "\n",
    "# add to prediction table\n",
    "train_meta = pd.merge(train_meta, corr_stock_pred_train, how='left', on=['stock_id','time_id'])\n",
    "test_meta = pd.merge(test_meta, corr_stock_pred_test, how='left', on=['stock_id','time_id'])\n",
    "# fillna (in case)\n",
    "train_meta = train_meta.fillna(train_meta.mean()).fillna(0)\n",
    "test_meta = test_meta.fillna(test_meta.mean()).fillna(0)\n",
    "# release memory\n",
    "del corr_stock_mapping, corr_stock_mapping_train, corr_stock_mapping_test, corr_stock_pred_train, corr_stock_pred_test\n",
    "gc.collect()\n",
    "# save data\n",
    "if INFERENCE==False:\n",
    "    train_meta.to_csv('train_meta.csv', index=False)\n",
    "    test_meta.to_csv('test_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5a906",
   "metadata": {
    "_cell_guid": "0263139b-3e75-4b15-9141-e70156db8431",
    "_uuid": "83f2c5f0-5ab2-4a27-a74f-1cfc08e545b4",
    "papermill": {
     "duration": 6.27485,
     "end_time": "2021-09-27T14:06:48.831263",
     "exception": false,
     "start_time": "2021-09-27T14:06:42.556413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c05108b",
   "metadata": {
    "_cell_guid": "9518e4ae-ff0b-45aa-9aa2-5f2627ac60e7",
    "_uuid": "e9a8ad8d-77f3-4652-b018-74694786f978",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:07:00.021232Z",
     "iopub.status.busy": "2021-09-27T14:07:00.019482Z",
     "iopub.status.idle": "2021-09-27T14:07:00.023896Z",
     "shell.execute_reply": "2021-09-27T14:07:00.023285Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.811284Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.702684,
     "end_time": "2021-09-27T14:07:00.024056",
     "exception": false,
     "start_time": "2021-09-27T14:06:54.321372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 0 ns, total: 13 µs\n",
      "Wall time: 16.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_params = dict(n_estimators = 80,\n",
    "                max_depth = 9,\n",
    "                min_samples_split = 14,\n",
    "                min_samples_leaf = 32,\n",
    "                max_features = 'sqrt',\n",
    "                max_samples = 0.45,\n",
    "                criterion='mse',\n",
    "                random_state=SEED)\n",
    "\n",
    "def train_and_eval_meta_rf():\n",
    "    if INFERENCE==False:\n",
    "        # model training\n",
    "        train_weights = 1 / np.square(train_meta.target)\n",
    "        model = RandomForestRegressor(**rf_params)\n",
    "        model.fit(X=train_meta[[c for c in train_meta if c not in train_key_cols]],\n",
    "                  y=train_meta.target,\n",
    "                  sample_weight=train_weights)\n",
    "        pickle.dump(model, open(f'meta_rf.p', 'wb'))\n",
    "    else:\n",
    "        model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'meta_rf.p'), 'rb'))\n",
    "    # predictions\n",
    "    train_pred_meta = model.predict(train_meta[[c for c in train_meta if c not in train_key_cols]])\n",
    "    test_pred_meta = model.predict(test_meta[[c for c in test_meta if c not in train_key_cols]])\n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = train_and_eval_meta_rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7148df",
   "metadata": {
    "_cell_guid": "80b33042-1937-401d-96c8-1ff267cb698a",
    "_uuid": "98867616-2ef2-4c34-bde4-41cd5f8e9344",
    "papermill": {
     "duration": 5.693019,
     "end_time": "2021-09-27T14:07:11.171872",
     "exception": false,
     "start_time": "2021-09-27T14:07:05.478853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 2: Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7e6c0a6",
   "metadata": {
    "_cell_guid": "bd6ae96d-cdb6-4e25-a32c-324f52950a46",
    "_uuid": "f02ace41-001d-4616-9bab-c00ce4bf7dc2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:07:22.765809Z",
     "iopub.status.busy": "2021-09-27T14:07:22.765071Z",
     "iopub.status.idle": "2021-09-27T14:07:22.768257Z",
     "shell.execute_reply": "2021-09-27T14:07:22.768830Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.823807Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.187428,
     "end_time": "2021-09-27T14:07:22.769010",
     "exception": false,
     "start_time": "2021-09-27T14:07:16.581582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 0 ns, total: 10 µs\n",
      "Wall time: 14.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def train_and_eval_meta_ols():\n",
    "    if INFERENCE==False:\n",
    "        # model training\n",
    "        train_weights = 1 / np.square(train_meta.target)\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model.fit(X=train_meta[[c for c in train_meta if c not in train_key_cols]],\n",
    "                  y=train_meta.target,\n",
    "                  sample_weight=train_weights)\n",
    "        print(f'Fitted coefficients are {model.coef_}')\n",
    "        pickle.dump(model, open(f'meta_ols.p', 'wb'))\n",
    "    else:\n",
    "        model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'meta_ols.p'), 'rb'))\n",
    "    # predictions\n",
    "    train_pred_meta = model.predict(train_meta[[c for c in train_meta if c not in train_key_cols]])\n",
    "    test_pred_meta = model.predict(test_meta[[c for c in test_meta if c not in train_key_cols]])\n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = train_and_eval_meta_ols()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d12e4e",
   "metadata": {
    "_cell_guid": "0585d0a8-bc39-4001-8e83-39be0e15c2d8",
    "_uuid": "ab51e0b6-eef2-476c-a795-9632e4dda311",
    "papermill": {
     "duration": 5.695019,
     "end_time": "2021-09-27T14:07:34.045856",
     "exception": false,
     "start_time": "2021-09-27T14:07:28.350837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 3: Equal Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "035ac2ba",
   "metadata": {
    "_cell_guid": "38c85f1e-ccaa-487a-b326-3bab5ffb91f1",
    "_uuid": "aff9dcf6-0cda-4931-bbfb-4a3352e15020",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:07:45.144436Z",
     "iopub.status.busy": "2021-09-27T14:07:45.143589Z",
     "iopub.status.idle": "2021-09-27T14:07:45.147855Z",
     "shell.execute_reply": "2021-09-27T14:07:45.148545Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.83804Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.701029,
     "end_time": "2021-09-27T14:07:45.148739",
     "exception": false,
     "start_time": "2021-09-27T14:07:39.447710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def equal_weight_meta():\n",
    "    nn_cols = [c for c in test_meta if 'nn' in c and c not in train_key_cols]    \n",
    "    train_pred_meta = (train_meta[nn_cols].mean(axis=1) + train_meta['lgb'] + train_meta['tabnet']) / 3\n",
    "    test_pred_meta = (test_meta[nn_cols].mean(axis=1) + test_meta['lgb'] + test_meta['tabnet']) / 3     \n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = equal_weight_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacca0d0",
   "metadata": {
    "_cell_guid": "e8baba4e-36b6-4ad2-b343-330e2a2652dd",
    "_uuid": "de6671fd-01b6-413a-9c1e-1c557cca6c84",
    "papermill": {
     "duration": 6.195678,
     "end_time": "2021-09-27T14:07:56.883599",
     "exception": false,
     "start_time": "2021-09-27T14:07:50.687921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 4: Weighted by CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59bb4665",
   "metadata": {
    "_cell_guid": "6fa30c31-4c74-454a-b3c6-c7fd91a2b916",
    "_uuid": "e58c71da-a99a-4068-aef5-8b3135a53258",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:08:08.274726Z",
     "iopub.status.busy": "2021-09-27T14:08:08.273869Z",
     "iopub.status.idle": "2021-09-27T14:08:08.276851Z",
     "shell.execute_reply": "2021-09-27T14:08:08.277452Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.849808Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.8604,
     "end_time": "2021-09-27T14:08:08.277637",
     "exception": false,
     "start_time": "2021-09-27T14:08:02.417237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def cv_score_weighting():\n",
    "    if INFERENCE==False:\n",
    "        # initialize cv score tables\n",
    "        base_cv = []\n",
    "        nn_cv = []\n",
    "        # calcualte OOF score of each model\n",
    "        base_cv.append(('lgb', rmspe(train_keys['target'], oof_pred_lgb)))\n",
    "        for i in range(len(hidden_units_list)):\n",
    "            nn_cv.append((f'nn_layer{i}', rmspe(train_keys['target'], oof_pred_nn_list[i])))\n",
    "        base_cv.append(('tabnet', rmspe(train_keys['target'], oof_pred_tbn)))\n",
    "        # transform list to table\n",
    "        base_cv = pd.DataFrame(base_cv, columns=['model','cv'])\n",
    "        nn_cv = pd.DataFrame(nn_cv, columns=['model','cv'])\n",
    "        # define contrast parameter\n",
    "        k_nn = 0.015\n",
    "        k_base = 0.2\n",
    "        # calculate NN weights\n",
    "        nn_cv['imp'] = np.exp((1/k_nn) * 1 / nn_cv['cv'])\n",
    "        nn_cv['weight'] = nn_cv['imp'] / np.sum(nn_cv['imp'])\n",
    "        nn_avg_cv = pd.DataFrame({'model':['nn'], 'cv':[np.sum(np.multiply(nn_cv.cv, nn_cv.weight))]})\n",
    "        # calculate base model weight (including the overall NN)\n",
    "        base_cv = pd.concat([base_cv, nn_avg_cv], axis=0).reset_index(drop=True)\n",
    "        base_cv['imp'] = np.exp((1/k_base) * 1 / base_cv['cv'])\n",
    "        base_cv['weight'] = base_cv['imp'] / np.sum(base_cv['imp'])\n",
    "        # derive final weight for all models\n",
    "        nn_cv['weight'] = nn_cv['weight'] * float(base_cv[base_cv.model=='nn']['weight'])\n",
    "        nn_cv = nn_cv[['model','weight']]\n",
    "        base_cv = base_cv[base_cv.model!='nn'][['model','weight']]\n",
    "        base_cv = pd.concat([base_cv, nn_cv], axis=0).reset_index(drop=True)\n",
    "        model_weights = dict(base_cv.to_records(index=False))\n",
    "        pickle.dump(model_weights, open(f'model_weights.p', 'wb'))\n",
    "    else:\n",
    "        model_weights = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'model_weights.p'), 'rb'))\n",
    "    # make predictions\n",
    "    train_pred_meta = sum([train_meta[m] * model_weights[m] for m in model_weights])\n",
    "    test_pred_meta = sum([test_meta[m] * model_weights[m] for m in model_weights])  \n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = cv_score_weighting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af1252",
   "metadata": {
    "_cell_guid": "02dd9678-dfa5-465f-803f-b95810250ef5",
    "_uuid": "f67fd6ba-503a-419c-9e07-4ceb04af83d4",
    "papermill": {
     "duration": 5.685641,
     "end_time": "2021-09-27T14:08:19.386693",
     "exception": false,
     "start_time": "2021-09-27T14:08:13.701052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 5: ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a41ca02b",
   "metadata": {
    "_cell_guid": "c13c157e-a41b-4081-b5a7-59e47b623288",
    "_uuid": "41dbd929-63e3-4585-849e-cab49d5467c4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:08:30.662540Z",
     "iopub.status.busy": "2021-09-27T14:08:30.661621Z",
     "iopub.status.idle": "2021-09-27T14:10:39.505290Z",
     "shell.execute_reply": "2021-09-27T14:10:39.504551Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.86472Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 134.64837,
     "end_time": "2021-09-27T14:10:39.505458",
     "exception": false,
     "start_time": "2021-09-27T14:08:24.857088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted coefficients are [ 0.06530569  0.09766328  0.22771501  0.08149597  0.19969643  0.32674456\n",
      "  0.15052084  0.21272125 -0.09283636 -0.01153631 -0.19988715 -0.05826142]\n",
      "CPU times: user 2min 2s, sys: 133 ms, total: 2min 2s\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.042867209818244524, tolerance: 0.00020025248550026333\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import ElasticNet\n",
    "def train_and_eval_meta_elasticnet():\n",
    "    if INFERENCE==False:\n",
    "        # model training\n",
    "        train_weights = 1 / np.square(train_meta.target)\n",
    "        model = ElasticNet(fit_intercept=False, alpha=1e-10, l1_ratio=0, positive=False, random_state=SEED, max_iter=10000)\n",
    "        model.fit(X=train_meta[[c for c in train_meta if c not in train_key_cols]],\n",
    "                  y=train_meta.target,\n",
    "                  sample_weight=train_weights)\n",
    "        print(f'Fitted coefficients are {model.coef_}')\n",
    "        pickle.dump(model, open(f'meta_elasticnet.p', 'wb'))\n",
    "    else:\n",
    "        model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'meta_elasticnet.p'), 'rb'))\n",
    "    # predictions\n",
    "    train_pred_meta = model.predict(train_meta[[c for c in train_meta if c not in train_key_cols]])\n",
    "    test_pred_meta = model.predict(test_meta[[c for c in test_meta if c not in train_key_cols]])\n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "test_pred_meta, train_pred_meta = train_and_eval_meta_elasticnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4593966",
   "metadata": {
    "_cell_guid": "a91842d8-65d1-4b7c-b04f-a4441ca33001",
    "_uuid": "c3ef12c3-b75f-48d0-b848-a010405ebb93",
    "papermill": {
     "duration": 6.253715,
     "end_time": "2021-09-27T14:10:51.262496",
     "exception": false,
     "start_time": "2021-09-27T14:10:45.008781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e069b5c",
   "metadata": {
    "_cell_guid": "9ffcd2cf-c09e-41aa-9a4b-299e3c191226",
    "_uuid": "0dfecd2e-4ff4-4745-b1d2-5a265cf62347",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:11:02.385659Z",
     "iopub.status.busy": "2021-09-27T14:11:02.384946Z",
     "iopub.status.idle": "2021-09-27T14:11:02.516229Z",
     "shell.execute_reply": "2021-09-27T14:11:02.516751Z",
     "shell.execute_reply.started": "2021-09-26T10:22:40.804706Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.81465,
     "end_time": "2021-09-27T14:11:02.516923",
     "exception": false,
     "start_time": "2021-09-27T14:10:56.702273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000105263 0.07032062\n"
     ]
    }
   ],
   "source": [
    "# clipping\n",
    "target = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n",
    "min_target, max_target = target.target.min(), target.target.max()\n",
    "# min_target, max_target = 0, target.target.max()\n",
    "print(min_target, max_target)\n",
    "\n",
    "oof_pred_lgb = np.clip(oof_pred_lgb, min_target, max_target)\n",
    "for i in range(len(hidden_units_list)):\n",
    "    oof_pred_nn_list[i] = np.clip(oof_pred_nn_list[i], min_target, max_target)\n",
    "oof_pred_tbn = np.clip(oof_pred_tbn, min_target, max_target)\n",
    "train_pred_meta = np.clip(train_pred_meta, min_target, max_target)\n",
    "test_pred_meta = np.clip(test_pred_meta, min_target, max_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716d5d6",
   "metadata": {
    "_cell_guid": "47fed96f-5951-4170-8749-91cd1e647464",
    "_uuid": "b0a70714-bdd9-4981-baa6-989632d47900",
    "papermill": {
     "duration": 5.669556,
     "end_time": "2021-09-27T14:11:13.579203",
     "exception": false,
     "start_time": "2021-09-27T14:11:07.909647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final score and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d48aabd7",
   "metadata": {
    "_cell_guid": "fa30c33c-51d9-4d33-877d-84fc5ee05884",
    "_uuid": "1c0b11c6-7378-4c86-8bfd-00f7be83dfc4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:11:24.810426Z",
     "iopub.status.busy": "2021-09-27T14:11:24.809269Z",
     "iopub.status.idle": "2021-09-27T14:11:24.833548Z",
     "shell.execute_reply": "2021-09-27T14:11:24.833914Z",
     "shell.execute_reply.started": "2021-09-26T10:22:40.988509Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.727197,
     "end_time": "2021-09-27T14:11:24.834076",
     "exception": false,
     "start_time": "2021-09-27T14:11:19.106879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM CV score is 0.21466\n",
      "NN0 CV score is 0.20927\n",
      "NN1 CV score is 0.20983\n",
      "NN2 CV score is 0.20959\n",
      "NN3 CV score is 0.20975\n",
      "TabNet CV score is 0.21051\n",
      "Meta Model CV score is 0.2069\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "if INFERENCE==False:\n",
    "    # LightGBM\n",
    "    lgb_cv_score = round(rmspe(train_keys['target'], oof_pred_lgb), 5)\n",
    "    print(f'LGBM CV score is {lgb_cv_score}')\n",
    "    # NN\n",
    "    for i in range(len(hidden_units_list)):\n",
    "        cv_score = round(rmspe(train_keys['target'], oof_pred_nn_list[i]), 5)\n",
    "        print(f'NN{i} CV score is {cv_score}')\n",
    "    # TabNet\n",
    "    tbn_cv_score = round(rmspe(train_keys['target'], oof_pred_tbn), 5)\n",
    "    print(f'TabNet CV score is {tbn_cv_score}')\n",
    "    # Ensembled\n",
    "    meta_cv_score = round(rmspe(train_keys['target'], train_pred_meta), 5)\n",
    "    print(f'Meta Model CV score is {meta_cv_score}')\n",
    "\n",
    "\n",
    "# submission\n",
    "test['target'] = test_pred_meta\n",
    "test = test[['row_id', 'target']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89c6f9f8",
   "metadata": {
    "_cell_guid": "28eab07c-06ad-41ca-9306-334112c08115",
    "_uuid": "b99eca85-b138-4988-b8db-0fdbfe204791",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T14:11:36.481995Z",
     "iopub.status.busy": "2021-09-27T14:11:36.481141Z",
     "iopub.status.idle": "2021-09-27T14:11:36.493237Z",
     "shell.execute_reply": "2021-09-27T14:11:36.492784Z",
     "shell.execute_reply.started": "2021-09-26T10:22:41.02358Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.234483,
     "end_time": "2021-09-27T14:11:36.493370",
     "exception": false,
     "start_time": "2021-09-27T14:11:30.258887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001644\n",
       "1   0-32  0.001659\n",
       "2   0-34  0.001659"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if INFERENCE==False:\n",
    "    test.to_csv('submission.csv', index=False)\n",
    "    display(test.head())\n",
    "else:\n",
    "    if TEST_MODE=='test':\n",
    "        orig_test = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/test.csv')\n",
    "        if test['row_id'].tolist()==orig_test['row_id'].tolist():\n",
    "            test.to_csv('submission.csv', index=False)\n",
    "            display(test.head())\n",
    "        else:\n",
    "            assert test['row_id'].tolist()==orig_test['row_id'].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7335.864971,
   "end_time": "2021-09-27T14:11:44.029358",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T12:09:28.164387",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
