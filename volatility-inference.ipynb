{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128aff3c",
   "metadata": {
    "_cell_guid": "298b69e7-8bde-427a-abac-9e8a7aba99af",
    "_uuid": "311bb499-4c39-46f6-80c2-58f1bb239e5d",
    "papermill": {
     "duration": 0.043953,
     "end_time": "2021-09-27T16:17:10.223031",
     "exception": false,
     "start_time": "2021-09-27T16:17:10.179078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Change Log\n",
    "* Version 1: Copied from original notebook\n",
    "* Version 2: train_nn created here; added holdout set\n",
    "* Version 3: Moved the creation of holdout set to training notebook\n",
    "* Version 5: Cleaned for only test processing\n",
    "* Version 6: Added min max scaler for NN dataset\n",
    "* Version 7:\n",
    "    - Prepare TabNet data inside this notebook\n",
    "    - Design synthetic test set for debugging submission scoring error\n",
    "* Version 8: Stock clustering (categorical column)\n",
    "* Version 10: NN label encoding for embedding\n",
    "* Version 12: Time clustering (categorical column)\n",
    "* Version 13: Time clustering - only log transform positive columns\n",
    "* Version 14: Added velocity features\n",
    "* Version 15: Added Time Series AutoEncoding features\n",
    "   \n",
    "\n",
    "### To do list:\n",
    "* Add skew and kurt and add to stock level aggregation\n",
    "* Add velocity\n",
    "* Wavelength and Amplitudes\n",
    "* Time clustering?\n",
    "* TS clustering by AE\n",
    "* Prepare TabNet data inside this notebook\n",
    "* Time features - only log-transform non-negative columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb578ce1",
   "metadata": {
    "_cell_guid": "80e9a498-bdb2-49d5-802c-c36348cabebc",
    "_uuid": "85e16dd4-e4ce-4a04-b320-1ec781a82c97",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:10.320173Z",
     "iopub.status.busy": "2021-09-27T16:17:10.319286Z",
     "iopub.status.idle": "2021-09-27T16:17:10.323036Z",
     "shell.execute_reply": "2021-09-27T16:17:10.322510Z",
     "shell.execute_reply.started": "2021-09-22T13:56:18.398472Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.056549,
     "end_time": "2021-09-27T16:17:10.323160",
     "exception": false,
     "start_time": "2021-09-27T16:17:10.266611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "TEST_MODE = 'test'\n",
    "FE_PATH = '/kaggle/input/volatility-fe-output-version-15'\n",
    "N_STOCK = 112\n",
    "N_SYN_STOCK = 2\n",
    "SEED = 1111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345942c4",
   "metadata": {
    "_cell_guid": "7bfa86c9-cf4d-486b-86aa-7875a0bc9516",
    "_uuid": "304b2530-7c6f-4bfb-8eac-94736d0c45dc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:10.413715Z",
     "iopub.status.busy": "2021-09-27T16:17:10.413039Z",
     "iopub.status.idle": "2021-09-27T16:17:11.310370Z",
     "shell.execute_reply": "2021-09-27T16:17:11.309900Z",
     "shell.execute_reply.started": "2021-09-22T13:56:18.406726Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.944579,
     "end_time": "2021-09-27T16:17:11.310497",
     "exception": false,
     "start_time": "2021-09-27T16:17:10.365918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c997f328",
   "metadata": {
    "_cell_guid": "4f888dfe-8547-4f82-a717-8cb3a42a7d4d",
    "_uuid": "07273c4b-2cac-41a0-9d7d-bb0ea85e9930",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:11.400045Z",
     "iopub.status.busy": "2021-09-27T16:17:11.398760Z",
     "iopub.status.idle": "2021-09-27T16:17:11.401553Z",
     "shell.execute_reply": "2021-09-27T16:17:11.401158Z",
     "shell.execute_reply.started": "2021-09-22T13:56:18.423641Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.048133,
     "end_time": "2021-09-27T16:17:11.401675",
     "exception": false,
     "start_time": "2021-09-27T16:17:11.353542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'a':[1,2,3,4,5], 'b':['a','a','b','b','c']})\n",
    "# df.groupby('b')['a'].agg(lambda s:s.max()-s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34faa45",
   "metadata": {
    "_cell_guid": "5b33764d-06ff-4fa8-9d03-91b0d8b9bd6a",
    "_uuid": "22144ad7-4a1e-4392-aaeb-719d678a5f10",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:11.515536Z",
     "iopub.status.busy": "2021-09-27T16:17:11.504368Z",
     "iopub.status.idle": "2021-09-27T16:17:11.550019Z",
     "shell.execute_reply": "2021-09-27T16:17:11.549607Z",
     "shell.execute_reply.started": "2021-09-22T13:56:18.523681Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.106162,
     "end_time": "2021-09-27T16:17:11.550131",
     "exception": false,
     "start_time": "2021-09-27T16:17:11.443969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# fill inf and nan of  a dataframe\n",
    "def fill_inf_nan(df):\n",
    "    df = df.replace([np.inf, -np.inf], np.nan) # replace Inf with NaN\n",
    "    df = df.fillna(df.mean()) # replace NaN with mean\n",
    "    df = df.fillna(0) # if there are still na, fill with zero\n",
    "    return df\n",
    "\n",
    "# function to load a book/trade train/test single stock file\n",
    "def load_single_stock(stock_id, train_test, book_trade):\n",
    "    path = f'/kaggle/input/optiver-realized-volatility-prediction/{book_trade}_{train_test}.parquet/stock_id={str(stock_id)}'\n",
    "    filename = os.path.join(path, os.listdir(path)[0])\n",
    "    return pd.read_parquet(filename)\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "def calc_wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "def calc_wap3(df):\n",
    "    return (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "def calc_wap4(df):\n",
    "    return (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# function to return range of a series\n",
    "def min_max_range(s):\n",
    "    return s.max() - s.min()\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    # Moving Avereage features\n",
    "    df['log_return1_sma'] = df.groupby('time_id')['log_return1'].apply(lambda s: s.rolling(window=50).mean())\n",
    "    df['log_return1_sms'] = df.groupby(['time_id'])['log_return1'].apply(lambda s: s.rolling(window=50).std())\n",
    "    df['log_return1_sma_diff'] = df.groupby('time_id')['log_return1'].apply(lambda s: s.rolling(window=20).mean()) - df.groupby('time_id')['log_return1'].apply(lambda s: s.rolling(window=50).mean())\n",
    "    df['log_return1_sms_diff'] = df.groupby(['time_id'])['log_return1'].apply(lambda s: s.rolling(window=20).std()) - df.groupby(['time_id'])['log_return1'].apply(lambda s: s.rolling(window=50).std())\n",
    "\n",
    "    # Dict for aggregations; These are the features that only exist in full seconds range (000-599)\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "        'log_return1_sma':['last', min_max_range],\n",
    "        'log_return1_sms':['last', min_max_range],\n",
    "        'log_return1_sma_diff':['last', min_max_range],\n",
    "        'log_return1_sms_diff':['last', min_max_range],\n",
    "    }\n",
    "    # These are the features that will be replicated for last 500, 400, 300, 200, 100 seconds\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'total_volume': ['sum','max'],\n",
    "        'volume_imbalance': ['sum','max']\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    # These are the features to be replicated for 500, 400,...100 seconds filter\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    # Create feautres for Tendency and Energy (?); Total 11 features applied to only full seconds (000-599)\n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        # vol vars\n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all seconds filter\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    stock_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility','trade_log_return_realized_volatility']\n",
    "\n",
    "    time_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200',\n",
    "                'log_return1_sma_last','log_return1_sma_diff_last','log_return1_sms_last','log_return1_sms_diff_last']\n",
    "    \n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[stock_cols].agg(['mean','min','max','std']).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the time id\n",
    "    df_time_id = df.groupby(['time_id'])[time_cols].agg(['mean','min','max','std']).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, mode='train'):\n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        print(f'Processing stock {stock_id}')\n",
    "        # Train\n",
    "        if mode=='train':\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        elif mode=='test':\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        return df_tmp\n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7718165e",
   "metadata": {
    "_cell_guid": "463f1698-d8f1-4435-9512-2f36e915999a",
    "_uuid": "95f6f9e6-36d5-48a8-8b80-07f3f603e123",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:11.643282Z",
     "iopub.status.busy": "2021-09-27T16:17:11.642590Z",
     "iopub.status.idle": "2021-09-27T16:17:20.141690Z",
     "shell.execute_reply": "2021-09-27T16:17:20.141043Z",
     "shell.execute_reply.started": "2021-09-22T13:56:18.604682Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.54886,
     "end_time": "2021-09-27T16:17:20.141986",
     "exception": false,
     "start_time": "2021-09-27T16:17:11.593126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 s, sys: 1.37 s, total: 3.49 s\n",
      "Wall time: 8.49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Create all basic features\n",
    "'''\n",
    "# Create basic features for train data\n",
    "if TRAIN==True:\n",
    "    train, test = read_train_test()\n",
    "    train_stock_ids = train['stock_id'].unique()[:N_STOCK]\n",
    "    train = train.merge(preprocessor(train_stock_ids, mode='train'), on = ['row_id'], how = 'left')\n",
    "    key_cols = ['stock_id','time_id','row_id','target']\n",
    "    feat_cols = [col for col in train.columns if col not in key_cols]\n",
    "    train[feat_cols] = train[feat_cols].fillna(train[feat_cols].mean()).fillna(0)\n",
    "    train = get_time_stock(train)\n",
    "else:\n",
    "    train = pd.read_feather(os.path.join(FE_PATH, 'train.f'))\n",
    "\n",
    "# Create basic features for test data\n",
    "if TEST_MODE=='test':\n",
    "    _ , test = read_train_test()\n",
    "    test_stock_ids = test['stock_id'].unique()[:N_STOCK]\n",
    "    test = test.merge(preprocessor(test_stock_ids, mode='test'), on=['row_id'], how='left')\n",
    "elif TEST_MODE=='syn':\n",
    "    test, _ = read_train_test()\n",
    "    syn_test_stock_ids = train['stock_id'].sort_values().unique()[:N_SYN_STOCK]\n",
    "    test = test.merge(preprocessor(syn_test_stock_ids, mode='train'), on=['row_id'], how='inner') # inner join is needed otherwise will return full 420K rows\n",
    "    syn_test_target = test[['stock_id','time_id','row_id','target']]\n",
    "    syn_test_target.to_csv('syn_test_target.csv', index=False)\n",
    "    test = test.drop('target', axis=1)\n",
    "key_cols = ['stock_id','time_id','row_id','target']\n",
    "feat_cols = [col for col in test.columns if col not in key_cols]\n",
    "test[feat_cols] = test[feat_cols].fillna(test[feat_cols].mean()).fillna(0)\n",
    "test = get_time_stock(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d68cb17",
   "metadata": {
    "_cell_guid": "af4eeea1-0480-4852-b406-1ecbd4d327fe",
    "_uuid": "f5c05d4b-0f4c-4560-8220-aff58dec4bbc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:20.245588Z",
     "iopub.status.busy": "2021-09-27T16:17:20.245050Z",
     "iopub.status.idle": "2021-09-27T16:17:20.311176Z",
     "shell.execute_reply": "2021-09-27T16:17:20.310205Z",
     "shell.execute_reply.started": "2021-09-22T13:57:43.132441Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.124539,
     "end_time": "2021-09-27T16:17:20.311290",
     "exception": false,
     "start_time": "2021-09-27T16:17:20.186751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-27 16:17:20.284963] Added 0 stocks time series.\n",
      "[2021-09-27 16:17:20.288102] Total time series size is 0 MB\n",
      "[2021-09-27 16:17:20.293612] Completed correlation for 0 time_id.\n",
      "CPU times: user 55.1 ms, sys: 4.17 ms, total: 59.2 ms\n",
      "Wall time: 61.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Adding correlated stocks features\n",
    "'''\n",
    "# function to get time series for all stocks for similarity calculation in next step\n",
    "def gen_knn_stock_data(train_test, n_stock, seconds_sample_interval):\n",
    "    # define parameters\n",
    "    stock_list = sorted([int(x.split('=')[1]) for x in os.listdir(f'/kaggle/input/optiver-realized-volatility-prediction/book_{train_test}.parquet')])\n",
    "    stock_list = stock_list[:n_stock]\n",
    "    # setup base table as all combinations of time x seconds\n",
    "    time_id_list = pd.DataFrame({'time_id':sorted(load_single_stock(0, train_test, 'book').time_id.unique().tolist())})\n",
    "    seconds_in_bucket_list = pd.DataFrame({'seconds_in_bucket':range(600)})\n",
    "    base = time_id_list.merge(seconds_in_bucket_list, how='cross')\n",
    "    # loop through stocks to get time series data\n",
    "    ts = []\n",
    "    for stock_id in stock_list:\n",
    "        # load data\n",
    "        book = load_single_stock(stock_id, train_test, 'book')\n",
    "        # fill NA\n",
    "        book = base.merge(book, how='left', on=['time_id','seconds_in_bucket'])\n",
    "        book = book.ffill().bfill()  \n",
    "        # create series\n",
    "        book['wap1'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / ( book['bid_size1'] +  book['ask_size1'])\n",
    "        book['wap1_lr'] = np.log(book['wap1']).diff()\n",
    "        book['wap1_sms50'] = book.groupby('time_id')['wap1_lr'].apply(lambda s: s.rolling(window=50).std())\n",
    "        # filtering\n",
    "        idx = book.index.tolist()[::seconds_sample_interval]\n",
    "        book = book.iloc[idx,:]\n",
    "        book = book[book['wap1_sms50'].isnull()==False].reset_index(drop=True)\n",
    "        # centering\n",
    "        book['wap1_sms50'] = book['wap1_sms50'] - book['time_id'].map(book.groupby('time_id')['wap1_sms50'].median())\n",
    "        # merge with master table\n",
    "        book[stock_id] = book['wap1_sms50']\n",
    "        book = book[['time_id','seconds_in_bucket',stock_id]].set_index(['time_id','seconds_in_bucket'])\n",
    "        ts.append(book)\n",
    "        if stock_list.index(stock_id)%20==0:\n",
    "            print(f'[{datetime.datetime.now()}] Added {stock_list.index(stock_id)} stocks time series.')\n",
    "    ts = pd.concat(ts, axis=1).reset_index().drop('seconds_in_bucket', axis=1)\n",
    "    print(f'[{datetime.datetime.now()}] Total time series size is {int(ts.memory_usage().sum() / 1024**2)} MB')\n",
    "    return ts\n",
    "\n",
    "# function to top N correlated stocks per stock per time_id\n",
    "def gen_corr_stock_mapping(data, n_top, show_distance):\n",
    "    n_top = min(n_top, len(data.columns.tolist()[1:])-1)\n",
    "    time_id_list = data.time_id.unique().tolist() \n",
    "    mapping = []\n",
    "    for time_id in time_id_list:\n",
    "        corr = data[data.time_id==time_id].iloc[:,1:].corr().reset_index().rename(columns={'index':'corr_stock_id'})\n",
    "        for stock_id in corr.columns.tolist()[1:]:\n",
    "            df = pd.DataFrame({'nearest_stocks': corr[corr.corr_stock_id!=stock_id].sort_values(by=stock_id, ascending=False)['corr_stock_id'].tolist()[:n_top]})\n",
    "            if show_distance==True:\n",
    "                df['nearest_stocks_corr'] = corr[corr.corr_stock_id!=stock_id].sort_values(by=stock_id, ascending=False)[stock_id].tolist()[:n_top]\n",
    "            df['stock_id'] = stock_id\n",
    "            df['time_id'] = time_id\n",
    "            mapping.append(df)\n",
    "        if time_id_list.index(time_id)%500==0:\n",
    "            print(f'[{datetime.datetime.now()}] Completed correlation for {time_id_list.index(time_id)} time_id.')\n",
    "    mapping = pd.concat(mapping, axis=0)\n",
    "    mapping = mapping[['stock_id','time_id','nearest_stocks'] + [col for col in mapping.columns if col not in ['stock_id','time_id','nearest_stocks']]]\n",
    "    return mapping\n",
    "\n",
    "# generate new features based on correlated stocks\n",
    "def gen_corr_stock_feats(data, mapping, target_cols):\n",
    "    new_feats = pd.merge(data.rename(columns={'stock_id':'nearest_stocks'})[['nearest_stocks','time_id'] + target_cols],\n",
    "                        mapping, how='inner', on=['nearest_stocks','time_id']).\\\n",
    "                        groupby(['stock_id','time_id'])[target_cols].\\\n",
    "                        mean().\\\n",
    "                        reset_index()\n",
    "    new_feats.columns = ['stock_id','time_id'] + [f'{col}_corr' for col in new_feats.columns if col not in ['stock_id','time_id']]\n",
    "    data = pd.merge(data, new_feats, how='left', on=['stock_id','time_id'])\n",
    "    return data\n",
    "\n",
    "# correlated stock features\n",
    "target_cols = ['log_return1_realized_volatility','log_return1_realized_volatility_300','log_return1_realized_volatility_100',\n",
    "               'log_return1_sma_last','log_return1_sma_diff_last','log_return1_sms_last','log_return1_sms_diff_last',\n",
    "               'total_volume_sum','volume_imbalance_sum','trade_size_sum','price_spread_sum']\n",
    "\n",
    "if TRAIN==True:\n",
    "    corr_mapping_train = gen_corr_stock_mapping(gen_knn_stock_data(train_test='train', n_stock=N_STOCK, seconds_sample_interval=10), n_top=5, show_distance=False)\n",
    "    train = gen_corr_stock_feats(data=train, mapping=corr_mapping_train, target_cols=target_cols)\n",
    "\n",
    "if TEST_MODE=='test':\n",
    "    corr_mapping_test = gen_corr_stock_mapping(gen_knn_stock_data(train_test='test', n_stock=N_STOCK, seconds_sample_interval=10), n_top=5, show_distance=False)\n",
    "    test = gen_corr_stock_feats(data=test, mapping=corr_mapping_test, target_cols=target_cols)\n",
    "elif TEST_MODE=='syn':\n",
    "    corr_mapping_test = gen_corr_stock_mapping(gen_knn_stock_data(train_test='train', n_stock=N_SYN_STOCK, seconds_sample_interval=10), n_top=5, show_distance=False)\n",
    "    test = gen_corr_stock_feats(data=test, mapping=corr_mapping_test, target_cols=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea50a897",
   "metadata": {
    "_cell_guid": "861f8b17-17b1-4005-95fc-7791c315eedb",
    "_uuid": "cc70106b-bbf9-4981-a790-ac17d399f3a6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:20.424690Z",
     "iopub.status.busy": "2021-09-27T16:17:20.409510Z",
     "iopub.status.idle": "2021-09-27T16:17:49.810728Z",
     "shell.execute_reply": "2021-09-27T16:17:49.810189Z",
     "shell.execute_reply.started": "2021-09-22T13:58:33.89658Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 29.454264,
     "end_time": "2021-09-27T16:17:49.810880",
     "exception": false,
     "start_time": "2021-09-27T16:17:20.356616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/tslearn052/tslearn-0.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (0.23.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (1.6.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (1.19.5)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (1.0.1)\r\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (0.29.23)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (0.53.1)\r\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba->tslearn==0.5.2) (0.36.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->tslearn==0.5.2) (49.6.0.post20210108)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->tslearn==0.5.2) (2.1.0)\r\n",
      "Installing collected packages: tslearn\r\n",
      "Successfully installed tslearn-0.5.2\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tslearn/clustering/kmeans.py:17: UserWarning: Scikit-learn <0.24 will be deprecated in a future release of tslearn\n",
      "  \"Scikit-learn <0.24 will be deprecated in a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-27 16:17:49.794238] Completed test data tagging for stock 0\n",
      "CPU times: user 1.59 s, sys: 254 ms, total: 1.84 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Adding features for Time Series shape clustering\n",
    "'''\n",
    "!pip install ../input/tslearn052/tslearn-0.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "from tslearn.clustering import TimeSeriesKMeans, KShape\n",
    "from tslearn.generators import random_walks\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "import math\n",
    "import random\n",
    "\n",
    "def preprocess_single_stock(stock_id, train_test, n_time_id_sample, seconds_sample_interval, remove_first_n_seconds, delete_unused_cols):\n",
    "    # load data\n",
    "    book = load_single_stock(stock_id, train_test, 'book')\n",
    "    # ffill and bfill\n",
    "    time_id_list =  pd.DataFrame({'time_id':book.time_id.unique().tolist()})\n",
    "    seconds_in_bucket_list = pd.DataFrame({'seconds_in_bucket':range(600)})\n",
    "    base = time_id_list.merge(seconds_in_bucket_list, how='cross')\n",
    "    book = base.merge(book, how='left', on=['time_id','seconds_in_bucket'])\n",
    "    book = book.ffill().bfill()\n",
    "    # sampling time_id\n",
    "    if n_time_id_sample >= 0:\n",
    "        time_id_samples = random.sample(book['time_id'].unique().tolist(), n_time_id_sample)\n",
    "        book = book[book.time_id.isin(time_id_samples)]\n",
    "    # create time series\n",
    "    book['wap1'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / ( book['bid_size1'] +  book['ask_size1'])\n",
    "    book['wap1_sma50'] = book.groupby('time_id')['wap1'].apply(lambda s: s.rolling(window=50).mean())\n",
    "    book['wap1_lr'] = np.log(book['wap1']).diff()\n",
    "    book['wap1_sms50'] = book.groupby('time_id')['wap1_lr'].apply(lambda s: s.rolling(window=50).std())\n",
    "    book['total_volume'] = book['ask_size1'] + book['ask_size2'] + book['bid_size1'] + book['bid_size2']\n",
    "    book['total_volume'] = (book['total_volume'] - book['total_volume'].mean()) / book['total_volume'].std()\n",
    "    book['total_volume_sma60'] = book.groupby('time_id')['total_volume'].apply(lambda s: s.rolling(window=60).mean())\n",
    "    book['volume_imbalance'] = (book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = (book['volume_imbalance'] - book['volume_imbalance'].mean()) / book['volume_imbalance'].std()\n",
    "    book['volume_imbalance_sma80'] = book.groupby('time_id')['volume_imbalance'].apply(lambda s: s.rolling(window=80).mean())\n",
    "    # remove first few entries to avoid NA\n",
    "    book = book[book.seconds_in_bucket >= remove_first_n_seconds].reset_index(drop=True)\n",
    "    # seconds interval filtering\n",
    "    idx = book.index.tolist()[::seconds_sample_interval]\n",
    "    book = book.iloc[idx,:]\n",
    "    # select only relevant columns\n",
    "    book['stock_id'] = stock_id\n",
    "    if delete_unused_cols==True:\n",
    "        book = book[['stock_id','time_id','seconds_in_bucket','wap1_sma50','wap1_sms50','total_volume_sma60','volume_imbalance_sma80']]\n",
    "    return book\n",
    "\n",
    "def tagging_cluster_all_stocks(n_stock, train_test, n_time_id_sample, seconds_sample_interval, remove_first_n_seconds, delete_unused_cols):\n",
    "    stock_list = sorted([int(x.split('=')[1]) for x in os.listdir(f'/kaggle/input/optiver-realized-volatility-prediction/book_{train_test}.parquet')])[:n_stock]\n",
    "    all_stocks_tagging = []\n",
    "    for stock_id in stock_list:\n",
    "        ts = preprocess_single_stock(stock_id=stock_id, train_test=train_test, n_time_id_sample=n_time_id_sample, \n",
    "                                     seconds_sample_interval=seconds_sample_interval, remove_first_n_seconds=remove_first_n_seconds, \n",
    "                                     delete_unused_cols=delete_unused_cols)\n",
    "        tagging = ts[['stock_id','time_id']].drop_duplicates()\n",
    "        path = '/kaggle/input/volatility-ts-clustering-models-v1' # define KMeans model path and names\n",
    "        clust_models = ['clust_wap1_sma50','clust_wap1_sms50','clust_total_volume_sma60','clust_volume_imbalance_sma80']\n",
    "        for name in clust_models[:4]: # loop through models to perform tagging\n",
    "            model = TimeSeriesKMeans.from_pickle(os.path.join(path, f'{name}.p'))\n",
    "            tagging[name] = model.predict(dataframe_to_ts(data=ts, series=name.replace('clust_','')))\n",
    "            tagging[name] = tagging[name].astype(int)\n",
    "        all_stocks_tagging.append(tagging)\n",
    "        print(f'[{datetime.datetime.now()}] Completed {train_test} data tagging for stock {stock_id}')\n",
    "    all_stocks_tagging = pd.concat(all_stocks_tagging, axis=0).reset_index(drop=True)\n",
    "    return all_stocks_tagging\n",
    "\n",
    "def dataframe_to_ts(data, series):\n",
    "    ts = [data[(data.stock_id==stock_id) & (data.time_id==time_id) & (data[series].isnull()==False)][series].tolist() for stock_id, time_id in data[['stock_id','time_id']].drop_duplicates().to_records(index=False).tolist()]\n",
    "    ts = to_time_series_dataset(ts)\n",
    "    return ts\n",
    "\n",
    "# create the new features\n",
    "# tagging_train = gen_ts_clustering_feats(train_test='train', n_stock=N_STOCK)\n",
    "if TRAIN==True:\n",
    "    tagging_train = pd.read_csv('/kaggle/input/stock-cluster-tagging-train/tagging_train.csv') # pre-computed for training set\n",
    "    train = pd.merge(train, tagging_train, how='left', on=['stock_id','time_id'])\n",
    "    cols = [col for col in train.columns if col[:6]=='clust_']\n",
    "    train[cols] = train[cols].fillna(0)\n",
    "\n",
    "if TEST_MODE=='test':\n",
    "    tagging_test = tagging_cluster_all_stocks(n_stock=N_STOCK, train_test='test', n_time_id_sample=-1, seconds_sample_interval=10, remove_first_n_seconds=10, delete_unused_cols=True)\n",
    "    test = pd.merge(test, tagging_test, how='left', on=['stock_id','time_id'])\n",
    "elif TEST_MODE=='syn':\n",
    "    tagging_test = tagging_cluster_all_stocks(n_stock=N_SYN_STOCK, train_test='train', n_time_id_sample=-1, seconds_sample_interval=10, remove_first_n_seconds=10, delete_unused_cols=True)\n",
    "    test = pd.merge(test, tagging_test, how='left', on=['stock_id','time_id'])\n",
    "cols = [col for col in test.columns if col[:6]=='clust_']\n",
    "test[cols] = test[cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0baff312",
   "metadata": {
    "_cell_guid": "61793a97-96e3-4aa5-87c0-c020821c2beb",
    "_uuid": "60c33de5-2124-4054-9fee-ab6e6d9eb3d9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:49.933300Z",
     "iopub.status.busy": "2021-09-27T16:17:49.932376Z",
     "iopub.status.idle": "2021-09-27T16:17:50.226608Z",
     "shell.execute_reply": "2021-09-27T16:17:50.226183Z",
     "shell.execute_reply.started": "2021-09-22T13:59:07.360454Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.361679,
     "end_time": "2021-09-27T16:17:50.226742",
     "exception": false,
     "start_time": "2021-09-27T16:17:49.865063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    33\n",
      "2    30\n",
      "5    27\n",
      "1    13\n",
      "0     8\n",
      "4     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Add Stock Clustering as categorical feature\n",
    "'''\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# prepare data for clustering\n",
    "data = pd.pivot_table(train, values='target', index='time_id', columns='stock_id', aggfunc=np.sum)\n",
    "data = data.fillna(data.mean()).T\n",
    "# model fitting\n",
    "model = AgglomerativeClustering(n_clusters=6, linkage='ward')\n",
    "label = model.fit_predict(data)\n",
    "print(pd.Series(label).value_counts())\n",
    "# create stock-label mapping table\n",
    "data['stock_clustering_label'] = label\n",
    "data = data.reset_index().rename(columns={'index':'stock_id'})\n",
    "data = data[['stock_id','stock_clustering_label']]\n",
    "# add label to train / test\n",
    "if TRAIN==True:\n",
    "    train = pd.merge(train, data, how='left', on='stock_id')\n",
    "test = pd.merge(test, data, how='left', on='stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc428e91",
   "metadata": {
    "_cell_guid": "1f50edb8-2f89-4afb-b6bd-1e58648041c1",
    "_uuid": "c7f768b3-ad0b-4fd6-abc9-eba39ba3e3b2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:50.336499Z",
     "iopub.status.busy": "2021-09-27T16:17:50.335526Z",
     "iopub.status.idle": "2021-09-27T16:17:50.485628Z",
     "shell.execute_reply": "2021-09-27T16:17:50.485185Z",
     "shell.execute_reply.started": "2021-09-22T13:59:18.18594Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.211591,
     "end_time": "2021-09-27T16:17:50.485765",
     "exception": false,
     "start_time": "2021-09-27T16:17:50.274174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add Time Clustering as categorical feature\n",
    "'''\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "feature_list = ['wap1_sum','log_return1_realized_volatility','log_return1_realized_volatility_200','log_return1_realized_volatility_400','total_volume_sum','volume_imbalance_sum','trade_size_sum','trade_order_count_sum','trade_log_return_realized_volatility']\n",
    "log_feats = ['log_return1_realized_volatility','log_return1_realized_volatility_200','log_return1_realized_volatility_400','total_volume_sum','trade_size_sum','trade_order_count_sum','trade_log_return_realized_volatility']\n",
    "\n",
    "if TRAIN==True:\n",
    "    # prepare data for clustering\n",
    "    train_time_feature = train.groupby('time_id')[feature_list].mean().reset_index()\n",
    "    train_time_feature[log_feats] = np.log(train_time_feature[log_feats])\n",
    "    train_time_feature = fill_inf_nan(train_time_feature)\n",
    "    # kmeans clustering\n",
    "    kmeans = KMeans(n_clusters=8, random_state=SEED).fit(train_time_feature[feature_list])\n",
    "    # transformation for train\n",
    "    train_time_feature['time_clustering_label'] = kmeans.predict(train_time_feature[feature_list])\n",
    "    train_time_feature = train_time_feature[['time_id','time_clustering_label']]\n",
    "    train = pd.merge(train, train_time_feature, how='left', on='time_id')\n",
    "    # transformation for test\n",
    "    test_time_feature = test.groupby('time_id')[feature_list].mean().reset_index()\n",
    "    test_time_feature[log_feats] = np.log(test_time_feature[log_feats])\n",
    "    test_time_feature = fill_inf_nan(test_time_feature)\n",
    "    test_time_feature['time_clustering_label'] = kmeans.predict(test_time_feature[feature_list])\n",
    "    test_time_feature = test_time_feature[['time_id','time_clustering_label']]\n",
    "    test = pd.merge(test, test_time_feature, how='left', on='time_id')\n",
    "    # save model\n",
    "    pickle.dump(kmeans, open(f'time_clustering_kmeans_model.p', 'wb'))\n",
    "else:\n",
    "    # transformation for test\n",
    "    test_time_feature = test.groupby('time_id')[feature_list].mean().reset_index()\n",
    "    test_time_feature[log_feats] = np.log(test_time_feature[log_feats])\n",
    "    test_time_feature = fill_inf_nan(test_time_feature)\n",
    "    kmeans = pickle.load(open(os.path.join(FE_PATH, f'time_clustering_kmeans_model.p'), 'rb'))\n",
    "    test_time_feature['time_clustering_label'] = kmeans.predict(test_time_feature[feature_list])\n",
    "    test_time_feature = test_time_feature[['time_id','time_clustering_label']]\n",
    "    test = pd.merge(test, test_time_feature, how='left', on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93277f3c",
   "metadata": {
    "_cell_guid": "10d59ef9-c0bf-46ae-b37d-30cc68dd59cc",
    "_uuid": "76e1cfc3-1d45-477b-a613-2cab7cf4aaee",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:17:50.601108Z",
     "iopub.status.busy": "2021-09-27T16:17:50.592328Z",
     "iopub.status.idle": "2021-09-27T16:18:08.694335Z",
     "shell.execute_reply": "2021-09-27T16:18:08.694765Z",
     "shell.execute_reply.started": "2021-09-22T13:59:29.439403Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 18.160338,
     "end_time": "2021-09-27T16:18:08.694918",
     "exception": false,
     "start_time": "2021-09-27T16:17:50.534580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-27 16:17:54.877264] Processed 0 stock_ids\n",
      "[2021-09-27 16:17:54.878171] Completed data preprocessing\n",
      "[2021-09-27 16:17:54.885460] Scaled bid_price1\n",
      "[2021-09-27 16:17:54.888540] Scaled ask_price1\n",
      "[2021-09-27 16:17:54.891484] Scaled bid_price2\n",
      "[2021-09-27 16:17:54.894323] Scaled ask_price2\n",
      "[2021-09-27 16:17:54.897167] Scaled bid_size1\n",
      "[2021-09-27 16:17:54.900038] Scaled ask_size1\n",
      "[2021-09-27 16:17:54.903345] Scaled bid_size2\n",
      "[2021-09-27 16:17:54.906330] Scaled ask_size2\n",
      "[2021-09-27 16:17:54.909233] Scaled price\n",
      "[2021-09-27 16:17:54.912243] Scaled size\n",
      "[2021-09-27 16:17:54.915083] Scaled order_count\n",
      "[2021-09-27 16:17:54.915135] Completed data scaling\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 41.4%\n",
      "[2021-09-27 16:17:54.923469] Completed memory reduction\n",
      "[2021-09-27 16:17:54.923539] Completed data preprocessing\n",
      "Shape of data_test is (1, 55, 8)\n",
      "Shape of test encoding is (1, 34)\n",
      "CPU times: user 9.48 s, sys: 1.74 s, total: 11.2 s\n",
      "Wall time: 18.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11501"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Time-Series LSTM AutoEncoding features\n",
    "'''\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# function to load a book/trade train/test single stock file\n",
    "def load_single_stock(stock_id, train_test, book_trade):\n",
    "    path = f'/kaggle/input/optiver-realized-volatility-prediction/{book_trade}_{train_test}.parquet/stock_id={str(stock_id)}'\n",
    "    filename = os.path.join(path, os.listdir(path)[0])\n",
    "    return pd.read_parquet(filename)\n",
    "\n",
    "def preprocess_single_stock(stock_id, train_test, n_time_id_sample, seconds_sample_interval):\n",
    "    # load data\n",
    "    book = load_single_stock(stock_id, train_test, 'book')\n",
    "    trade = load_single_stock(stock_id, train_test, 'trade')\n",
    "    # ffill and bfill\n",
    "    time_id_list =  pd.DataFrame({'time_id':book.time_id.unique().tolist()})\n",
    "    seconds_in_bucket_list = pd.DataFrame({'seconds_in_bucket':range(600)})\n",
    "    base = time_id_list.merge(seconds_in_bucket_list, how='cross')\n",
    "    book = base.merge(book, how='left', on=['time_id','seconds_in_bucket'])\n",
    "    book = book.ffill().bfill()\n",
    "    trade = base.merge(trade, how='left', on=['time_id','seconds_in_bucket'])\n",
    "    trade[['price']] = trade[['price']].ffill().bfill()\n",
    "    trade[['size','order_count']] = trade[['size','order_count']].fillna(0)\n",
    "    # joining book and trade\n",
    "    df = pd.merge(book, trade, how='inner', on=['time_id','seconds_in_bucket'])\n",
    "    # sampling time_id\n",
    "    if n_time_id_sample >= 0:\n",
    "        time_id_samples = random.sample(df['time_id'].unique().tolist(), n_time_id_sample)\n",
    "        df = df[df.time_id.isin(time_id_samples)]\n",
    "    # smoothing all time series\n",
    "    ts = [c for c in df if c not in ['time_id','seconds_in_bucket']]\n",
    "    for c in ts:\n",
    "        df[c] = df.groupby('time_id')[c].apply(lambda s: s.rolling(window=50).mean())\n",
    "    # seconds interval filtering\n",
    "    idx = df.index.tolist()[::seconds_sample_interval]\n",
    "    df = df.iloc[idx,:]\n",
    "    # remove NA entries due to moving average\n",
    "    df = df[df.isnull().sum(axis=1)==0].reset_index(drop=True)\n",
    "    # select only relevant columns\n",
    "    df['stock_id'] = stock_id\n",
    "    df = df[['stock_id'] + [c for c in df if c not in ['stock_id']]]\n",
    "    df = df.drop('seconds_in_bucket', axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_all_stocks(n_stock, train_test, n_time_id_sample, seconds_sample_interval, reduce_mem):\n",
    "    stock_list = sorted([int(x.split('=')[1]) for x in os.listdir(f'/kaggle/input/optiver-realized-volatility-prediction/book_{train_test}.parquet')])\n",
    "    stock_list = stock_list[:n_stock]\n",
    "    df = []\n",
    "    for stock_id in stock_list:\n",
    "        df.append(preprocess_single_stock(stock_id, train_test, n_time_id_sample, seconds_sample_interval))\n",
    "        if stock_list.index(stock_id) % 10 == 0:\n",
    "            print(f'[{datetime.datetime.now()}] Processed {stock_list.index(stock_id) % 10} stock_ids')\n",
    "    df = pd.concat(df, axis=0).reset_index(drop=True)\n",
    "    print(f'[{datetime.datetime.now()}] Completed data preprocessing')\n",
    "    \n",
    "    # scaling\n",
    "    num_feats = [c for c in df if c not in ['stock_id','time_id']]\n",
    "    scalers = pickle.load(open(os.path.join('/kaggle/input/volatility-time-series-autoencoder-data', 'ts_ae_stdscalers.p'), 'rb'))\n",
    "    for c in num_feats:\n",
    "        df[[c]] = scalers[c].transform(df[[c]])\n",
    "        print(f'[{datetime.datetime.now()}] Scaled {c}')\n",
    "    print(f'[{datetime.datetime.now()}] Completed data scaling')\n",
    "    \n",
    "    # memory reduction\n",
    "    if reduce_mem==True:\n",
    "        df = reduce_mem_usage(df)\n",
    "        print(f'[{datetime.datetime.now()}] Completed memory reduction')\n",
    "    return df\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:5] == 'float':\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# create dataset\n",
    "if TEST_MODE=='test':\n",
    "    data_test = preprocess_all_stocks(n_stock=N_STOCK, train_test='test', n_time_id_sample=-1, seconds_sample_interval=10, reduce_mem=True)\n",
    "elif TEST_MODE=='syn':\n",
    "    data_test = preprocess_all_stocks(n_stock=N_SYN_STOCK, train_test='train', n_time_id_sample=-1, seconds_sample_interval=10, reduce_mem=True)\n",
    "print(f'[{datetime.datetime.now()}] Completed data preprocessing')\n",
    "\n",
    "# remove price\n",
    "# data_train = data_train.drop(['ask_price1','bid_price2','ask_price2'], axis=1)\n",
    "data_test = data_test.drop(['ask_price1','bid_price2','ask_price2'], axis=1)\n",
    "\n",
    "# convert to np array\n",
    "data_test_keys = data_test[['stock_id','time_id']].drop_duplicates()\n",
    "n_split = data_test_keys.shape[0]\n",
    "data_test = np.array(np.split(data_test.iloc[:,2:].values, n_split))\n",
    "print(f'Shape of data_test is {data_test.shape}')\n",
    "\n",
    "# load pre-trained encoder\n",
    "path = os.path.join('/kaggle/input/volatility-ts-encoding-version-5', 'TimeSeries_Encoder')\n",
    "ts_encoder = tf.keras.models.load_model(path)\n",
    "encoding_dim = 32\n",
    "\n",
    "# encoding\n",
    "if TRAIN==True:\n",
    "    data_train_keys = pd.read_csv('/kaggle/input/volatility-ts-encoding-version-5/train_encoded.csv')\n",
    "    print(f'Shape of train encoding is {data_train_keys.shape}')\n",
    "    train = pd.merge(train, data_train_keys, how='left', on=['stock_id','time_id'])\n",
    "    del data_train_keys\n",
    "    gc.collect()\n",
    "data_test_keys[[f'ts_ae{i}' for i in range(encoding_dim)]] = ts_encoder.predict(data_test)\n",
    "print(f'Shape of test encoding is {data_test_keys.shape}')\n",
    "test = pd.merge(test, data_test_keys, how='left', on=['stock_id','time_id'])\n",
    "del data_test, data_test_keys\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b31f17a",
   "metadata": {
    "_cell_guid": "638d110a-ce3c-4fa9-b923-b56396ef59cd",
    "_uuid": "78561eb6-4ad8-4aec-b983-94d9ae650423",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:18:08.817513Z",
     "iopub.status.busy": "2021-09-27T16:18:08.808777Z",
     "iopub.status.idle": "2021-09-27T16:18:08.820300Z",
     "shell.execute_reply": "2021-09-27T16:18:08.820713Z",
     "shell.execute_reply.started": "2021-09-22T13:59:44.893402Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.079352,
     "end_time": "2021-09-27T16:18:08.820853",
     "exception": false,
     "start_time": "2021-09-27T16:18:08.741501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create \"tau\" features\n",
    "if TRAIN==True:\n",
    "    train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "    train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "    train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "    train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "    train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "    cols = [col for col in train.columns if 'tau' in col]\n",
    "    train[cols] = train[cols].replace([np.inf, -np.inf], np.nan)\n",
    "    train[cols] = train[cols].fillna(train[cols].mean())\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n",
    "cols = [col for col in test.columns if 'tau' in col]\n",
    "test[cols] = test[cols].replace([np.inf, -np.inf], np.nan)\n",
    "test[cols] = test[cols].fillna(test[cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a2efdd",
   "metadata": {
    "_cell_guid": "f3e8fce6-8b7b-4ff2-afd9-451b854d66df",
    "_uuid": "72ece7c5-5041-4dac-9938-922e4afd10f8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:18:08.929810Z",
     "iopub.status.busy": "2021-09-27T16:18:08.922161Z",
     "iopub.status.idle": "2021-09-27T16:19:14.998028Z",
     "shell.execute_reply": "2021-09-27T16:19:14.997142Z",
     "shell.execute_reply.started": "2021-09-22T13:59:45.808699Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 66.130728,
     "end_time": "2021-09-27T16:19:14.998168",
     "exception": false,
     "start_time": "2021-09-27T16:18:08.867440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 32, 0: 31, 4: 19, 2: 19, 6: 8, 1: 2, 5: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:43: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stock cluster features\n",
    "For each time_id, statistics for each of 5 clusters (for only that time_id) will be added\n",
    "'''\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "# clustering based on target correlation matrix\n",
    "n_clust = 7\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "corr = train_p.corr()\n",
    "kmeans = KMeans(n_clusters=n_clust, random_state=SEED).fit(corr.values)\n",
    "print(Counter(kmeans.labels_))\n",
    "\n",
    "# output the list of stocks for each cluster\n",
    "stock_groups = []\n",
    "ids = corr.index\n",
    "for n in range(n_clust):\n",
    "    stock_groups.append([(x-1) for x in ((ids+1)*(kmeans.labels_==n)) if x > 0])\n",
    "\n",
    "def gen_clust_feats(stock_groups, train, test):\n",
    "    df_train = []\n",
    "    df_test = []\n",
    "    for i in range(len(stock_groups)):\n",
    "        group = stock_groups[i]\n",
    "        newDf = train.loc[train['stock_id'].isin(group)]\n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = f'c{i}'\n",
    "        df_train.append(newDf)\n",
    "\n",
    "        newDf = test.loc[test['stock_id'].isin(group)]    \n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = f'c{i}'\n",
    "        df_test.append(newDf)\n",
    "\n",
    "    df_train = pd.concat(df_train).reset_index()\n",
    "    df_train = df_train[[c for c in df_train if c!='target']]\n",
    "    df_test = pd.concat(df_test).reset_index()\n",
    "\n",
    "    df_test = pd.concat([df_test, df_train[(~df_train.stock_id.isin(df_test.stock_id)) & (df_train.time_id==5)]]) # suspicious; probably this is to ensure that test data contains all stocks; it works on the assumption that time_id 5 is not required for submission\n",
    "    df_train = df_train.pivot(index='time_id', columns='stock_id')\n",
    "    df_train.columns = [\"_\".join(x) for x in df_train.columns.ravel()]\n",
    "    df_train.reset_index(inplace=True)\n",
    "\n",
    "    df_test = df_test.pivot(index='time_id', columns='stock_id')\n",
    "    df_test.columns = [\"_\".join(x) for x in df_test.columns.ravel()]\n",
    "    df_test.reset_index(inplace=True)\n",
    "\n",
    "    # add to base features\n",
    "    target_cols = ['time_id'] + [f'{col}_c{i}' for i in [0,2,3,4,6] for col in ['log_return1_realized_volatility','total_volume_sum','trade_size_sum','trade_order_count_sum',\n",
    "                                                                                'price_spread_sum','bid_spread_sum','ask_spread_sum','volume_imbalance_sum','bid_ask_spread_sum','size_tau2']]\n",
    "    train = pd.merge(train, df_train[target_cols], how='left', on='time_id')\n",
    "    test = pd.merge(test, df_test[target_cols], how='left', on='time_id')\n",
    "\n",
    "    # free memory\n",
    "    import gc\n",
    "    del df_train, df_test\n",
    "    gc.collect()\n",
    "    return train, test\n",
    "\n",
    "if TRAIN==True:\n",
    "    train, test = gen_clust_feats(stock_groups=stock_groups, train=train[[c for c in train if c[-3:-1]!='_c']], test=test)\n",
    "else:\n",
    "    _ , test = gen_clust_feats(stock_groups=stock_groups, train=train, test=test)\n",
    "    \n",
    "train = train.fillna(train.mean()).fillna(0)\n",
    "test = test.fillna(test.mean()).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2103c67",
   "metadata": {
    "_cell_guid": "396bc4f6-f5ff-4f5e-8ddf-4c2d70365d1f",
    "_uuid": "d73bfbd2-af41-455a-bfb8-eb72c4f1e7a9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:19:15.100769Z",
     "iopub.status.busy": "2021-09-27T16:19:15.099505Z",
     "iopub.status.idle": "2021-09-27T16:19:15.101787Z",
     "shell.execute_reply": "2021-09-27T16:19:15.102207Z",
     "shell.execute_reply.started": "2021-09-22T14:00:58.384781Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.056638,
     "end_time": "2021-09-27T16:19:15.102334",
     "exception": false,
     "start_time": "2021-09-27T16:19:15.045696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # remove highly correlated features\n",
    "# corr = train[[col for col in train.columns if col[-3:-1]!='_c' and col[:6]!='clust_' and col not in key_cols]].corr()\n",
    "# all_features = corr.columns.tolist()\n",
    "# print(f'Original feats is {len(all_features)}')\n",
    "# correlated_features = set()\n",
    "# for i in range(len(corr.columns)):\n",
    "#     for j in range(i):\n",
    "#         if corr.iloc[i, j] >= 0.99:\n",
    "#             correlated_features.add(corr.columns[i])\n",
    "# print(f'Removed feats is {len(correlated_features)}')\n",
    "# train = train.drop(correlated_features, axis=1)\n",
    "# test = test.drop(correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fdb2c17",
   "metadata": {
    "_cell_guid": "97f99c3b-de6e-4ba3-8a88-7df34d15fdc5",
    "_uuid": "e1a71109-4f2a-4634-80e6-2a85912e2c1f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:19:15.204617Z",
     "iopub.status.busy": "2021-09-27T16:19:15.203814Z",
     "iopub.status.idle": "2021-09-27T16:20:05.266723Z",
     "shell.execute_reply": "2021-09-27T16:20:05.266210Z",
     "shell.execute_reply.started": "2021-09-22T14:00:58.391297Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 50.117503,
     "end_time": "2021-09-27T16:20:05.266872",
     "exception": false,
     "start_time": "2021-09-27T16:19:15.149369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final fillna\n",
    "train = fill_inf_nan(train)\n",
    "test = fill_inf_nan(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6e1d4b4",
   "metadata": {
    "_cell_guid": "65dc811c-98c3-4291-bd0f-9b955db8d6ac",
    "_uuid": "0d0dec92-0ae5-4d3d-a097-29062f52efa7",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:20:05.366400Z",
     "iopub.status.busy": "2021-09-27T16:20:05.365852Z",
     "iopub.status.idle": "2021-09-27T16:20:05.369108Z",
     "shell.execute_reply": "2021-09-27T16:20:05.369694Z",
     "shell.execute_reply.started": "2021-09-22T14:01:54.039719Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.055705,
     "end_time": "2021-09-27T16:20:05.369864",
     "exception": false,
     "start_time": "2021-09-27T16:20:05.314159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features is 301\n"
     ]
    }
   ],
   "source": [
    "# Print number of features\n",
    "colNames = [col for col in list(train.columns) if col not in [\"stock_id\", \"time_id\", \"target\", \"row_id\"]]\n",
    "print(f'Number of features is {len(colNames)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abea9c",
   "metadata": {
    "_cell_guid": "6204ed07-7b0d-4bad-bb4d-26667b6f9ac7",
    "_uuid": "82be4e65-83e0-448a-96f7-08a9fc176f4f",
    "papermill": {
     "duration": 0.046771,
     "end_time": "2021-09-27T16:20:05.464383",
     "exception": false,
     "start_time": "2021-09-27T16:20:05.417612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952a3329",
   "metadata": {
    "_cell_guid": "d9e8b804-a677-4303-963b-49c7cae6da1c",
    "_uuid": "a176010d-1eb2-42bc-9582-a9482f973b16",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:20:05.574265Z",
     "iopub.status.busy": "2021-09-27T16:20:05.573761Z",
     "iopub.status.idle": "2021-09-27T16:21:08.246427Z",
     "shell.execute_reply": "2021-09-27T16:21:08.245774Z",
     "shell.execute_reply.started": "2021-09-22T14:01:54.054703Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 62.73527,
     "end_time": "2021-09-27T16:21:08.246605",
     "exception": false,
     "start_time": "2021-09-27T16:20:05.511335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:43: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n"
     ]
    }
   ],
   "source": [
    "# prepare NN dataset\n",
    "\n",
    "# features that requires quantile transformation for NN model\n",
    "nn_qt_feats = [col for col in train.columns if \\\n",
    "               col[-3:-1]!='_c' and \\\n",
    "               col[:6]!='clust_' and \\\n",
    "               'clustering_label' not in col and \\\n",
    "               col not in ['stock_id','time_id','row_id','target']]\n",
    "\n",
    "if TRAIN==True:\n",
    "    train_nn=train[nn_qt_feats].copy()\n",
    "else:\n",
    "    train_nn = pd.read_feather(os.path.join(FE_PATH, 'train_nn.f'))\n",
    "test_nn=test[nn_qt_feats].copy()\n",
    "\n",
    "# quantile transformation\n",
    "if TRAIN==True:\n",
    "    qt = QuantileTransformer(random_state=SEED, n_quantiles=2000, output_distribution='normal')\n",
    "    qt.fit(train_nn[nn_qt_feats])\n",
    "    train_nn[nn_qt_feats] = qt.transform(train_nn[nn_qt_feats])\n",
    "    test_nn[nn_qt_feats] = qt.transform(test_nn[nn_qt_feats])\n",
    "    pickle.dump(qt, open(f'quantile_transformer.p', 'wb'))\n",
    "else:\n",
    "    qt = pickle.load(open(os.path.join(FE_PATH, 'quantile_transformer.p'), 'rb'))\n",
    "    test_nn[nn_qt_feats] = qt.transform(test_nn[nn_qt_feats])\n",
    "\n",
    "# reset key columns\n",
    "train_nn[['stock_id','time_id','row_id','target']] = train[['stock_id','time_id','row_id','target']].reset_index(drop=True)\n",
    "test_nn[['stock_id','time_id','row_id']] = test[['stock_id','time_id','row_id']].reset_index(drop=True)\n",
    "\n",
    "# add back stock and time clustering labels\n",
    "stock_time_clust_label_feats = [c for c in train if 'clustering_label' in c]\n",
    "train_nn[stock_time_clust_label_feats] = train[stock_time_clust_label_feats]\n",
    "test_nn[stock_time_clust_label_feats] = test[stock_time_clust_label_feats]\n",
    "\n",
    "# generate stock cluster features\n",
    "if TRAIN==True:\n",
    "    train_nn, test_nn = gen_clust_feats(stock_groups=stock_groups, train=train_nn, test=test_nn)\n",
    "else:\n",
    "    _ , test_nn = gen_clust_feats(stock_groups=stock_groups, train=train_nn, test=test_nn)\n",
    "\n",
    "# remove correlated features\n",
    "# correlated_features = [c for c in correlated_features if c in train_nn.columns.tolist()]\n",
    "# train_nn = train_nn.drop(correlated_features, axis=1)\n",
    "# test_nn = test_nn.drop(correlated_features, axis=1)\n",
    "\n",
    "# fillna with mean\n",
    "train_nn = fill_inf_nan(train_nn)\n",
    "test_nn = fill_inf_nan(test_nn)\n",
    "\n",
    "# stock_id label encoding (required for NN embedding)\n",
    "nn_cat_cols = ['stock_id'] + stock_time_clust_label_feats\n",
    "for col in nn_cat_cols:\n",
    "    if TRAIN==True:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train_nn[col].values)\n",
    "        train_nn[col] = encoder.transform(train_nn[col].values)\n",
    "        test_nn[col] = encoder.transform(test_nn[col].values)\n",
    "        pickle.dump(encoder, open(f'nn_label_encoder_{col}.p', 'wb'))\n",
    "    else:\n",
    "        encoder = pickle.load(open(os.path.join(FE_PATH, f'nn_label_encoder_{col}.p'), 'rb'))\n",
    "        test_nn[col] = encoder.transform(test_nn[col].values)\n",
    "        \n",
    "# min max scaling\n",
    "nn_num_feats = [c for c in train_nn if c not in ['stock_id','time_id','row_id','target'] and 'clustering_label' not in c]\n",
    "if TRAIN==True:\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(train_nn[nn_num_feats])\n",
    "    train_nn[nn_num_feats] = scaler.transform(train_nn[nn_num_feats])\n",
    "    pickle.dump(scaler, open(f'nn_min_max_scaler.p', 'wb'))\n",
    "    test_nn[nn_num_feats] = scaler.transform(test_nn[nn_num_feats])\n",
    "else:\n",
    "    scaler = pickle.load(open(os.path.join(FE_PATH, 'nn_min_max_scaler.p'), 'rb'))\n",
    "    test_nn[nn_num_feats] = scaler.transform(test_nn[nn_num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebf1d9",
   "metadata": {
    "_cell_guid": "d8f068a8-c7d6-470a-b872-d43fd0425030",
    "_uuid": "decb09b9-2b67-474d-bec2-6915d9f54a79",
    "papermill": {
     "duration": 0.04777,
     "end_time": "2021-09-27T16:21:08.358626",
     "exception": false,
     "start_time": "2021-09-27T16:21:08.310856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TabNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "363bfa81",
   "metadata": {
    "_cell_guid": "4fbe7372-2638-4f82-8ab0-93062b71f519",
    "_uuid": "5847bf4d-7223-41fc-bd2b-1c6d466663f9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:08.484301Z",
     "iopub.status.busy": "2021-09-27T16:21:08.483097Z",
     "iopub.status.idle": "2021-09-27T16:21:08.895597Z",
     "shell.execute_reply": "2021-09-27T16:21:08.894684Z",
     "shell.execute_reply.started": "2021-09-22T14:04:43.152335Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.481776,
     "end_time": "2021-09-27T16:21:08.895751",
     "exception": false,
     "start_time": "2021-09-27T16:21:08.413975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define dataset for TabNet\n",
    "train_tbn = train_nn.copy()\n",
    "test_tbn = test_nn.copy()\n",
    "\n",
    "# identify categorical and numerical columns\n",
    "cat_cols = ['stock_id'] + stock_time_clust_label_feats\n",
    "num_cols = [c for c in train_tbn if c not in ['stock_id','time_id','row_id','target'] and c not in stock_time_clust_label_feats]\n",
    "\n",
    "# label encoding catergorical cols\n",
    "for col in cat_cols:\n",
    "    if TRAIN==True:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train_tbn[col].values)\n",
    "        train_tbn[col] = encoder.transform(train_tbn[col].values)\n",
    "        test_tbn[col] = encoder.transform(test_tbn[col].values)\n",
    "        pickle.dump(encoder, open(f'tabnet_label_encoder_{col}.p', 'wb'))\n",
    "    else:\n",
    "        encoder = pickle.load(open(os.path.join(FE_PATH, f'tabnet_label_encoder_{col}.p'), 'rb'))\n",
    "        test_tbn[col] = encoder.transform(test_tbn[col].values)\n",
    "    \n",
    "# standard scaling numerical cols\n",
    "if TRAIN==True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_tbn[num_cols])\n",
    "    train_tbn[num_cols] = scaler.transform(train_tbn[num_cols])\n",
    "    test_tbn[num_cols] = scaler.transform(test_tbn[num_cols])\n",
    "    pickle.dump(scaler, open(f'tabnet_std_scaler.p', 'wb'))\n",
    "else:\n",
    "    scaler = pickle.load(open(os.path.join(FE_PATH, 'tabnet_std_scaler.p'), 'rb'))\n",
    "    test_tbn[num_cols] = scaler.transform(test_tbn[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40b29410",
   "metadata": {
    "_cell_guid": "78325435-35b3-4fd6-a98b-f0a0e9446a92",
    "_uuid": "aab0ee83-653e-4232-9a90-c1718f213f72",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:08.996877Z",
     "iopub.status.busy": "2021-09-27T16:21:08.996344Z",
     "iopub.status.idle": "2021-09-27T16:21:09.000041Z",
     "shell.execute_reply": "2021-09-27T16:21:08.999535Z",
     "shell.execute_reply.started": "2021-09-22T14:05:24.984272Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.055955,
     "end_time": "2021-09-27T16:21:09.000145",
     "exception": false,
     "start_time": "2021-09-27T16:21:08.944190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "if TRAIN==True:\n",
    "    train.reset_index(drop=True).to_feather('train.f')\n",
    "    test.reset_index(drop=True).to_feather('test.f')\n",
    "    train_nn.reset_index(drop=True).to_feather('train_nn.f')\n",
    "    test_nn.reset_index(drop=True).to_feather('test_nn.f')\n",
    "    train_tbn.reset_index(drop=True).to_feather('train_tbn.f')\n",
    "    test_tbn.reset_index(drop=True).to_feather('test_tbn.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d99e003",
   "metadata": {
    "_cell_guid": "fd53bb0a-6955-4ef9-9fc9-e68c65b28b5a",
    "_uuid": "02c53290-b884-4dec-9319-a14943e977fb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:09.100462Z",
     "iopub.status.busy": "2021-09-27T16:21:09.099758Z",
     "iopub.status.idle": "2021-09-27T16:21:09.104666Z",
     "shell.execute_reply": "2021-09-27T16:21:09.105199Z",
     "shell.execute_reply.started": "2021-09-22T14:05:28.934833Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.057297,
     "end_time": "2021-09-27T16:21:09.105358",
     "exception": false,
     "start_time": "2021-09-27T16:21:09.048061",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 305)\n",
      "(3, 304)\n",
      "(428932, 301)\n",
      "(3, 300)\n",
      "(428932, 301)\n",
      "(3, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train_nn.shape)\n",
    "print(test_nn.shape)\n",
    "print(train_tbn.shape)\n",
    "print(test_tbn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d3179",
   "metadata": {
    "_cell_guid": "65ea6a71-368c-49b4-b1c2-bb30476226ee",
    "_uuid": "c51be637-a4ae-40cc-a526-ce55e0f94ebb",
    "papermill": {
     "duration": 0.047519,
     "end_time": "2021-09-27T16:21:09.200890",
     "exception": false,
     "start_time": "2021-09-27T16:21:09.153371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Change Log\n",
    "* Version 40: LGBM: stock clust + time clust; NN: stock clust; TabNet: time clust\n",
    "* Version 41: LGBM: time clust; NN: stock clust; TabNet: time clust\n",
    "* Version 42: Clipping min=0\n",
    "* Version 44: Ensembling - Random Forest\n",
    "* Version 45: Ensembling - ElasticNet with correlated stocks\n",
    "* Version 46: FE with enhanced error handling\n",
    "* Version 47: Ensembling - ElasticNet with correlated stocks (not forcing positive weights)\n",
    "* Version 48: Ensembling - Random Forest with correlated stocks\n",
    "* Version 49: Ensembling - ElasticNet with correlated stocks (not forcing positive weights)\n",
    "* Version 50: Testing of new FE dataset with syn testing (only run LGBM)\n",
    "* Version 51: Removing >0.99 correlated features\n",
    "* Version 53:\n",
    "    - Optimized LGBM + TabNet params\n",
    "    - Remove correlated feats for TabNet\n",
    "    - Ensembling - Random Forest with correlated stocks\n",
    "* Version 54: ElasticNet\n",
    "\n",
    "Feature:\n",
    "* Stock / Time clustering - direclty put clustering centroid as feature\n",
    "* Correlation between time series as a feature\n",
    "* Remove highly correlated features\n",
    "* Quarticity https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/267096\n",
    "\n",
    "Model:\n",
    "* RF meta model\n",
    "* Add correlated stock prediction to meta model\n",
    "* Final HP Tuning of all models\n",
    "* AutoEncoder+1dCNN from public\n",
    "* 1D CNN\n",
    "* 2D CNN https://towardsdatascience.com/how-to-encode-time-series-into-images-for-financial-forecasting-using-convolutional-neural-networks-5683eb5c53d9\n",
    "    - GAF for time series to image transformation\n",
    "    - 11 features corresponding to 11 channels\n",
    "* LSTM + Dense layer regression\n",
    "\n",
    "Ensembling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0911f2ae",
   "metadata": {
    "_cell_guid": "cb685a21-2b0b-4d23-9857-b50994fbeb91",
    "_uuid": "963e2dbb-5ddd-46f9-a1b9-445182fc4b96",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:09.301182Z",
     "iopub.status.busy": "2021-09-27T16:21:09.300663Z",
     "iopub.status.idle": "2021-09-27T16:21:09.304383Z",
     "shell.execute_reply": "2021-09-27T16:21:09.303989Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.470291Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.055808,
     "end_time": "2021-09-27T16:21:09.304484",
     "exception": false,
     "start_time": "2021-09-27T16:21:09.248676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE = True\n",
    "TEST_MODE = 'test'\n",
    "\n",
    "FE_PATH = '/kaggle/input/volatility-fe-output-version-15'\n",
    "BASE_MODEL_PATH = '/kaggle/input/volatility-model-training-output-version-54' # this is for fixing the pre-trained base models (for submission only)\n",
    "\n",
    "SEED = 1111\n",
    "N_FOLD = 5\n",
    "\n",
    "LGBM_NUM_BOOST = 3000\n",
    "NN_EPOCH = 1000\n",
    "TABNET_EPOCH = 1000\n",
    "# LGBM_NUM_BOOST = 1\n",
    "# NN_EPOCH = 1\n",
    "# TABNET_EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2235d4e5",
   "metadata": {
    "_cell_guid": "2d27616a-cf6d-4eaa-bed8-852f4baa3fce",
    "_uuid": "241cbb38-a311-448a-812d-c5635db692f3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:09.408475Z",
     "iopub.status.busy": "2021-09-27T16:21:09.407988Z",
     "iopub.status.idle": "2021-09-27T16:21:10.288207Z",
     "shell.execute_reply": "2021-09-27T16:21:10.287684Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.47842Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.935824,
     "end_time": "2021-09-27T16:21:10.288327",
     "exception": false,
     "start_time": "2021-09-27T16:21:09.352503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "import random\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# set seed\n",
    "def seed_everything(seed=SEED):\n",
    "    import torch\n",
    "    import random\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    tf.random.set_seed(SEED)\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcaf7b20",
   "metadata": {
    "_cell_guid": "5adc5354-e49c-46ef-806f-f439622d779c",
    "_uuid": "e98f43cf-69a9-4f60-885f-76c22f3662e2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:10.390562Z",
     "iopub.status.busy": "2021-09-27T16:21:10.389312Z",
     "iopub.status.idle": "2021-09-27T16:21:10.392211Z",
     "shell.execute_reply": "2021-09-27T16:21:10.391760Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.503171Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.055093,
     "end_time": "2021-09-27T16:21:10.392382",
     "exception": false,
     "start_time": "2021-09-27T16:21:10.337289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'a':[1,2,3,4,5], 'b':['a','a','b','b','c']})\n",
    "# df.groupby('b')['a'].agg(lambda s:s.max()-s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3012dd7d",
   "metadata": {
    "_cell_guid": "32f7a686-3d46-4ebf-a715-9e53bd399639",
    "_uuid": "b633d66f-e268-4a67-a55d-99052a61949a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:10.492511Z",
     "iopub.status.busy": "2021-09-27T16:21:10.492035Z",
     "iopub.status.idle": "2021-09-27T16:21:10.495808Z",
     "shell.execute_reply": "2021-09-27T16:21:10.495391Z",
     "shell.execute_reply.started": "2021-09-26T10:12:26.516523Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.055183,
     "end_time": "2021-09-27T16:21:10.495914",
     "exception": false,
     "start_time": "2021-09-27T16:21:10.440731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import from FE script\n",
    "if INFERENCE==False:\n",
    "    train = pd.read_feather(os.path.join(FE_PATH, 'train.f'))\n",
    "    test = pd.read_feather(os.path.join(FE_PATH, 'test.f'))\n",
    "    print(f'Train data shape is {train.shape}')\n",
    "    print(f'Test data shape is {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "753d761f",
   "metadata": {
    "_cell_guid": "53d33def-f0dd-4a40-8d07-7152aa10d034",
    "_uuid": "6d0372c8-843e-4791-82cb-61b670a1addc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:10.598034Z",
     "iopub.status.busy": "2021-09-27T16:21:10.597254Z",
     "iopub.status.idle": "2021-09-27T16:21:10.614461Z",
     "shell.execute_reply": "2021-09-27T16:21:10.614861Z",
     "shell.execute_reply.started": "2021-09-26T10:12:28.698699Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.071288,
     "end_time": "2021-09-27T16:21:10.614999",
     "exception": false,
     "start_time": "2021-09-27T16:21:10.543711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the keys of each dataset which are preserved in the whole notebook\n",
    "train_key_cols = ['stock_id','time_id','row_id','target']\n",
    "test_key_cols = ['stock_id','time_id','row_id']\n",
    "train_keys = train[train_key_cols].reset_index(drop=True)\n",
    "test_keys = test[test_key_cols].reset_index(drop=True)\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ac4ceb9",
   "metadata": {
    "_cell_guid": "369500f3-cbe1-4278-a8c5-ce76522f0a85",
    "_uuid": "b68f4ada-e1ee-4977-841c-415eb187800f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:10.714752Z",
     "iopub.status.busy": "2021-09-27T16:21:10.714149Z",
     "iopub.status.idle": "2021-09-27T16:21:10.718721Z",
     "shell.execute_reply": "2021-09-27T16:21:10.719097Z",
     "shell.execute_reply.started": "2021-09-26T10:12:28.747188Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.056116,
     "end_time": "2021-09-27T16:21:10.719239",
     "exception": false,
     "start_time": "2021-09-27T16:21:10.663123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 305)\n",
      "(428932,)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29ee6dd",
   "metadata": {
    "_cell_guid": "e3c9d6d6-c3af-4a67-a022-641c27a8fded",
    "_uuid": "d30400ac-863b-4e09-b744-03164518d4df",
    "papermill": {
     "duration": 0.048775,
     "end_time": "2021-09-27T16:21:10.815743",
     "exception": false,
     "start_time": "2021-09-27T16:21:10.766968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5abe7d50",
   "metadata": {
    "_cell_guid": "0f61d8b6-3725-4a24-9234-3f4627d4cbcb",
    "_uuid": "8145a5d6-838b-47ae-822c-7b1d3baba003",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:10.924427Z",
     "iopub.status.busy": "2021-09-27T16:21:10.923877Z",
     "iopub.status.idle": "2021-09-27T16:21:13.784960Z",
     "shell.execute_reply": "2021-09-27T16:21:13.785392Z",
     "shell.execute_reply.started": "2021-09-26T10:12:28.760595Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 2.92132,
     "end_time": "2021-09-27T16:21:13.785534",
     "exception": false,
     "start_time": "2021-09-27T16:21:10.864214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features considered for LightGBM is 270\n",
      "Inferring fold 0\n",
      "Inferring fold 1\n",
      "Inferring fold 2\n",
      "Inferring fold 3\n",
      "Inferring fold 4\n",
      "LGBM averaged CV score is 1.0\n",
      "CPU times: user 1.44 s, sys: 120 ms, total: 1.56 s\n",
      "Wall time: 2.86 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_features = [col for col in train.columns if col not in [\"time_id\", \"target\", \"row_id\"] and 'ts_ae' not in col]\n",
    "\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': 60,\n",
    "    'max_bin': 250,\n",
    "    'min_data_in_leaf':422,\n",
    "    'learning_rate': 0.014565641020775516,\n",
    "    'subsample': 0.6563801192981948,\n",
    "    'subsample_freq': 1,\n",
    "    'feature_fraction': 0.525513036358404,\n",
    "    'lambda_l1': 8.177995270216595,\n",
    "    'lambda_l2': 3.8822889556906657,\n",
    "    'categorical_column': [lgb_features.index('stock_id'),\n",
    "                           lgb_features.index('clust_wap1_sma50'),\n",
    "                           lgb_features.index('clust_wap1_sms50'),\n",
    "                           lgb_features.index('clust_total_volume_sma60'),\n",
    "                           lgb_features.index('clust_volume_imbalance_sma80'),\n",
    "                           lgb_features.index('stock_clustering_label'),\n",
    "                           lgb_features.index('time_clustering_label')],\n",
    "    'seed': SEED,\n",
    "    'feature_fraction_seed': SEED,\n",
    "    'bagging_seed': SEED,\n",
    "    'drop_seed': SEED,\n",
    "    'data_random_seed': SEED,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):  \n",
    "    print(f'Number of features considered for LightGBM is {len(lgb_features)}')\n",
    "    oof_pred = np.zeros(train.shape[0])\n",
    "    test_pred = []\n",
    "    kfold = GroupKFold(n_splits=N_FOLD)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train, y, train_keys.time_id)):\n",
    "        \n",
    "        if INFERENCE==False:\n",
    "            print(f'Training fold {fold}')\n",
    "            x_train, x_val = train.iloc[trn_idx], train.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "            train_weights = 1 / np.square(y_train)\n",
    "            val_weights = 1 / np.square(y_val)\n",
    "            train_dataset = lgb.Dataset(x_train[lgb_features], y_train, weight=train_weights)\n",
    "            val_dataset = lgb.Dataset(x_val[lgb_features], y_val, weight=val_weights)\n",
    "            model = lgb.train(params=params,\n",
    "                              num_boost_round=LGBM_NUM_BOOST,\n",
    "                              train_set=train_dataset, \n",
    "                              valid_sets=[train_dataset, val_dataset], \n",
    "                              verbose_eval=250,\n",
    "                              early_stopping_rounds=50,\n",
    "                              feval=feval_rmspe)\n",
    "            # predictions\n",
    "            oof_pred[val_idx] = model.predict(x_val[lgb_features])\n",
    "            test_pred.append(model.predict(test[lgb_features]))\n",
    "            # save model\n",
    "            pickle.dump(model, open(f'lgbm_fold{fold}.p', 'wb'))\n",
    "        \n",
    "        elif INFERENCE==True:\n",
    "            print(f'Inferring fold {fold}')\n",
    "            model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'lgbm_fold{fold}.p'), 'rb'))\n",
    "            test_pred.append(model.predict(test[lgb_features]))\n",
    "    # Return test predictions\n",
    "    return oof_pred, test_pred\n",
    "\n",
    "# Traing and evaluate\n",
    "oof_pred_lgb, test_pred_lgb = train_and_evaluate_lgb(train, test, params0)\n",
    "cv_score_lgb = round(rmspe(train['target'], oof_pred_lgb), 5)\n",
    "print(f'LGBM averaged CV score is {cv_score_lgb}')\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31007bf9",
   "metadata": {
    "_cell_guid": "93cabbad-adf3-4df7-bf5b-74b9445d8d26",
    "_uuid": "fb34542f-4b03-4e01-abf6-49069177a198",
    "papermill": {
     "duration": 0.051244,
     "end_time": "2021-09-27T16:21:13.887276",
     "exception": false,
     "start_time": "2021-09-27T16:21:13.836032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22b4fcd9",
   "metadata": {
    "_cell_guid": "081c6700-36e3-48aa-bc2f-6907f18575cc",
    "_uuid": "3efb98ad-f315-49cd-a49e-c2beb3b31490",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:13.992422Z",
     "iopub.status.busy": "2021-09-27T16:21:13.991568Z",
     "iopub.status.idle": "2021-09-27T16:21:13.994464Z",
     "shell.execute_reply": "2021-09-27T16:21:13.994055Z",
     "shell.execute_reply.started": "2021-09-26T10:14:17.258156Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.057492,
     "end_time": "2021-09-27T16:21:13.994577",
     "exception": false,
     "start_time": "2021-09-27T16:21:13.937085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if INFERENCE==False:\n",
    "    train_nn = pd.read_feather(os.path.join(FE_PATH, 'train_nn.f'))\n",
    "    test_nn = pd.read_feather(os.path.join(FE_PATH, 'test_nn.f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caf00c04",
   "metadata": {
    "_cell_guid": "d1bd83ac-1c2b-4373-9491-7019377a1441",
    "_uuid": "371d5b6f-ff02-4946-aa2f-801f2b5324ca",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:14.101757Z",
     "iopub.status.busy": "2021-09-27T16:21:14.101117Z",
     "iopub.status.idle": "2021-09-27T16:21:14.143994Z",
     "shell.execute_reply": "2021-09-27T16:21:14.143095Z",
     "shell.execute_reply.started": "2021-09-26T10:14:18.527002Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.09934,
     "end_time": "2021-09-27T16:21:14.144136",
     "exception": false,
     "start_time": "2021-09-27T16:21:14.044796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b89585e6",
   "metadata": {
    "_cell_guid": "7bcb4b85-d88e-4fe7-9f34-fc30b2eb60a2",
    "_uuid": "efca835d-8e94-41e5-b46c-8da8f95cf7ca",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:14.259125Z",
     "iopub.status.busy": "2021-09-27T16:21:14.258547Z",
     "iopub.status.idle": "2021-09-27T16:21:22.337901Z",
     "shell.execute_reply": "2021-09-27T16:21:22.336950Z",
     "shell.execute_reply.started": "2021-09-26T10:14:18.546481Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 8.143169,
     "end_time": "2021-09-27T16:21:22.338054",
     "exception": false,
     "start_time": "2021-09-27T16:21:14.194885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train[out_train.time_id.isin(train_nn.time_id.unique())]\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "# code to add the just the read data after first execution\n",
    "# data separation based on knn ++\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "nind = int(mat.shape[0] / N_FOLD) # number of individuals\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=N_FOLD, replace=False)\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "for n in range(N_FOLD):\n",
    "    totDist.append(np.zeros(mat.shape[0]-N_FOLD))\n",
    "# saves index\n",
    "for n in range(N_FOLD):\n",
    "    values.append([lineNumber[n]])    \n",
    "s=[]\n",
    "for n in range(N_FOLD):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "for n in range(nind-1):    \n",
    "    luck = np.random.uniform(0,1,N_FOLD)\n",
    "    for cycle in range(N_FOLD):\n",
    "         # saves the values of index           \n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(N_FOLD):\n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "for n_mod in range(N_FOLD):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d4e42e4",
   "metadata": {
    "_cell_guid": "73a2f2af-7116-4a4a-9443-46f5fe9aae76",
    "_uuid": "f14f0725-741a-4ec9-b2f4-91e540581735",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:22.450794Z",
     "iopub.status.busy": "2021-09-27T16:21:22.450006Z",
     "iopub.status.idle": "2021-09-27T16:21:22.455073Z",
     "shell.execute_reply": "2021-09-27T16:21:22.454628Z",
     "shell.execute_reply.started": "2021-09-26T10:14:27.869839Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.066126,
     "end_time": "2021-09-27T16:21:22.455180",
     "exception": false,
     "start_time": "2021-09-27T16:21:22.389054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NN Model Architecture\n",
    "\n",
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "def create_nn_model(hidden_units):\n",
    "    # initialize input list (later to be concatenated)\n",
    "    raw_input_list = []\n",
    "    concat_input_list = []\n",
    "    \n",
    "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
    "    num_input = keras.Input(shape=(len(numerical_feats),), name='num_data')\n",
    "    raw_input_list.append(num_input)\n",
    "    concat_input_list.append(num_input)\n",
    "\n",
    "    # stock_id embedding\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    stock_id_embedded = keras.layers.Embedding(stock_id_orig_dim, stock_id_emb_dim, input_length=1, name='stock_id_embedding')(stock_id_input)\n",
    "    stock_id_flattened = keras.layers.Flatten()(stock_id_embedded)\n",
    "    raw_input_list.append(stock_id_input)\n",
    "    concat_input_list.append(stock_id_flattened)\n",
    "    \n",
    "    # stock clustering embedding\n",
    "    stock_clustering_label_input = keras.Input(shape=(1,), name='stock_clustering_label')\n",
    "    stock_clustering_label_embedded = keras.layers.Embedding(stock_clustering_label_orig_dim, stock_clustering_label_emb_dim, input_length=1, name='stock_clustering_label_embedding')(stock_clustering_label_input)\n",
    "    stock_clustering_label_flattened = keras.layers.Flatten()(stock_clustering_label_embedded)\n",
    "    raw_input_list.append(stock_clustering_label_input)\n",
    "    concat_input_list.append(stock_clustering_label_flattened)\n",
    "    \n",
    "#     # time clustering embedding\n",
    "#     time_clustering_label_input = keras.Input(shape=(1,), name='time_clustering_label')\n",
    "#     time_clustering_label_embedded = keras.layers.Embedding(time_clustering_label_orig_dim, time_clustering_label_emb_dim, input_length=1, name='time_clustering_label_embedding')(time_clustering_label_input)\n",
    "#     time_clustering_label_flattened = keras.layers.Flatten()(time_clustering_label_embedded)\n",
    "#     raw_input_list.append(time_clustering_label_input)\n",
    "#     concat_input_list.append(time_clustering_label_flattened)\n",
    "    \n",
    "    # concatencate all input layers\n",
    "    x = keras.layers.Concatenate()(concat_input_list)\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "        x = keras.layers.Dense(n_hidden, activation='swish')(x)\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=raw_input_list, outputs=out)\n",
    "    model.compile(keras.optimizers.Adam(learning_rate=0.006), loss=root_mean_squared_per_error)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ad7c060",
   "metadata": {
    "_cell_guid": "3921de83-5d80-4ab7-84b3-f43d1cc22d8c",
    "_uuid": "b395ebb4-ac60-4fe9-ac31-9a81afa4a80e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:22.567858Z",
     "iopub.status.busy": "2021-09-27T16:21:22.567287Z",
     "iopub.status.idle": "2021-09-27T16:21:49.337374Z",
     "shell.execute_reply": "2021-09-27T16:21:49.338070Z",
     "shell.execute_reply.started": "2021-09-26T10:14:27.8857Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 26.833338,
     "end_time": "2021-09-27T16:21:49.338322",
     "exception": false,
     "start_time": "2021-09-27T16:21:22.504984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring layer 0 fold 0\n",
      "Inferring layer 0 fold 1\n",
      "Inferring layer 0 fold 2\n",
      "Inferring layer 0 fold 3\n",
      "Inferring layer 0 fold 4\n",
      "Hidden units is  (240, 200, 160, 120, 80, 40, 20)\n",
      "Individual folds score is []\n",
      "Inferring layer 1 fold 0\n",
      "Inferring layer 1 fold 1\n",
      "Inferring layer 1 fold 2\n",
      "Inferring layer 1 fold 3\n",
      "Inferring layer 1 fold 4\n",
      "Hidden units is  (220, 180, 140, 100, 50, 25)\n",
      "Individual folds score is []\n",
      "Inferring layer 2 fold 0\n",
      "Inferring layer 2 fold 1\n",
      "Inferring layer 2 fold 2\n",
      "Inferring layer 2 fold 3\n",
      "Inferring layer 2 fold 4\n",
      "Hidden units is  (200, 150, 100, 50, 25)\n",
      "Individual folds score is []\n",
      "Inferring layer 3 fold 0\n",
      "Inferring layer 3 fold 1\n",
      "Inferring layer 3 fold 2\n",
      "Inferring layer 3 fold 3\n",
      "Inferring layer 3 fold 4\n",
      "Hidden units is  (180, 120, 60, 30)\n",
      "Individual folds score is []\n",
      "CPU times: user 21.1 s, sys: 1 s, total: 22.1 s\n",
      "Wall time: 26.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15462"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "hidden_units_list = [(240,200,160,120,80,40,20), (220,180,140,100,50,25), (200,150,100,50,25), (180,120,60,30)]\n",
    "# hidden_units_list = [(4,2), (4,2), (4,2), (4,2)]\n",
    "# hidden_units_list = [(128,64,32)]\n",
    "stock_id_orig_dim, stock_id_emb_dim = 112, 24\n",
    "stock_clustering_label_orig_dim, stock_clustering_label_emb_dim = 6, 4\n",
    "# time_clustering_label_orig_dim, time_clustering_label_emb_dim = 8, 4\n",
    "numerical_feats = [c for c in train_nn if c not in train_key_cols and 'clustering_label' not in c]\n",
    "train_nn_stock_id = train_nn['stock_id']\n",
    "train_nn_stock_clustering_label = train_nn['stock_clustering_label']\n",
    "# train_nn_time_clustering_label = train_nn['time_clustering_label']\n",
    "\n",
    "def train_and_eval_nn():\n",
    "    oof_pred_nn_list = []\n",
    "    test_pred_nn_list = []\n",
    "    for h in range(len(hidden_units_list)):\n",
    "        # initialize predictions and scores\n",
    "        oof_pred_nn = np.zeros(train_nn.shape[0])\n",
    "        fold_scores = []\n",
    "        test_pred_nn = []\n",
    "        for fold in range(N_FOLD):\n",
    "            if INFERENCE==False:\n",
    "                print(f'Training fold {fold}...')\n",
    "                # train-test split\n",
    "                indexes = np.arange(N_FOLD).astype(int)    \n",
    "                indexes = np.delete(indexes, obj=fold, axis=0) \n",
    "                indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "                trn_idx = train_nn[train_nn.time_id.isin(indexes)].index.tolist()\n",
    "                val_idx = train_nn[train_nn.time_id.isin(values[fold])].index.tolist()\n",
    "                X_train = train_nn.iloc[trn_idx,:][numerical_feats + ['stock_id','stock_clustering_label']]\n",
    "                y_train = train_nn.iloc[trn_idx,:]['target']\n",
    "                X_valid = train_nn.iloc[val_idx,:][numerical_feats + ['stock_id','stock_clustering_label']]\n",
    "                y_valid = train_nn.iloc[val_idx,:]['target']\n",
    "\n",
    "                # define numerical and categorical data for train set\n",
    "                X_train_num = X_train[numerical_feats].values\n",
    "                X_train_stock_id = X_train['stock_id']\n",
    "                X_train_stock_clustering_label = X_train['stock_clustering_label']\n",
    "#                 X_train_time_clustering_label = X_train['time_clustering_label']\n",
    "\n",
    "                # define numerical and categorical data for validation set\n",
    "                X_valid_num = X_valid[numerical_feats].values\n",
    "                X_valid_stock_id = X_valid['stock_id']\n",
    "                X_valid_stock_clustering_label = X_valid['stock_clustering_label']\n",
    "#                 X_valid_time_clustering_label = X_valid['time_clustering_label']\n",
    "\n",
    "                # model training\n",
    "                model = create_nn_model(hidden_units=hidden_units_list[h])\n",
    "                model.fit([X_train_num, X_train_stock_id, X_train_stock_clustering_label], \n",
    "                          y_train,               \n",
    "                          batch_size=2048,\n",
    "                          epochs=NN_EPOCH,\n",
    "                          validation_data=([X_valid_num, X_valid_stock_id, X_valid_stock_clustering_label], y_valid),\n",
    "                          callbacks=[es, plateau],\n",
    "                          validation_batch_size=len(y_valid),\n",
    "                          shuffle=True,\n",
    "                          verbose=1)\n",
    "\n",
    "                # validation result\n",
    "                val_pred = model.predict([X_valid_num, X_valid_stock_id, X_valid_stock_clustering_label]).reshape(1,-1)[0]\n",
    "                oof_pred_nn[val_idx] = val_pred\n",
    "                score = round(rmspe(y_valid, val_pred),5)\n",
    "                fold_scores.append(score)\n",
    "                print('Fold {}: {}'.format(fold, score))\n",
    "\n",
    "                # test data prediction\n",
    "                test_pred_nn.append(model.predict([test_nn[numerical_feats].values, test_nn['stock_id'], test_nn['stock_clustering_label']]).reshape(1,-1)[0].clip(0,1e10))\n",
    "                # save model\n",
    "                model.save(f'nn_layer{h}_fold{fold}')\n",
    "            \n",
    "            elif INFERENCE==True:\n",
    "                print(f'Inferring layer {h} fold {fold}')\n",
    "                path = os.path.join(BASE_MODEL_PATH, f'nn_layer{h}_fold{fold}')\n",
    "                model = tf.keras.models.load_model(path, compile=False)\n",
    "                test_pred_nn.append(model.predict([test_nn[numerical_feats].values, test_nn['stock_id'], test_nn['stock_clustering_label']]).reshape(1,-1)[0].clip(0,1e10))\n",
    "                \n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "    \n",
    "        # check OOF data quality\n",
    "        print('Hidden units is ', hidden_units_list[h])\n",
    "        print(f'Individual folds score is', fold_scores)\n",
    "        oof_pred_nn_list.append(oof_pred_nn)\n",
    "        test_pred_nn_list.append(test_pred_nn)\n",
    "    return oof_pred_nn_list, test_pred_nn_list\n",
    "\n",
    "oof_pred_nn_list, test_pred_nn_list = train_and_eval_nn()\n",
    "\n",
    "del train_nn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13059adc",
   "metadata": {
    "_cell_guid": "5ae2f60a-0ecc-4e92-9708-028422e72aba",
    "_uuid": "8771d6ae-1055-4041-b2ed-3d85f396d8df",
    "papermill": {
     "duration": 0.093159,
     "end_time": "2021-09-27T16:21:49.523780",
     "exception": false,
     "start_time": "2021-09-27T16:21:49.430621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8940fea",
   "metadata": {
    "_cell_guid": "5dced165-190c-4a59-a54a-a98e791f3735",
    "_uuid": "ecc7703f-9beb-44a6-95ef-21fa7ddee10f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:49.720654Z",
     "iopub.status.busy": "2021-09-27T16:21:49.718818Z",
     "iopub.status.idle": "2021-09-27T16:21:49.722024Z",
     "shell.execute_reply": "2021-09-27T16:21:49.721369Z",
     "shell.execute_reply.started": "2021-09-26T10:17:50.814434Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.104061,
     "end_time": "2021-09-27T16:21:49.722173",
     "exception": false,
     "start_time": "2021-09-27T16:21:49.618112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if INFERENCE==False:\n",
    "    train_tbn = pd.read_feather(os.path.join(FE_PATH, 'train_tbn.f'))\n",
    "    test_tbn = pd.read_feather(os.path.join(FE_PATH, 'test_tbn.f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73ae4278",
   "metadata": {
    "_cell_guid": "9c3a2f22-2c52-4e13-916d-aa94d719c039",
    "_uuid": "e0a7a5e6-8072-4cd0-966a-d7254adaf5be",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:21:49.898350Z",
     "iopub.status.busy": "2021-09-27T16:21:49.865133Z",
     "iopub.status.idle": "2021-09-27T16:22:16.029897Z",
     "shell.execute_reply": "2021-09-27T16:22:16.030438Z",
     "shell.execute_reply.started": "2021-09-26T10:17:52.064645Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 26.233792,
     "end_time": "2021-09-27T16:22:16.030664",
     "exception": false,
     "start_time": "2021-09-27T16:21:49.796872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "2\n",
      "Mon Sep 27 16:22:15 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0    33W / 250W |  16059MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "import psutil\n",
    "print(psutil.cpu_count())\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6d700fd",
   "metadata": {
    "_cell_guid": "54fc1a0a-0f33-4ca9-800d-505db717047d",
    "_uuid": "6a2c0376-c935-4f02-bb59-407a10cffe1b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:22:16.151942Z",
     "iopub.status.busy": "2021-09-27T16:22:16.151297Z",
     "iopub.status.idle": "2021-09-27T16:22:16.153781Z",
     "shell.execute_reply": "2021-09-27T16:22:16.154155Z",
     "shell.execute_reply.started": "2021-09-26T10:18:18.41307Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.065705,
     "end_time": "2021-09-27T16:22:16.154278",
     "exception": false,
     "start_time": "2021-09-27T16:22:16.088573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize = False\n",
    "    def __call__(self, y_true, y_score):\n",
    "        return np.sqrt(np.mean(np.square((y_true - y_score) / y_true)))\n",
    "\n",
    "def RMSPELoss(y_pred, y_true):\n",
    "    return torch.sqrt(torch.mean( ((y_true - y_pred) / y_true) ** 2 )).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13e98e85",
   "metadata": {
    "_cell_guid": "20fede11-1ad3-4d5e-b2b5-687e8b9a99b2",
    "_uuid": "b48a37b4-e875-44d6-8747-03295df91a7a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:22:16.276280Z",
     "iopub.status.busy": "2021-09-27T16:22:16.275764Z",
     "iopub.status.idle": "2021-09-27T16:22:16.288503Z",
     "shell.execute_reply": "2021-09-27T16:22:16.288107Z",
     "shell.execute_reply.started": "2021-09-26T10:18:18.422874Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.07751,
     "end_time": "2021-09-27T16:22:16.288608",
     "exception": false,
     "start_time": "2021-09-27T16:22:16.211098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# identify categorical and numerical columns\n",
    "correlated_features = pickle.load(open(os.path.join('/kaggle/input/volatility-correlated-features', f'correlated_features.p'), 'rb'))\n",
    "cat_cols = ['stock_id','time_clustering_label']\n",
    "num_cols = [c for c in train_tbn if c not in ['stock_id','time_id','row_id','target'] and 'clustering_label' not in c and c not in correlated_features]\n",
    "tabnet_feats = [c for c in train_tbn if c in cat_cols + num_cols]\n",
    "\n",
    "# define categorical features index and dimentions for Tabnet params\n",
    "cat_idxs = [tabnet_feats.index(c) for c in cat_cols]\n",
    "cat_dims = [112, 8]\n",
    "cat_emb_dim = [24, 4]\n",
    "\n",
    "tabnet_params = dict(\n",
    "    cat_idxs = cat_idxs,\n",
    "    cat_dims = cat_dims,\n",
    "    cat_emb_dim = cat_emb_dim,\n",
    "    n_d = 24,\n",
    "    n_a = 24,\n",
    "    n_steps = 1,\n",
    "    gamma = 2.0,\n",
    "    n_independent = 2,\n",
    "    n_shared = 2,\n",
    "    lambda_sparse = 2.166158737727093e-06,\n",
    "    mask_type = \"entmax\",\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    scheduler_params=dict(mode=\"min\", patience=3, min_lr=1e-5, factor=0.5),\n",
    "    seed = SEED,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98ee031c",
   "metadata": {
    "_cell_guid": "5ecf7d2e-4aeb-4585-b3ac-64d3801179bb",
    "_uuid": "a035b31e-3bda-4e57-871e-d87cb898794f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:22:16.412257Z",
     "iopub.status.busy": "2021-09-27T16:22:16.411436Z",
     "iopub.status.idle": "2021-09-27T16:22:17.341243Z",
     "shell.execute_reply": "2021-09-27T16:22:17.341695Z",
     "shell.execute_reply.started": "2021-09-26T10:18:18.437769Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.996314,
     "end_time": "2021-09-27T16:22:17.341858",
     "exception": false,
     "start_time": "2021-09-27T16:22:16.345544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "OOF score across folds: 1.0\n",
      "CPU times: user 616 ms, sys: 72.8 ms, total: 689 ms\n",
      "Wall time: 923 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "def zip_directory(folder_path, zip_path):\n",
    "    with zipfile.ZipFile(zip_path, mode='w') as zipf:\n",
    "        len_dir_path = len(folder_path)\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, file_path[len_dir_path:])\n",
    "    return\n",
    "                \n",
    "def train_and_eval_tabnet():\n",
    "    kfold = GroupKFold(n_splits=N_FOLD)\n",
    "    oof_pred_tbn = np.zeros(train_tbn.shape[0])\n",
    "    test_pred_tbn = []\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train_tbn, train_tbn.target, train_tbn.time_id)):\n",
    "        if INFERENCE==False:\n",
    "            print(f'Training fold {fold}......')\n",
    "            X_train, X_val = train_tbn[tabnet_feats].iloc[trn_idx].values, train_tbn[tabnet_feats].iloc[val_idx].values\n",
    "            y_train, y_val = train_tbn.target.iloc[trn_idx].values.reshape(-1,1), train_tbn.target.iloc[val_idx].values.reshape(-1,1)\n",
    "            \n",
    "            model = TabNetRegressor(**tabnet_params)\n",
    "            model.fit(\n",
    "              X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              max_epochs = TABNET_EPOCH,\n",
    "              patience = 10,\n",
    "              batch_size = 1024, \n",
    "              virtual_batch_size = 128,\n",
    "              num_workers = 0,\n",
    "              drop_last = False,\n",
    "              eval_metric=[RMSPE],\n",
    "              loss_fn=RMSPELoss\n",
    "              )\n",
    "            # saving model\n",
    "            saving_path_name = f\"TabNet_fold{fold}\"\n",
    "            saved_filepath = model.save_model(saving_path_name)\n",
    "            # predictions\n",
    "            oof_pred = model.predict(X_val).flatten()\n",
    "            oof_pred_tbn[val_idx] = oof_pred\n",
    "            val_rmspe = rmspe(y_val.flatten(), oof_pred)\n",
    "            print(f'TabNet fold {fold} RMSPE is {val_rmspe}')\n",
    "            test_pred_tbn.append(model.predict(test_tbn[tabnet_feats].values).flatten())\n",
    "\n",
    "        elif INFERENCE==True:\n",
    "            input_path = os.path.join(BASE_MODEL_PATH, f'TabNet_fold{fold}')\n",
    "            output_filename = f'TabNet_fold{fold}.zip'\n",
    "            zip_directory(input_path, output_filename)\n",
    "            model = TabNetRegressor()\n",
    "            model.load_model(output_filename)\n",
    "            test_pred_tbn.append(model.predict(test_tbn[tabnet_feats].values).flatten())\n",
    "\n",
    "    return oof_pred_tbn, test_pred_tbn\n",
    "\n",
    "oof_pred_tbn, test_pred_tbn = train_and_eval_tabnet()\n",
    "print(f'OOF score across folds: {rmspe(y, oof_pred_tbn)}')\n",
    "\n",
    "del train_tbn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4ea66",
   "metadata": {
    "_cell_guid": "3a5b01e7-f1e4-4733-a1d5-9a268e48199a",
    "_uuid": "fcfe0db4-8c5c-4333-aa4c-452e06b401dc",
    "papermill": {
     "duration": 0.058159,
     "end_time": "2021-09-27T16:22:17.458693",
     "exception": false,
     "start_time": "2021-09-27T16:22:17.400534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8162025",
   "metadata": {
    "_cell_guid": "d386aaa6-f2be-45f0-8813-24a589f4b14e",
    "_uuid": "1d22685e-0118-415c-9e97-aa594664c883",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:22:17.584086Z",
     "iopub.status.busy": "2021-09-27T16:22:17.582499Z",
     "iopub.status.idle": "2021-09-27T16:22:17.641691Z",
     "shell.execute_reply": "2021-09-27T16:22:17.641014Z",
     "shell.execute_reply.started": "2021-09-26T10:21:39.505034Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.12531,
     "end_time": "2021-09-27T16:22:17.641851",
     "exception": false,
     "start_time": "2021-09-27T16:22:17.516541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of meta test is (3, 9)\n"
     ]
    }
   ],
   "source": [
    "# combining oof set\n",
    "train_meta = pd.DataFrame(np.column_stack([oof_pred_lgb] + [oof for oof in oof_pred_nn_list] + [oof_pred_tbn]),\n",
    "                                columns=['lgb'] + [f'nn_layer{i}' for i in range(len(hidden_units_list))] + ['tabnet'])\n",
    "train_meta = pd.concat([train_keys, train_meta], axis=1).reset_index(drop=True)\n",
    "    \n",
    "# combining test set    \n",
    "test_pred_lgb = pd.DataFrame(np.mean(np.column_stack(test_pred_lgb), axis=1), columns=['lgb'])\n",
    "test_pred_nn = [pd.DataFrame(np.mean(np.column_stack(test_pred_nn_list[i]), axis=1), columns=[f'nn_layer{i}']) for i in range(len(hidden_units_list))]\n",
    "test_pred_tbn = pd.DataFrame(np.mean(np.column_stack(test_pred_tbn), axis=1), columns=['tabnet'])\n",
    "test_meta = pd.concat([test_keys] + [test_pred_lgb] + test_pred_nn + [test_pred_tbn], axis=1).reset_index(drop=True)\n",
    "print(f'Shape of meta test is {test_meta.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db0a1215",
   "metadata": {
    "_cell_guid": "e81a6ab9-2800-4cb1-974f-5e51cece8372",
    "_uuid": "81fae498-1d68-448a-a0ef-8baf622d31bd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:22:17.776423Z",
     "iopub.status.busy": "2021-09-27T16:22:17.772160Z",
     "iopub.status.idle": "2021-09-27T16:23:06.181307Z",
     "shell.execute_reply": "2021-09-27T16:23:06.180180Z",
     "shell.execute_reply.started": "2021-09-26T10:21:39.614845Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 48.480329,
     "end_time": "2021-09-27T16:23:06.181441",
     "exception": false,
     "start_time": "2021-09-27T16:22:17.701112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train correlated stocks mean prediciton is (428932, 8)\n",
      "Shape of test correlated stocks mean prediciton is (0, 8)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Getting predictions of correlated stocks\n",
    "'''\n",
    "# correlation based on realized volatility (prediction target)\n",
    "def get_sxt_corr_stock_mapping(data, metric, n_top, log_transform, show_distance):\n",
    "    n_top = min(len(data.stock_id.unique())-1, n_top)\n",
    "    # calculate correlations\n",
    "    if log_transform==False:\n",
    "        corr = pd.pivot_table(data, values=metric, index='time_id', columns='stock_id', aggfunc=np.sum).corr()\n",
    "    elif log_transform==True:\n",
    "        corr = np.log(pd.pivot_table(data, values=metric, index='time_id', columns='stock_id', aggfunc=np.sum)).corr()\n",
    "    # compile mapping table\n",
    "    mapping = []\n",
    "    for stock_id in corr.columns:\n",
    "        df = pd.DataFrame({'nearest_stocks':corr[stock_id].sort_values(ascending=False)[1:n_top+1].index.tolist()})\n",
    "        if show_distance==True:\n",
    "            df['nearest_stocks_corr'] = corr[stock_id].sort_values(ascending=False)[1:n_top+1].tolist()\n",
    "        df['stock_id'] = stock_id\n",
    "        mapping.append(df)\n",
    "    mapping = pd.concat(mapping, axis=0).reset_index(drop=True)\n",
    "    return mapping\n",
    "\n",
    "# generate mapping table\n",
    "target = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n",
    "corr_stock_mapping = get_sxt_corr_stock_mapping(data=target, metric='target', n_top=5, log_transform=True, show_distance=False)\n",
    "\n",
    "# cross join with time_id\n",
    "train_id_list = train_meta[['stock_id','time_id']].drop_duplicates()\n",
    "corr_stock_mapping_train = pd.merge(corr_stock_mapping, train_id_list, how='inner',on='stock_id')[['stock_id','time_id','nearest_stocks']]\n",
    "test_id_list = test_meta[['stock_id','time_id']].drop_duplicates()\n",
    "corr_stock_mapping_test = pd.merge(corr_stock_mapping, test_id_list, how='inner',on='stock_id')[['stock_id','time_id','nearest_stocks']]\n",
    "\n",
    "# calculate mean predictions of correlated stocks\n",
    "corr_stock_pred_train = pd.merge(corr_stock_mapping_train, \n",
    "                                 train_meta.drop(['row_id','target'], axis=1).rename(columns={'stock_id':'nearest_stocks'}), \n",
    "                                 how='inner', \n",
    "                                 on=['nearest_stocks','time_id']).\\\n",
    "                        groupby(['stock_id','time_id'])[['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']].\\\n",
    "                        mean().\\\n",
    "                        reset_index()\n",
    "corr_stock_pred_train.columns = ['stock_id','time_id'] + [f'{c}_corr' for c in ['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']]\n",
    "print(f'Shape of train correlated stocks mean prediciton is {corr_stock_pred_train.shape}')\n",
    "corr_stock_pred_test = pd.merge(corr_stock_mapping_test, \n",
    "                                 test_meta.drop(['row_id'], axis=1).rename(columns={'stock_id':'nearest_stocks'}), \n",
    "                                 how='inner', \n",
    "                                 on=['nearest_stocks','time_id']).\\\n",
    "                        groupby(['stock_id','time_id'])[['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']].\\\n",
    "                        mean().\\\n",
    "                        reset_index()\n",
    "corr_stock_pred_test.columns = ['stock_id','time_id'] + [f'{c}_corr' for c in ['lgb','nn_layer0','nn_layer1','nn_layer2','nn_layer3','tabnet']]\n",
    "print(f'Shape of test correlated stocks mean prediciton is {corr_stock_pred_test.shape}')\n",
    "\n",
    "# add to prediction table\n",
    "train_meta = pd.merge(train_meta, corr_stock_pred_train, how='left', on=['stock_id','time_id'])\n",
    "test_meta = pd.merge(test_meta, corr_stock_pred_test, how='left', on=['stock_id','time_id'])\n",
    "# fillna (in case)\n",
    "train_meta = train_meta.fillna(train_meta.mean()).fillna(0)\n",
    "test_meta = test_meta.fillna(test_meta.mean()).fillna(0)\n",
    "# release memory\n",
    "del corr_stock_mapping, corr_stock_mapping_train, corr_stock_mapping_test, corr_stock_pred_train, corr_stock_pred_test\n",
    "gc.collect()\n",
    "# save data\n",
    "if INFERENCE==False:\n",
    "    train_meta.to_csv('train_meta.csv', index=False)\n",
    "    test_meta.to_csv('test_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd8f4c",
   "metadata": {
    "_cell_guid": "f2426b64-1f16-4163-aba2-30dd04524af8",
    "_uuid": "90443c09-9d29-4ece-83f3-9023fda0b500",
    "papermill": {
     "duration": 0.09774,
     "end_time": "2021-09-27T16:23:06.362528",
     "exception": false,
     "start_time": "2021-09-27T16:23:06.264788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9906f30",
   "metadata": {
    "_cell_guid": "cadac01c-9988-4dcf-a5b3-69684adcbf71",
    "_uuid": "64249add-4a41-4958-8018-7780c3b0c746",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:23:06.576510Z",
     "iopub.status.busy": "2021-09-27T16:23:06.575692Z",
     "iopub.status.idle": "2021-09-27T16:23:06.582950Z",
     "shell.execute_reply": "2021-09-27T16:23:06.583646Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.811284Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.116723,
     "end_time": "2021-09-27T16:23:06.583837",
     "exception": false,
     "start_time": "2021-09-27T16:23:06.467114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 0 ns, total: 18 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_params = dict(n_estimators = 80,\n",
    "                max_depth = 9,\n",
    "                min_samples_split = 14,\n",
    "                min_samples_leaf = 32,\n",
    "                max_features = 'sqrt',\n",
    "                max_samples = 0.45,\n",
    "                criterion='mse',\n",
    "                random_state=SEED)\n",
    "\n",
    "def train_and_eval_meta_rf():\n",
    "    if INFERENCE==False:\n",
    "        # model training\n",
    "        train_weights = 1 / np.square(train_meta.target)\n",
    "        model = RandomForestRegressor(**rf_params)\n",
    "        model.fit(X=train_meta[[c for c in train_meta if c not in train_key_cols]],\n",
    "                  y=train_meta.target,\n",
    "                  sample_weight=train_weights)\n",
    "        pickle.dump(model, open(f'meta_rf.p', 'wb'))\n",
    "    else:\n",
    "        model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'meta_rf.p'), 'rb'))\n",
    "    # predictions\n",
    "    train_pred_meta = model.predict(train_meta[[c for c in train_meta if c not in train_key_cols]])\n",
    "    test_pred_meta = model.predict(test_meta[[c for c in test_meta if c not in train_key_cols]])\n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = train_and_eval_meta_rf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25416b41",
   "metadata": {
    "_cell_guid": "6dea2602-5761-4c26-8c64-22a4b53b9502",
    "_uuid": "38be1720-f07b-4b4b-aad9-89323f559a8c",
    "papermill": {
     "duration": 0.095497,
     "end_time": "2021-09-27T16:23:06.777430",
     "exception": false,
     "start_time": "2021-09-27T16:23:06.681933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 2: Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcccff83",
   "metadata": {
    "_cell_guid": "3daeb4a7-18d6-43ee-a0f0-b5636d1cecd3",
    "_uuid": "f2dbe366-837b-4d12-9763-7850acded4ce",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:23:06.908725Z",
     "iopub.status.busy": "2021-09-27T16:23:06.907227Z",
     "iopub.status.idle": "2021-09-27T16:23:06.911431Z",
     "shell.execute_reply": "2021-09-27T16:23:06.910804Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.823807Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.070677,
     "end_time": "2021-09-27T16:23:06.911595",
     "exception": false,
     "start_time": "2021-09-27T16:23:06.840918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 s, sys: 0 ns, total: 13 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def train_and_eval_meta_ols():\n",
    "    if INFERENCE==False:\n",
    "        # model training\n",
    "        train_weights = 1 / np.square(train_meta.target)\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model.fit(X=train_meta[[c for c in train_meta if c not in train_key_cols]],\n",
    "                  y=train_meta.target,\n",
    "                  sample_weight=train_weights)\n",
    "        print(f'Fitted coefficients are {model.coef_}')\n",
    "        pickle.dump(model, open(f'meta_ols.p', 'wb'))\n",
    "    else:\n",
    "        model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'meta_ols.p'), 'rb'))\n",
    "    # predictions\n",
    "    train_pred_meta = model.predict(train_meta[[c for c in train_meta if c not in train_key_cols]])\n",
    "    test_pred_meta = model.predict(test_meta[[c for c in test_meta if c not in train_key_cols]])\n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = train_and_eval_meta_ols()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5def2f0",
   "metadata": {
    "_cell_guid": "f0c90581-0e6e-42e6-9918-d964bf8d650f",
    "_uuid": "fd967e79-d50a-4c51-8cff-8d4c7d94c245",
    "papermill": {
     "duration": 0.059207,
     "end_time": "2021-09-27T16:23:07.031487",
     "exception": false,
     "start_time": "2021-09-27T16:23:06.972280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 3: Equal Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60d1c090",
   "metadata": {
    "_cell_guid": "33438b09-d1d1-4e0f-9e6f-6f7f28587f25",
    "_uuid": "da18abca-0a62-4e56-b438-82d6760424bf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:23:07.155528Z",
     "iopub.status.busy": "2021-09-27T16:23:07.154947Z",
     "iopub.status.idle": "2021-09-27T16:23:07.159285Z",
     "shell.execute_reply": "2021-09-27T16:23:07.159847Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.83804Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.069298,
     "end_time": "2021-09-27T16:23:07.160030",
     "exception": false,
     "start_time": "2021-09-27T16:23:07.090732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 s, sys: 1 s, total: 5 s\n",
      "Wall time: 8.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def equal_weight_meta():\n",
    "    nn_cols = [c for c in test_meta if 'nn' in c and c not in train_key_cols]    \n",
    "    train_pred_meta = (train_meta[nn_cols].mean(axis=1) + train_meta['lgb'] + train_meta['tabnet']) / 3\n",
    "    test_pred_meta = (test_meta[nn_cols].mean(axis=1) + test_meta['lgb'] + test_meta['tabnet']) / 3     \n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = equal_weight_meta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec453c",
   "metadata": {
    "_cell_guid": "e81d03ce-d69b-48a6-9f22-33f4d6ad7472",
    "_uuid": "70a93040-97c5-4951-acb4-29d95ba9f349",
    "papermill": {
     "duration": 0.059525,
     "end_time": "2021-09-27T16:23:07.279964",
     "exception": false,
     "start_time": "2021-09-27T16:23:07.220439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 4: Weighted by CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81e40162",
   "metadata": {
    "_cell_guid": "c90b96a0-9551-492d-8a0d-1b64d3b579e7",
    "_uuid": "7115b18f-80af-4cac-8b9d-a205c7c06b5d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:23:07.409416Z",
     "iopub.status.busy": "2021-09-27T16:23:07.408877Z",
     "iopub.status.idle": "2021-09-27T16:23:07.413136Z",
     "shell.execute_reply": "2021-09-27T16:23:07.412533Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.849808Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.073551,
     "end_time": "2021-09-27T16:23:07.413250",
     "exception": false,
     "start_time": "2021-09-27T16:23:07.339699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 s, sys: 0 ns, total: 5 s\n",
      "Wall time: 8.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def cv_score_weighting():\n",
    "    if INFERENCE==False:\n",
    "        # initialize cv score tables\n",
    "        base_cv = []\n",
    "        nn_cv = []\n",
    "        # calcualte OOF score of each model\n",
    "        base_cv.append(('lgb', rmspe(train_keys['target'], oof_pred_lgb)))\n",
    "        for i in range(len(hidden_units_list)):\n",
    "            nn_cv.append((f'nn_layer{i}', rmspe(train_keys['target'], oof_pred_nn_list[i])))\n",
    "        base_cv.append(('tabnet', rmspe(train_keys['target'], oof_pred_tbn)))\n",
    "        # transform list to table\n",
    "        base_cv = pd.DataFrame(base_cv, columns=['model','cv'])\n",
    "        nn_cv = pd.DataFrame(nn_cv, columns=['model','cv'])\n",
    "        # define contrast parameter\n",
    "        k_nn = 0.015\n",
    "        k_base = 0.2\n",
    "        # calculate NN weights\n",
    "        nn_cv['imp'] = np.exp((1/k_nn) * 1 / nn_cv['cv'])\n",
    "        nn_cv['weight'] = nn_cv['imp'] / np.sum(nn_cv['imp'])\n",
    "        nn_avg_cv = pd.DataFrame({'model':['nn'], 'cv':[np.sum(np.multiply(nn_cv.cv, nn_cv.weight))]})\n",
    "        # calculate base model weight (including the overall NN)\n",
    "        base_cv = pd.concat([base_cv, nn_avg_cv], axis=0).reset_index(drop=True)\n",
    "        base_cv['imp'] = np.exp((1/k_base) * 1 / base_cv['cv'])\n",
    "        base_cv['weight'] = base_cv['imp'] / np.sum(base_cv['imp'])\n",
    "        # derive final weight for all models\n",
    "        nn_cv['weight'] = nn_cv['weight'] * float(base_cv[base_cv.model=='nn']['weight'])\n",
    "        nn_cv = nn_cv[['model','weight']]\n",
    "        base_cv = base_cv[base_cv.model!='nn'][['model','weight']]\n",
    "        base_cv = pd.concat([base_cv, nn_cv], axis=0).reset_index(drop=True)\n",
    "        model_weights = dict(base_cv.to_records(index=False))\n",
    "        pickle.dump(model_weights, open(f'model_weights.p', 'wb'))\n",
    "    else:\n",
    "        model_weights = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'model_weights.p'), 'rb'))\n",
    "    # make predictions\n",
    "    train_pred_meta = sum([train_meta[m] * model_weights[m] for m in model_weights])\n",
    "    test_pred_meta = sum([test_meta[m] * model_weights[m] for m in model_weights])  \n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "# test_pred_meta, train_pred_meta = cv_score_weighting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87f17b",
   "metadata": {
    "_cell_guid": "d5c24a25-5d8a-42d9-9606-8110b2dd9d58",
    "_uuid": "9a5f09df-ee80-4d33-88ab-a65dbe3d1708",
    "papermill": {
     "duration": 0.15165,
     "end_time": "2021-09-27T16:23:07.625239",
     "exception": false,
     "start_time": "2021-09-27T16:23:07.473589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble method 5: ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f98f3ef6",
   "metadata": {
    "_cell_guid": "f0b7827c-008e-43d1-9361-a041a5e52494",
    "_uuid": "bbf35589-d5d9-4ec5-9aa3-f48362c1e391",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:23:07.824205Z",
     "iopub.status.busy": "2021-09-27T16:23:07.823440Z",
     "iopub.status.idle": "2021-09-27T16:23:07.862328Z",
     "shell.execute_reply": "2021-09-27T16:23:07.863304Z",
     "shell.execute_reply.started": "2021-09-26T10:22:39.86472Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.143235,
     "end_time": "2021-09-27T16:23:07.863509",
     "exception": false,
     "start_time": "2021-09-27T16:23:07.720274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 ms, sys: 940 s, total: 27.4 ms\n",
      "Wall time: 33.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import ElasticNet\n",
    "def train_and_eval_meta_elasticnet():\n",
    "    if INFERENCE==False:\n",
    "        # model training\n",
    "        train_weights = 1 / np.square(train_meta.target)\n",
    "        model = ElasticNet(fit_intercept=False, alpha=1e-10, l1_ratio=0, positive=False, random_state=SEED, max_iter=10000)\n",
    "        model.fit(X=train_meta[[c for c in train_meta if c not in train_key_cols]],\n",
    "                  y=train_meta.target,\n",
    "                  sample_weight=train_weights)\n",
    "        print(f'Fitted coefficients are {model.coef_}')\n",
    "        pickle.dump(model, open(f'meta_elasticnet.p', 'wb'))\n",
    "    else:\n",
    "        model = pickle.load(open(os.path.join(BASE_MODEL_PATH, f'meta_elasticnet.p'), 'rb'))\n",
    "    # predictions\n",
    "    train_pred_meta = model.predict(train_meta[[c for c in train_meta if c not in train_key_cols]])\n",
    "    test_pred_meta = model.predict(test_meta[[c for c in test_meta if c not in train_key_cols]])\n",
    "    return test_pred_meta, train_pred_meta\n",
    "\n",
    "test_pred_meta, train_pred_meta = train_and_eval_meta_elasticnet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be260404",
   "metadata": {
    "_cell_guid": "6ff27223-2917-4903-a6e4-e73bb4f3ce22",
    "_uuid": "3784ae84-c6fd-47b8-8f61-1387ae612065",
    "papermill": {
     "duration": 0.10846,
     "end_time": "2021-09-27T16:23:08.107190",
     "exception": false,
     "start_time": "2021-09-27T16:23:07.998730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bfd4785",
   "metadata": {
    "_cell_guid": "cf0a23eb-c5e9-4392-a43c-2236892fa84c",
    "_uuid": "e103453d-c766-4f09-b66e-c555521f036f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:23:08.326161Z",
     "iopub.status.busy": "2021-09-27T16:23:08.319034Z",
     "iopub.status.idle": "2021-09-27T16:23:08.517177Z",
     "shell.execute_reply": "2021-09-27T16:23:08.518338Z",
     "shell.execute_reply.started": "2021-09-26T10:22:40.804706Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.306521,
     "end_time": "2021-09-27T16:23:08.518578",
     "exception": false,
     "start_time": "2021-09-27T16:23:08.212057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000105263 0.07032062\n"
     ]
    }
   ],
   "source": [
    "# clipping\n",
    "target = pd.read_csv('/kaggle/input/optiver-realized-volatility-prediction/train.csv')\n",
    "min_target, max_target = target.target.min(), target.target.max()\n",
    "# min_target, max_target = 0, target.target.max()\n",
    "print(min_target, max_target)\n",
    "\n",
    "oof_pred_lgb = np.clip(oof_pred_lgb, min_target, max_target)\n",
    "for i in range(len(hidden_units_list)):\n",
    "    oof_pred_nn_list[i] = np.clip(oof_pred_nn_list[i], min_target, max_target)\n",
    "oof_pred_tbn = np.clip(oof_pred_tbn, min_target, max_target)\n",
    "train_pred_meta = np.clip(train_pred_meta, min_target, max_target)\n",
    "test_pred_meta = np.clip(test_pred_meta, min_target, max_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c0b04",
   "metadata": {
    "_cell_guid": "268d3d50-4fd2-4919-8872-76142d18861a",
    "_uuid": "27432bb1-1c76-4d69-a0ac-7fad218aa8b7",
    "papermill": {
     "duration": 0.084837,
     "end_time": "2021-09-27T16:23:08.704776",
     "exception": false,
     "start_time": "2021-09-27T16:23:08.619939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final score and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "729ed881",
   "metadata": {
    "_cell_guid": "d6014056-3ba4-42d3-9362-162e02993785",
    "_uuid": "bccd563b-319f-42f1-9d92-9c5e2bd65495",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T16:23:08.837087Z",
     "iopub.status.busy": "2021-09-27T16:23:08.836228Z",
     "iopub.status.idle": "2021-09-27T16:23:08.850996Z",
     "shell.execute_reply": "2021-09-27T16:23:08.850517Z",
     "shell.execute_reply.started": "2021-09-26T10:22:40.988509Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.085822,
     "end_time": "2021-09-27T16:23:08.851123",
     "exception": false,
     "start_time": "2021-09-27T16:23:08.765301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.001659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001644\n",
       "1   0-32  0.001659\n",
       "2   0-34  0.001659"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluation\n",
    "if INFERENCE==False:\n",
    "    # LightGBM\n",
    "    lgb_cv_score = round(rmspe(train_keys['target'], oof_pred_lgb), 5)\n",
    "    print(f'LGBM CV score is {lgb_cv_score}')\n",
    "    # NN\n",
    "    for i in range(len(hidden_units_list)):\n",
    "        cv_score = round(rmspe(train_keys['target'], oof_pred_nn_list[i]), 5)\n",
    "        print(f'NN{i} CV score is {cv_score}')\n",
    "    # TabNet\n",
    "    tbn_cv_score = round(rmspe(train_keys['target'], oof_pred_tbn), 5)\n",
    "    print(f'TabNet CV score is {tbn_cv_score}')\n",
    "    # Ensembled\n",
    "    meta_cv_score = round(rmspe(train_keys['target'], train_pred_meta), 5)\n",
    "    print(f'Meta Model CV score is {meta_cv_score}')\n",
    "\n",
    "\n",
    "# submission\n",
    "test['target'] = test_pred_meta\n",
    "test = test[['row_id', 'target']].reset_index(drop=True)\n",
    "test.to_csv('submission.csv', index=False)\n",
    "display(test.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 368.853677,
   "end_time": "2021-09-27T16:23:12.475238",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T16:17:03.621561",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
