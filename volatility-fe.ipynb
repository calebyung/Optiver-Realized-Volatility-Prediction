{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85eac7d7",
   "metadata": {
    "_cell_guid": "e33b644e-a4f4-496d-9e19-dc85496ca829",
    "_uuid": "63b5d5d4-508a-40d8-abd6-4c2dcbfdfab1",
    "papermill": {
     "duration": 0.023252,
     "end_time": "2021-09-27T03:58:37.847673",
     "exception": false,
     "start_time": "2021-09-27T03:58:37.824421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Change Log\n",
    "* Version 1: Copied from original notebook\n",
    "* Version 2: train_nn created here; added holdout set\n",
    "* Version 3: Moved the creation of holdout set to training notebook\n",
    "* Version 5: Cleaned for only test processing\n",
    "* Version 6: Added min max scaler for NN dataset\n",
    "* Version 7:\n",
    "    - Prepare TabNet data inside this notebook\n",
    "    - Design synthetic test set for debugging submission scoring error\n",
    "* Version 8: Stock clustering (categorical column)\n",
    "* Version 10: NN label encoding for embedding\n",
    "* Version 12: Time clustering (categorical column)\n",
    "* Version 13: Time clustering - only log transform positive columns\n",
    "* Version 14: Added velocity features\n",
    "* Version 15: Added Time Series AutoEncoding features\n",
    "* Version 18: Error handling re kaggle staff reminder (ERROR / unused)\n",
    "* Version 20: Removed >0.99 correlated features (ERROR / unused)\n",
    "* Version 23: Error handling re kaggle staff reminder\n",
    "* Version 24: Removed >0.99 correlated features\n",
    "   \n",
    "\n",
    "### To do list:\n",
    "* Add skew and kurt and add to stock level aggregation\n",
    "* Add velocity\n",
    "* Wavelength and Amplitudes\n",
    "* Time clustering?\n",
    "* TS clustering by AE\n",
    "* Prepare TabNet data inside this notebook\n",
    "* Time features - only log-transform non-negative columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8780a698",
   "metadata": {
    "_cell_guid": "14774c2d-3cd3-43fc-a95a-65ddd56f1194",
    "_uuid": "b0465334-ebba-437b-a19d-e6818d65a21c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T03:58:37.906907Z",
     "iopub.status.busy": "2021-09-27T03:58:37.906242Z",
     "iopub.status.idle": "2021-09-27T03:58:37.909849Z",
     "shell.execute_reply": "2021-09-27T03:58:37.909148Z",
     "shell.execute_reply.started": "2021-09-27T03:41:49.596997Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.037548,
     "end_time": "2021-09-27T03:58:37.910025",
     "exception": false,
     "start_time": "2021-09-27T03:58:37.872477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "TEST_MODE = 'test'\n",
    "FE_PATH = '/kaggle/input/volatility-fe-output-version-15'\n",
    "N_STOCK = 112\n",
    "SEED = 1111\n",
    "\n",
    "if TEST_MODE=='test':\n",
    "    data_path = 'optiver-realized-volatility-prediction'\n",
    "elif TEST_MODE=='syn':\n",
    "    data_path = 'optiver-realized-volatility-prediction-synthetic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b824906",
   "metadata": {
    "_cell_guid": "6f366467-ef5b-43dc-8af1-f156b3396370",
    "_uuid": "9752fd13-c84c-4edf-94c1-9789e1c855de",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T03:58:37.960905Z",
     "iopub.status.busy": "2021-09-27T03:58:37.960219Z",
     "iopub.status.idle": "2021-09-27T03:58:39.103352Z",
     "shell.execute_reply": "2021-09-27T03:58:39.102776Z",
     "shell.execute_reply.started": "2021-09-27T03:41:49.604476Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.171743,
     "end_time": "2021-09-27T03:58:39.103511",
     "exception": false,
     "start_time": "2021-09-27T03:58:37.931768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc24b4dd",
   "metadata": {
    "_cell_guid": "e589f948-7393-43fc-9fc7-d360a13a38f3",
    "_uuid": "d691f7ce-cf03-411b-85e2-fe1d9df6d58a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T03:58:39.152836Z",
     "iopub.status.busy": "2021-09-27T03:58:39.152138Z",
     "iopub.status.idle": "2021-09-27T03:58:39.155396Z",
     "shell.execute_reply": "2021-09-27T03:58:39.154857Z",
     "shell.execute_reply.started": "2021-09-27T03:41:49.619259Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028732,
     "end_time": "2021-09-27T03:58:39.155537",
     "exception": false,
     "start_time": "2021-09-27T03:58:39.126805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'a':[1,2,3,4,5], 'b':['a','a','b','b','c']})\n",
    "# df.groupby('b')['a'].agg(lambda s:s.max()-s.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988249cc",
   "metadata": {
    "_cell_guid": "2b00464a-d9b9-42e7-8b1e-e0657745e4fe",
    "_uuid": "62f93b87-5d05-41f4-a56a-e24dea3c2f72",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T03:58:39.203641Z",
     "iopub.status.busy": "2021-09-27T03:58:39.202610Z",
     "iopub.status.idle": "2021-09-27T03:58:39.273110Z",
     "shell.execute_reply": "2021-09-27T03:58:39.273720Z",
     "shell.execute_reply.started": "2021-09-27T03:41:49.716397Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.096264,
     "end_time": "2021-09-27T03:58:39.273904",
     "exception": false,
     "start_time": "2021-09-27T03:58:39.177640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fill inf and nan of  a dataframe\n",
    "def fill_inf_nan(df):\n",
    "    df = df.replace([np.inf, -np.inf], np.nan) # replace Inf with NaN\n",
    "    df = df.fillna(df.mean()) # replace NaN with mean\n",
    "    df = df.fillna(0) # if there are still na, fill with zero\n",
    "    return df\n",
    "\n",
    "# function to load a book/trade train/test single stock file\n",
    "def load_single_stock(stock_id, train_test, book_trade):\n",
    "    path = f'/kaggle/input/{data_path}/{book_trade}_{train_test}.parquet/stock_id={str(stock_id)}'\n",
    "    filename = os.path.join(path, os.listdir(path)[0])\n",
    "    return pd.read_parquet(filename)\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "def calc_wap2(df):\n",
    "    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "def calc_wap3(df):\n",
    "    return (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "def calc_wap4(df):\n",
    "    return (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# function to return range of a series\n",
    "def min_max_range(s):\n",
    "    return s.max() - s.min()\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv(f'../input/{data_path}/train.csv')\n",
    "    test = pd.read_csv(f'../input/{data_path}/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    # Moving Avereage features\n",
    "    df['log_return1_sma'] = df.groupby('time_id')['log_return1'].apply(lambda s: s.rolling(window=50).mean())\n",
    "    df['log_return1_sms'] = df.groupby(['time_id'])['log_return1'].apply(lambda s: s.rolling(window=50).std())\n",
    "    df['log_return1_sma_diff'] = df.groupby('time_id')['log_return1'].apply(lambda s: s.rolling(window=20).mean()) - df.groupby('time_id')['log_return1'].apply(lambda s: s.rolling(window=50).mean())\n",
    "    df['log_return1_sms_diff'] = df.groupby(['time_id'])['log_return1'].apply(lambda s: s.rolling(window=20).std()) - df.groupby(['time_id'])['log_return1'].apply(lambda s: s.rolling(window=50).std())\n",
    "\n",
    "    # Dict for aggregations; These are the features that only exist in full seconds range (000-599)\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "        'log_return1_sma':['last', min_max_range],\n",
    "        'log_return1_sms':['last', min_max_range],\n",
    "        'log_return1_sma_diff':['last', min_max_range],\n",
    "        'log_return1_sms_diff':['last', min_max_range],\n",
    "    }\n",
    "    # These are the features that will be replicated for last 500, 400, 300, 200, 100 seconds\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'total_volume': ['sum','max'],\n",
    "        'volume_imbalance': ['sum','max']\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    # These are the features to be replicated for 500, 400,...100 seconds filter\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    # Create feautres for Tendency and Energy (?); Total 11 features applied to only full seconds (000-599)\n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        # vol vars\n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    df_lr = pd.DataFrame(lis)\n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all seconds filter\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    stock_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility','trade_log_return_realized_volatility']\n",
    "\n",
    "    time_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200',\n",
    "                'log_return1_sma_last','log_return1_sma_diff_last','log_return1_sms_last','log_return1_sms_diff_last']\n",
    "    \n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[stock_cols].agg(['mean','min','max','std']).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the time id\n",
    "    df_time_id = df.groupby(['time_id'])[time_cols].agg(['mean','min','max','std']).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "\n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, mode='train'):\n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        print(f'Processing stock {stock_id}')\n",
    "        # Train\n",
    "        file_path_book = f'/kaggle/input/{data_path}/book_{mode}.parquet/stock_id={stock_id}'\n",
    "        file_path_trade = f'/kaggle/input/{data_path}/trade_{mode}.parquet/stock_id={stock_id}'\n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        return df_tmp\n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "914bfc6d",
   "metadata": {
    "_cell_guid": "3be91fea-6acf-49c2-885a-fb92f4d10d93",
    "_uuid": "9dc28e42-62f6-4d43-b3f1-9ae5e6a23eaf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T03:58:39.320741Z",
     "iopub.status.busy": "2021-09-27T03:58:39.320036Z",
     "iopub.status.idle": "2021-09-27T04:51:29.253149Z",
     "shell.execute_reply": "2021-09-27T04:51:29.252362Z",
     "shell.execute_reply.started": "2021-09-27T03:41:49.781530Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3169.957476,
     "end_time": "2021-09-27T04:51:29.253343",
     "exception": false,
     "start_time": "2021-09-27T03:58:39.295867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 52.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 3.96 s, total: 17.6 s\n",
      "Wall time: 52min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Create all basic features\n",
    "'''\n",
    "# Create basic features for train data\n",
    "if TRAIN==True:\n",
    "    train, test = read_train_test()\n",
    "    train_stock_ids = train['stock_id'].unique()[:N_STOCK]\n",
    "    train = train.merge(preprocessor(train_stock_ids, mode='train'), on = ['row_id'], how = 'left')\n",
    "    key_cols = ['stock_id','time_id','row_id','target']\n",
    "    feat_cols = [col for col in train.columns if col not in key_cols]\n",
    "    train[feat_cols] = train[feat_cols].fillna(train[feat_cols].mean()).fillna(0)\n",
    "    train = get_time_stock(train)\n",
    "else:\n",
    "    train = pd.read_feather(os.path.join(FE_PATH, 'train.f'))\n",
    "\n",
    "# Create basic features for test data\n",
    "_ , test = read_train_test()\n",
    "test_stock_ids = test['stock_id'].unique()[:N_STOCK]\n",
    "test = test.merge(preprocessor(test_stock_ids, mode='test'), on=['row_id'], how='left')\n",
    "key_cols = ['stock_id','time_id','row_id','target']\n",
    "feat_cols = [col for col in test.columns if col not in key_cols]\n",
    "test[feat_cols] = test[feat_cols].fillna(test[feat_cols].mean()).fillna(0)\n",
    "test = get_time_stock(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7536c9",
   "metadata": {
    "_cell_guid": "854ce2b2-fe30-44f1-96c1-df66dfa771a5",
    "_uuid": "b790d6ba-11a9-4e48-84f3-d8cf8c888d5e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T04:51:29.320242Z",
     "iopub.status.busy": "2021-09-27T04:51:29.319032Z",
     "iopub.status.idle": "2021-09-27T05:17:02.305268Z",
     "shell.execute_reply": "2021-09-27T05:17:02.306114Z",
     "shell.execute_reply.started": "2021-09-27T03:41:54.328777Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1533.028874,
     "end_time": "2021-09-27T05:17:02.306621",
     "exception": false,
     "start_time": "2021-09-27T04:51:29.277747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-27 04:51:33.136873] Added 0 stocks time series.\n",
      "[2021-09-27 04:52:44.490979] Added 20 stocks time series.\n",
      "[2021-09-27 04:53:54.329212] Added 40 stocks time series.\n",
      "[2021-09-27 04:55:04.826318] Added 60 stocks time series.\n",
      "[2021-09-27 04:56:15.226218] Added 80 stocks time series.\n",
      "[2021-09-27 04:57:25.912532] Added 100 stocks time series.\n",
      "[2021-09-27 04:58:05.818057] Total time series size is 181 MB\n",
      "[2021-09-27 04:58:06.112345] Completed correlation for 0 time_id.\n",
      "[2021-09-27 05:00:18.597769] Completed correlation for 500 time_id.\n",
      "[2021-09-27 05:02:31.494279] Completed correlation for 1000 time_id.\n",
      "[2021-09-27 05:04:45.353627] Completed correlation for 1500 time_id.\n",
      "[2021-09-27 05:06:57.383135] Completed correlation for 2000 time_id.\n",
      "[2021-09-27 05:09:09.653840] Completed correlation for 2500 time_id.\n",
      "[2021-09-27 05:11:24.569412] Completed correlation for 3000 time_id.\n",
      "[2021-09-27 05:13:37.538963] Completed correlation for 3500 time_id.\n",
      "[2021-09-27 05:17:02.239526] Added 0 stocks time series.\n",
      "[2021-09-27 05:17:02.243565] Total time series size is 0 MB\n",
      "[2021-09-27 05:17:02.250491] Completed correlation for 0 time_id.\n",
      "CPU times: user 25min 2s, sys: 59 s, total: 26min 1s\n",
      "Wall time: 25min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Adding correlated stocks features\n",
    "'''\n",
    "# function to get time series for all stocks for similarity calculation in next step\n",
    "def gen_knn_stock_data(train_test, n_stock, seconds_sample_interval):\n",
    "    # define parameters\n",
    "    stock_list = sorted([int(x.split('=')[1]) for x in os.listdir(f'/kaggle/input/{data_path}/book_{train_test}.parquet')])\n",
    "    stock_list = stock_list[:n_stock]\n",
    "    # setup base table as all combinations of time x seconds\n",
    "    time_id_list = pd.DataFrame({'time_id':sorted(load_single_stock(0, train_test, 'book').time_id.unique().tolist())})\n",
    "    seconds_in_bucket_list = pd.DataFrame({'seconds_in_bucket':range(600)})\n",
    "    base = time_id_list.merge(seconds_in_bucket_list, how='cross')\n",
    "    # loop through stocks to get time series data\n",
    "    ts = []\n",
    "    for stock_id in stock_list:\n",
    "        # load data\n",
    "        book = load_single_stock(stock_id, train_test, 'book')\n",
    "        # fill NA\n",
    "        book = base.merge(book, how='left', on=['time_id','seconds_in_bucket'])\n",
    "        book = book.ffill().bfill()  \n",
    "        # create series\n",
    "        book['wap1'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / ( book['bid_size1'] +  book['ask_size1'])\n",
    "        book['wap1_lr'] = np.log(book['wap1']).diff()\n",
    "        book['wap1_sms50'] = book.groupby('time_id')['wap1_lr'].apply(lambda s: s.rolling(window=50).std())\n",
    "        # filtering\n",
    "        idx = book.index.tolist()[::seconds_sample_interval]\n",
    "        book = book.iloc[idx,:]\n",
    "        book = book[book['wap1_sms50'].isnull()==False].reset_index(drop=True)\n",
    "        # centering\n",
    "        book['wap1_sms50'] = book['wap1_sms50'] - book['time_id'].map(book.groupby('time_id')['wap1_sms50'].median())\n",
    "        # merge with master table\n",
    "        book[stock_id] = book['wap1_sms50']\n",
    "        book = book[['time_id','seconds_in_bucket',stock_id]].set_index(['time_id','seconds_in_bucket'])\n",
    "        ts.append(book)\n",
    "        if stock_list.index(stock_id)%20==0:\n",
    "            print(f'[{datetime.datetime.now()}] Added {stock_list.index(stock_id)} stocks time series.')\n",
    "    ts = pd.concat(ts, axis=1).reset_index().drop('seconds_in_bucket', axis=1)\n",
    "    print(f'[{datetime.datetime.now()}] Total time series size is {int(ts.memory_usage().sum() / 1024**2)} MB')\n",
    "    return ts\n",
    "\n",
    "# function to top N correlated stocks per stock per time_id\n",
    "def gen_corr_stock_mapping(data, n_top, show_distance):\n",
    "    n_top = min(n_top, len(data.columns.tolist()[1:])-1)\n",
    "    time_id_list = data.time_id.unique().tolist() \n",
    "    mapping = []\n",
    "    for time_id in time_id_list:\n",
    "        corr = data[data.time_id==time_id].iloc[:,1:].corr().reset_index().rename(columns={'index':'corr_stock_id'})\n",
    "        for stock_id in corr.columns.tolist()[1:]:\n",
    "            df = pd.DataFrame({'nearest_stocks': corr[corr.corr_stock_id!=stock_id].sort_values(by=stock_id, ascending=False)['corr_stock_id'].tolist()[:n_top]})\n",
    "            if show_distance==True:\n",
    "                df['nearest_stocks_corr'] = corr[corr.corr_stock_id!=stock_id].sort_values(by=stock_id, ascending=False)[stock_id].tolist()[:n_top]\n",
    "            df['stock_id'] = stock_id\n",
    "            df['time_id'] = time_id\n",
    "            mapping.append(df)\n",
    "        if time_id_list.index(time_id)%500==0:\n",
    "            print(f'[{datetime.datetime.now()}] Completed correlation for {time_id_list.index(time_id)} time_id.')\n",
    "    mapping = pd.concat(mapping, axis=0)\n",
    "    mapping = mapping[['stock_id','time_id','nearest_stocks'] + [col for col in mapping.columns if col not in ['stock_id','time_id','nearest_stocks']]]\n",
    "    return mapping\n",
    "\n",
    "# generate new features based on correlated stocks\n",
    "def gen_corr_stock_feats(data, mapping, target_cols):\n",
    "    new_feats = pd.merge(data.rename(columns={'stock_id':'nearest_stocks'})[['nearest_stocks','time_id'] + target_cols],\n",
    "                        mapping, how='inner', on=['nearest_stocks','time_id']).\\\n",
    "                        groupby(['stock_id','time_id'])[target_cols].\\\n",
    "                        mean().\\\n",
    "                        reset_index()\n",
    "    new_feats.columns = ['stock_id','time_id'] + [f'{col}_corr' for col in new_feats.columns if col not in ['stock_id','time_id']]\n",
    "    data = pd.merge(data, new_feats, how='left', on=['stock_id','time_id'])\n",
    "    return data\n",
    "\n",
    "# correlated stock features\n",
    "target_cols = ['log_return1_realized_volatility','log_return1_realized_volatility_300','log_return1_realized_volatility_100',\n",
    "               'log_return1_sma_last','log_return1_sma_diff_last','log_return1_sms_last','log_return1_sms_diff_last',\n",
    "               'total_volume_sum','volume_imbalance_sum','trade_size_sum','price_spread_sum']\n",
    "\n",
    "if TRAIN==True:\n",
    "    corr_mapping_train = gen_corr_stock_mapping(gen_knn_stock_data(train_test='train', n_stock=N_STOCK, seconds_sample_interval=10), n_top=5, show_distance=False)\n",
    "    train = gen_corr_stock_feats(data=train, mapping=corr_mapping_train, target_cols=target_cols)\n",
    "\n",
    "corr_mapping_test = gen_corr_stock_mapping(gen_knn_stock_data(train_test='test', n_stock=N_STOCK, seconds_sample_interval=10), n_top=5, show_distance=False)\n",
    "test = gen_corr_stock_feats(data=test, mapping=corr_mapping_test, target_cols=target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e5875c",
   "metadata": {
    "_cell_guid": "0c134e46-51d2-46d8-89b3-ba4382e76cb7",
    "_uuid": "7ce4575d-a0d4-4d18-b9f2-255a5a7dba82",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:17:02.395057Z",
     "iopub.status.busy": "2021-09-27T05:17:02.389046Z",
     "iopub.status.idle": "2021-09-27T05:17:34.808083Z",
     "shell.execute_reply": "2021-09-27T05:17:34.807504Z",
     "shell.execute_reply.started": "2021-09-27T03:41:54.471508Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 32.47011,
     "end_time": "2021-09-27T05:17:34.808247",
     "exception": false,
     "start_time": "2021-09-27T05:17:02.338137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/tslearn052/tslearn-0.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (1.19.5)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (0.53.1)\r\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (0.29.23)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (1.6.3)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (1.0.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from tslearn==0.5.2) (0.23.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->tslearn==0.5.2) (49.6.0.post20210108)\r\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba->tslearn==0.5.2) (0.36.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->tslearn==0.5.2) (2.1.0)\r\n",
      "Installing collected packages: tslearn\r\n",
      "Successfully installed tslearn-0.5.2\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tslearn/clustering/kmeans.py:17: UserWarning: Scikit-learn <0.24 will be deprecated in a future release of tslearn\n",
      "  \"Scikit-learn <0.24 will be deprecated in a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-27 05:17:34.791585] Completed test data tagging for stock 0\n",
      "CPU times: user 2.58 s, sys: 698 ms, total: 3.27 s\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Adding features for Time Series shape clustering\n",
    "'''\n",
    "!pip install ../input/tslearn052/tslearn-0.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
    "from tslearn.clustering import TimeSeriesKMeans, KShape\n",
    "from tslearn.generators import random_walks\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "import math\n",
    "import random\n",
    "\n",
    "def preprocess_single_stock(stock_id, train_test, n_time_id_sample, seconds_sample_interval, remove_first_n_seconds, delete_unused_cols):\n",
    "    # load data\n",
    "    book = load_single_stock(stock_id, train_test, 'book')\n",
    "    # ffill and bfill\n",
    "    time_id_list =  pd.DataFrame({'time_id':book.time_id.unique().tolist()})\n",
    "    seconds_in_bucket_list = pd.DataFrame({'seconds_in_bucket':range(600)})\n",
    "    base = time_id_list.merge(seconds_in_bucket_list, how='cross')\n",
    "    book = base.merge(book, how='left', on=['time_id','seconds_in_bucket'])\n",
    "    book = book.ffill().bfill()\n",
    "    # sampling time_id\n",
    "    if n_time_id_sample >= 0:\n",
    "        time_id_samples = random.sample(book['time_id'].unique().tolist(), n_time_id_sample)\n",
    "        book = book[book.time_id.isin(time_id_samples)]\n",
    "    # create time series\n",
    "    book['wap1'] = (book['bid_price1'] * book['ask_size1'] + book['ask_price1'] * book['bid_size1']) / ( book['bid_size1'] +  book['ask_size1'])\n",
    "    book['wap1_sma50'] = book.groupby('time_id')['wap1'].apply(lambda s: s.rolling(window=50).mean())\n",
    "    book['wap1_lr'] = np.log(book['wap1']).diff()\n",
    "    book['wap1_sms50'] = book.groupby('time_id')['wap1_lr'].apply(lambda s: s.rolling(window=50).std())\n",
    "    book['total_volume'] = book['ask_size1'] + book['ask_size2'] + book['bid_size1'] + book['bid_size2']\n",
    "    book['total_volume'] = (book['total_volume'] - book['total_volume'].mean()) / book['total_volume'].std()\n",
    "    book['total_volume_sma60'] = book.groupby('time_id')['total_volume'].apply(lambda s: s.rolling(window=60).mean())\n",
    "    book['volume_imbalance'] = (book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2'])\n",
    "    book['volume_imbalance'] = (book['volume_imbalance'] - book['volume_imbalance'].mean()) / book['volume_imbalance'].std()\n",
    "    book['volume_imbalance_sma80'] = book.groupby('time_id')['volume_imbalance'].apply(lambda s: s.rolling(window=80).mean())\n",
    "    # remove first few entries to avoid NA\n",
    "    book = book[book.seconds_in_bucket >= remove_first_n_seconds].reset_index(drop=True)\n",
    "    # seconds interval filtering\n",
    "    idx = book.index.tolist()[::seconds_sample_interval]\n",
    "    book = book.iloc[idx,:]\n",
    "    # select only relevant columns\n",
    "    book['stock_id'] = stock_id\n",
    "    if delete_unused_cols==True:\n",
    "        book = book[['stock_id','time_id','seconds_in_bucket','wap1_sma50','wap1_sms50','total_volume_sma60','volume_imbalance_sma80']]\n",
    "    return book\n",
    "\n",
    "def tagging_cluster_all_stocks(n_stock, train_test, n_time_id_sample, seconds_sample_interval, remove_first_n_seconds, delete_unused_cols):\n",
    "    stock_list = sorted([int(x.split('=')[1]) for x in os.listdir(f'/kaggle/input/{data_path}/book_{train_test}.parquet')])[:n_stock]\n",
    "    all_stocks_tagging = []\n",
    "    for stock_id in stock_list:\n",
    "        ts = preprocess_single_stock(stock_id=stock_id, train_test=train_test, n_time_id_sample=n_time_id_sample, \n",
    "                                     seconds_sample_interval=seconds_sample_interval, remove_first_n_seconds=remove_first_n_seconds, \n",
    "                                     delete_unused_cols=delete_unused_cols)\n",
    "        tagging = ts[['stock_id','time_id']].drop_duplicates()\n",
    "        path = '/kaggle/input/volatility-ts-clustering-models-v1' # define KMeans model path and names\n",
    "        clust_models = ['clust_wap1_sma50','clust_wap1_sms50','clust_total_volume_sma60','clust_volume_imbalance_sma80']\n",
    "        for name in clust_models[:4]: # loop through models to perform tagging\n",
    "            model = TimeSeriesKMeans.from_pickle(os.path.join(path, f'{name}.p'))\n",
    "            tagging[name] = model.predict(dataframe_to_ts(data=ts, series=name.replace('clust_','')))\n",
    "            tagging[name] = tagging[name].astype(int)\n",
    "        all_stocks_tagging.append(tagging)\n",
    "        print(f'[{datetime.datetime.now()}] Completed {train_test} data tagging for stock {stock_id}')\n",
    "    all_stocks_tagging = pd.concat(all_stocks_tagging, axis=0).reset_index(drop=True)\n",
    "    return all_stocks_tagging\n",
    "\n",
    "def dataframe_to_ts(data, series):\n",
    "    ts = [data[(data.stock_id==stock_id) & (data.time_id==time_id) & (data[series].isnull()==False)][series].tolist() for stock_id, time_id in data[['stock_id','time_id']].drop_duplicates().to_records(index=False).tolist()]\n",
    "    ts = to_time_series_dataset(ts)\n",
    "    return ts\n",
    "\n",
    "# create the new features\n",
    "# tagging_train = gen_ts_clustering_feats(train_test='train', n_stock=N_STOCK)\n",
    "if TRAIN==True:\n",
    "    tagging_train = pd.read_csv('/kaggle/input/stock-cluster-tagging-train/tagging_train.csv') # pre-computed for training set\n",
    "    train = pd.merge(train, tagging_train, how='left', on=['stock_id','time_id'])\n",
    "    cols = [col for col in train.columns if col[:6]=='clust_']\n",
    "    train[cols] = train[cols].fillna(0)\n",
    "\n",
    "tagging_test = tagging_cluster_all_stocks(n_stock=N_STOCK, train_test='test', n_time_id_sample=-1, seconds_sample_interval=10, remove_first_n_seconds=10, delete_unused_cols=True)\n",
    "test = pd.merge(test, tagging_test, how='left', on=['stock_id','time_id'])\n",
    "cols = [col for col in test.columns if col[:6]=='clust_']\n",
    "test[cols] = test[cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ffd999",
   "metadata": {
    "_cell_guid": "a6af1aaa-7528-4cee-9c82-d2620b9984e8",
    "_uuid": "45df3194-a8be-418b-b2de-eb04426a1e74",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:17:34.889155Z",
     "iopub.status.busy": "2021-09-27T05:17:34.888450Z",
     "iopub.status.idle": "2021-09-27T05:17:42.651528Z",
     "shell.execute_reply": "2021-09-27T05:17:42.650806Z",
     "shell.execute_reply.started": "2021-09-27T03:42:20.865197Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 7.812141,
     "end_time": "2021-09-27T05:17:42.651677",
     "exception": false,
     "start_time": "2021-09-27T05:17:34.839536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    33\n",
      "2    30\n",
      "5    27\n",
      "1    13\n",
      "0     8\n",
      "4     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Add Stock Clustering as categorical feature\n",
    "'''\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# prepare data for clustering\n",
    "data = pd.pivot_table(train, values='target', index='time_id', columns='stock_id', aggfunc=np.sum)\n",
    "data = data.fillna(data.mean()).T\n",
    "# model fitting\n",
    "model = AgglomerativeClustering(n_clusters=6, linkage='ward')\n",
    "label = model.fit_predict(data)\n",
    "print(pd.Series(label).value_counts())\n",
    "# create stock-label mapping table\n",
    "data['stock_clustering_label'] = label\n",
    "data = data.reset_index().rename(columns={'index':'stock_id'})\n",
    "data = data[['stock_id','stock_clustering_label']]\n",
    "# add label to train / test\n",
    "if TRAIN==True:\n",
    "    train = pd.merge(train, data, how='left', on='stock_id')\n",
    "test = pd.merge(test, data, how='left', on='stock_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f867653d",
   "metadata": {
    "_cell_guid": "91cca131-ad03-4e7e-bbab-37083764b7b3",
    "_uuid": "01cf6f4d-cf36-4df2-bd65-f981e9476ae8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:17:42.731732Z",
     "iopub.status.busy": "2021-09-27T05:17:42.730962Z",
     "iopub.status.idle": "2021-09-27T05:17:52.242779Z",
     "shell.execute_reply": "2021-09-27T05:17:52.241716Z",
     "shell.execute_reply.started": "2021-09-27T03:42:21.141457Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.557952,
     "end_time": "2021-09-27T05:17:52.242931",
     "exception": false,
     "start_time": "2021-09-27T05:17:42.684979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Add Time Clustering as categorical feature\n",
    "'''\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "feature_list = ['wap1_sum','log_return1_realized_volatility','log_return1_realized_volatility_200','log_return1_realized_volatility_400','total_volume_sum','volume_imbalance_sum','trade_size_sum','trade_order_count_sum','trade_log_return_realized_volatility']\n",
    "log_feats = ['log_return1_realized_volatility','log_return1_realized_volatility_200','log_return1_realized_volatility_400','total_volume_sum','trade_size_sum','trade_order_count_sum','trade_log_return_realized_volatility']\n",
    "\n",
    "if TRAIN==True:\n",
    "    # prepare data for clustering\n",
    "    train_time_feature = train.groupby('time_id')[feature_list].mean().reset_index()\n",
    "    train_time_feature[log_feats] = np.log(train_time_feature[log_feats])\n",
    "    train_time_feature = fill_inf_nan(train_time_feature)\n",
    "    # kmeans clustering\n",
    "    kmeans = KMeans(n_clusters=8, random_state=SEED).fit(train_time_feature[feature_list])\n",
    "    # transformation for train\n",
    "    train_time_feature['time_clustering_label'] = kmeans.predict(train_time_feature[feature_list])\n",
    "    train_time_feature = train_time_feature[['time_id','time_clustering_label']]\n",
    "    train = pd.merge(train, train_time_feature, how='left', on='time_id')\n",
    "    # transformation for test\n",
    "    test_time_feature = test.groupby('time_id')[feature_list].mean().reset_index()\n",
    "    test_time_feature[log_feats] = np.log(test_time_feature[log_feats])\n",
    "    test_time_feature = fill_inf_nan(test_time_feature)\n",
    "    test_time_feature['time_clustering_label'] = kmeans.predict(test_time_feature[feature_list])\n",
    "    test_time_feature = test_time_feature[['time_id','time_clustering_label']]\n",
    "    test = pd.merge(test, test_time_feature, how='left', on='time_id')\n",
    "    # save model\n",
    "    pickle.dump(kmeans, open(f'time_clustering_kmeans_model.p', 'wb'))\n",
    "else:\n",
    "    # transformation for test\n",
    "    test_time_feature = test.groupby('time_id')[feature_list].mean().reset_index()\n",
    "    test_time_feature[log_feats] = np.log(test_time_feature[log_feats])\n",
    "    test_time_feature = fill_inf_nan(test_time_feature)\n",
    "    kmeans = pickle.load(open(os.path.join(FE_PATH, f'time_clustering_kmeans_model.p'), 'rb'))\n",
    "    test_time_feature['time_clustering_label'] = kmeans.predict(test_time_feature[feature_list])\n",
    "    test_time_feature = test_time_feature[['time_id','time_clustering_label']]\n",
    "    test = pd.merge(test, test_time_feature, how='left', on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff53f2a",
   "metadata": {
    "_cell_guid": "597184ab-5515-4b33-90b6-455e663e18ea",
    "_uuid": "58d4c0dc-e72e-4f99-8d7d-7fa164f3a9ea",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:17:52.327477Z",
     "iopub.status.busy": "2021-09-27T05:17:52.326737Z",
     "iopub.status.idle": "2021-09-27T05:18:13.426183Z",
     "shell.execute_reply": "2021-09-27T05:18:13.426739Z",
     "shell.execute_reply.started": "2021-09-27T03:42:21.178283Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 21.15246,
     "end_time": "2021-09-27T05:18:13.426917",
     "exception": false,
     "start_time": "2021-09-27T05:17:52.274457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-09-27 05:17:57.394309] Processed 0 stock_ids\n",
      "[2021-09-27 05:17:57.395068] Completed data preprocessing\n",
      "[2021-09-27 05:17:57.407340] Scaled bid_price1\n",
      "[2021-09-27 05:17:57.410803] Scaled ask_price1\n",
      "[2021-09-27 05:17:57.414088] Scaled bid_price2\n",
      "[2021-09-27 05:17:57.417365] Scaled ask_price2\n",
      "[2021-09-27 05:17:57.420620] Scaled bid_size1\n",
      "[2021-09-27 05:17:57.423847] Scaled ask_size1\n",
      "[2021-09-27 05:17:57.427062] Scaled bid_size2\n",
      "[2021-09-27 05:17:57.430265] Scaled ask_size2\n",
      "[2021-09-27 05:17:57.433454] Scaled price\n",
      "[2021-09-27 05:17:57.436655] Scaled size\n",
      "[2021-09-27 05:17:57.439891] Scaled order_count\n",
      "[2021-09-27 05:17:57.439962] Completed data scaling\n",
      "Memory usage after optimization is: 0.00 MB\n",
      "Decreased by 41.4%\n",
      "[2021-09-27 05:17:57.450609] Completed memory reduction\n",
      "[2021-09-27 05:17:57.450716] Completed data preprocessing\n",
      "Shape of data_test is (1, 55, 8)\n",
      "Shape of train encoding is (428932, 34)\n",
      "Shape of test encoding is (1, 34)\n",
      "CPU times: user 13.8 s, sys: 1.13 s, total: 15 s\n",
      "Wall time: 21.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "698"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "Time-Series LSTM AutoEncoding features\n",
    "'''\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# function to load a book/trade train/test single stock file\n",
    "def load_single_stock(stock_id, train_test, book_trade):\n",
    "    path = f'/kaggle/input/{data_path}/{book_trade}_{train_test}.parquet/stock_id={str(stock_id)}'\n",
    "    filename = os.path.join(path, os.listdir(path)[0])\n",
    "    return pd.read_parquet(filename)\n",
    "\n",
    "def preprocess_single_stock(stock_id, train_test, n_time_id_sample, seconds_sample_interval):\n",
    "    # load data\n",
    "    book = load_single_stock(stock_id, train_test, 'book')\n",
    "    trade = load_single_stock(stock_id, train_test, 'trade')\n",
    "    # ffill and bfill\n",
    "    time_id_list =  pd.DataFrame({'time_id':book.time_id.unique().tolist()})\n",
    "    seconds_in_bucket_list = pd.DataFrame({'seconds_in_bucket':range(600)})\n",
    "    base = time_id_list.merge(seconds_in_bucket_list, how='cross')\n",
    "    book = base.merge(book, how='left', on=['time_id','seconds_in_bucket'])\n",
    "    book = book.ffill().bfill()\n",
    "    trade = base.merge(trade, how='left', on=['time_id','seconds_in_bucket'])\n",
    "    trade[['price']] = trade[['price']].ffill().bfill()\n",
    "    trade[['size','order_count']] = trade[['size','order_count']].fillna(0)\n",
    "    # joining book and trade\n",
    "    df = pd.merge(book, trade, how='inner', on=['time_id','seconds_in_bucket'])\n",
    "    # sampling time_id\n",
    "    if n_time_id_sample >= 0:\n",
    "        time_id_samples = random.sample(df['time_id'].unique().tolist(), n_time_id_sample)\n",
    "        df = df[df.time_id.isin(time_id_samples)]\n",
    "    # smoothing all time series\n",
    "    ts = [c for c in df if c not in ['time_id','seconds_in_bucket']]\n",
    "    for c in ts:\n",
    "        df[c] = df.groupby('time_id')[c].apply(lambda s: s.rolling(window=50).mean())\n",
    "    # seconds interval filtering\n",
    "    idx = df.index.tolist()[::seconds_sample_interval]\n",
    "    df = df.iloc[idx,:]\n",
    "    # remove NA entries due to moving average\n",
    "    df = df[df.isnull().sum(axis=1)==0].reset_index(drop=True)\n",
    "    # select only relevant columns\n",
    "    df['stock_id'] = stock_id\n",
    "    df = df[['stock_id'] + [c for c in df if c not in ['stock_id']]]\n",
    "    df = df.drop('seconds_in_bucket', axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_all_stocks(n_stock, train_test, n_time_id_sample, seconds_sample_interval, reduce_mem):\n",
    "    stock_list = sorted([int(x.split('=')[1]) for x in os.listdir(f'/kaggle/input/{data_path}/book_{train_test}.parquet')])\n",
    "    stock_list = stock_list[:n_stock]\n",
    "    df = []\n",
    "    for stock_id in stock_list:\n",
    "        df.append(preprocess_single_stock(stock_id, train_test, n_time_id_sample, seconds_sample_interval))\n",
    "        if stock_list.index(stock_id) % 10 == 0:\n",
    "            print(f'[{datetime.datetime.now()}] Processed {stock_list.index(stock_id) % 10} stock_ids')\n",
    "    df = pd.concat(df, axis=0).reset_index(drop=True)\n",
    "    print(f'[{datetime.datetime.now()}] Completed data preprocessing')\n",
    "    \n",
    "    # scaling\n",
    "    num_feats = [c for c in df if c not in ['stock_id','time_id']]\n",
    "    scalers = pickle.load(open(os.path.join('/kaggle/input/volatility-time-series-autoencoder-data', 'ts_ae_stdscalers.p'), 'rb'))\n",
    "    for c in num_feats:\n",
    "        df[[c]] = scalers[c].transform(df[[c]])\n",
    "        print(f'[{datetime.datetime.now()}] Scaled {c}')\n",
    "    print(f'[{datetime.datetime.now()}] Completed data scaling')\n",
    "    \n",
    "    # memory reduction\n",
    "    if reduce_mem==True:\n",
    "        df = reduce_mem_usage(df)\n",
    "        print(f'[{datetime.datetime.now()}] Completed memory reduction')\n",
    "    return df\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:5] == 'float':\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# create dataset\n",
    "data_test = preprocess_all_stocks(n_stock=N_STOCK, train_test='test', n_time_id_sample=-1, seconds_sample_interval=10, reduce_mem=True)\n",
    "print(f'[{datetime.datetime.now()}] Completed data preprocessing')\n",
    "\n",
    "# remove price\n",
    "# data_train = data_train.drop(['ask_price1','bid_price2','ask_price2'], axis=1)\n",
    "data_test = data_test.drop(['ask_price1','bid_price2','ask_price2'], axis=1)\n",
    "\n",
    "# convert to np array\n",
    "data_test_keys = data_test[['stock_id','time_id']].drop_duplicates()\n",
    "n_split = data_test_keys.shape[0]\n",
    "data_test = np.array(np.split(data_test.iloc[:,2:].values, n_split))\n",
    "print(f'Shape of data_test is {data_test.shape}')\n",
    "\n",
    "# load pre-trained encoder\n",
    "path = os.path.join('/kaggle/input/volatility-ts-encoding-version-5', 'TimeSeries_Encoder')\n",
    "ts_encoder = tf.keras.models.load_model(path)\n",
    "encoding_dim = 32\n",
    "\n",
    "# encoding\n",
    "if TRAIN==True:\n",
    "    data_train_keys = pd.read_csv('/kaggle/input/volatility-ts-encoding-version-5/train_encoded.csv')\n",
    "    print(f'Shape of train encoding is {data_train_keys.shape}')\n",
    "    train = pd.merge(train, data_train_keys, how='left', on=['stock_id','time_id'])\n",
    "    del data_train_keys\n",
    "    gc.collect()\n",
    "data_test_keys[[f'ts_ae{i}' for i in range(encoding_dim)]] = ts_encoder.predict(data_test)\n",
    "print(f'Shape of test encoding is {data_test_keys.shape}')\n",
    "test = pd.merge(test, data_test_keys, how='left', on=['stock_id','time_id'])\n",
    "del data_test, data_test_keys\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dbbbd9c",
   "metadata": {
    "_cell_guid": "65335aef-4c52-4051-a8e5-63f598999414",
    "_uuid": "0777007a-fc73-45a8-a2aa-6b8347a0c2a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:18:13.507407Z",
     "iopub.status.busy": "2021-09-27T05:18:13.506460Z",
     "iopub.status.idle": "2021-09-27T05:18:14.124942Z",
     "shell.execute_reply": "2021-09-27T05:18:14.125443Z",
     "shell.execute_reply.started": "2021-09-27T03:42:27.856980Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.665701,
     "end_time": "2021-09-27T05:18:14.125687",
     "exception": false,
     "start_time": "2021-09-27T05:18:13.459986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create \"tau\" features\n",
    "if TRAIN==True:\n",
    "    train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "    train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "    train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "    train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "    train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "    train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "    cols = [col for col in train.columns if 'tau' in col]\n",
    "    train[cols] = train[cols].replace([np.inf, -np.inf], np.nan)\n",
    "    train[cols] = train[cols].fillna(train[cols].mean())\n",
    "\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n",
    "cols = [col for col in test.columns if 'tau' in col]\n",
    "test[cols] = test[cols].replace([np.inf, -np.inf], np.nan)\n",
    "test[cols] = test[cols].fillna(test[cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8192c06f",
   "metadata": {
    "_cell_guid": "77f7b76e-5c48-481d-a994-698b40655f5a",
    "_uuid": "b7ede51b-33dd-4e0c-8430-b4fc90747459",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:18:14.196051Z",
     "iopub.status.busy": "2021-09-27T05:18:14.195384Z",
     "iopub.status.idle": "2021-09-27T05:19:17.963377Z",
     "shell.execute_reply": "2021-09-27T05:19:17.962757Z",
     "shell.execute_reply.started": "2021-09-27T03:42:27.891407Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 63.80462,
     "end_time": "2021-09-27T05:19:17.963531",
     "exception": false,
     "start_time": "2021-09-27T05:18:14.158911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 32, 0: 31, 4: 19, 2: 19, 6: 8, 1: 2, 5: 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:43: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Stock cluster features\n",
    "For each time_id, statistics for each of 5 clusters (for only that time_id) will be added\n",
    "'''\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "# clustering based on target correlation matrix\n",
    "n_clust = 7\n",
    "train_p = pd.read_csv(f'../input/{data_path}/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "corr = train_p.corr()\n",
    "kmeans = KMeans(n_clusters=n_clust, random_state=SEED).fit(corr.values)\n",
    "print(Counter(kmeans.labels_))\n",
    "\n",
    "# output the list of stocks for each cluster\n",
    "stock_groups = []\n",
    "ids = corr.index\n",
    "for n in range(n_clust):\n",
    "    stock_groups.append([(x-1) for x in ((ids+1)*(kmeans.labels_==n)) if x > 0])\n",
    "\n",
    "def gen_clust_feats(stock_groups, train, test):\n",
    "    df_train = []\n",
    "    df_test = []\n",
    "    for i in range(len(stock_groups)):\n",
    "        group = stock_groups[i]\n",
    "        newDf = train.loc[train['stock_id'].isin(group)]\n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = f'c{i}'\n",
    "        df_train.append(newDf)\n",
    "\n",
    "        newDf = test.loc[test['stock_id'].isin(group)]    \n",
    "        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "        newDf.loc[:,'stock_id'] = f'c{i}'\n",
    "        df_test.append(newDf)\n",
    "\n",
    "    df_train = pd.concat(df_train).reset_index()\n",
    "    df_train = df_train[[c for c in df_train if c!='target']]\n",
    "    df_test = pd.concat(df_test).reset_index()\n",
    "\n",
    "    df_test = pd.concat([df_test, df_train[(~df_train.stock_id.isin(df_test.stock_id)) & (df_train.time_id==5)]]) # suspicious; probably this is to ensure that test data contains all stocks; it works on the assumption that time_id 5 is not required for submission\n",
    "    df_train = df_train.pivot(index='time_id', columns='stock_id')\n",
    "    df_train.columns = [\"_\".join(x) for x in df_train.columns.ravel()]\n",
    "    df_train.reset_index(inplace=True)\n",
    "\n",
    "    df_test = df_test.pivot(index='time_id', columns='stock_id')\n",
    "    df_test.columns = [\"_\".join(x) for x in df_test.columns.ravel()]\n",
    "    df_test.reset_index(inplace=True)\n",
    "\n",
    "    # add to base features\n",
    "    target_cols = ['time_id'] + [f'{col}_c{i}' for i in [0,2,3,4,6] for col in ['log_return1_realized_volatility','total_volume_sum','trade_size_sum','trade_order_count_sum',\n",
    "                                                                                'price_spread_sum','bid_spread_sum','ask_spread_sum','volume_imbalance_sum','bid_ask_spread_sum','size_tau2']]\n",
    "    train = pd.merge(train, df_train[target_cols], how='left', on='time_id')\n",
    "    test = pd.merge(test, df_test[target_cols], how='left', on='time_id')\n",
    "\n",
    "    # free memory\n",
    "    import gc\n",
    "    del df_train, df_test\n",
    "    gc.collect()\n",
    "    return train, test\n",
    "\n",
    "if TRAIN==True:\n",
    "    train, test = gen_clust_feats(stock_groups=stock_groups, train=train[[c for c in train if c[-3:-1]!='_c']], test=test)\n",
    "else:\n",
    "    _ , test = gen_clust_feats(stock_groups=stock_groups, train=train, test=test)\n",
    "    \n",
    "train = train.fillna(train.mean()).fillna(0)\n",
    "test = test.fillna(test.mean()).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403c62d2",
   "metadata": {
    "_cell_guid": "94265423-741a-4d8a-af5a-ec41936fc8ed",
    "_uuid": "31173b79-98d3-4143-8657-e76713aeec2b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:19:18.241264Z",
     "iopub.status.busy": "2021-09-27T05:19:18.240313Z",
     "iopub.status.idle": "2021-09-27T05:20:28.537654Z",
     "shell.execute_reply": "2021-09-27T05:20:28.538380Z",
     "shell.execute_reply.started": "2021-09-27T03:43:34.072708Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 70.540834,
     "end_time": "2021-09-27T05:20:28.538633",
     "exception": false,
     "start_time": "2021-09-27T05:19:17.997799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feats is 245\n",
      "Removed feats is 51\n"
     ]
    }
   ],
   "source": [
    "# remove highly correlated features\n",
    "if TRAIN==True:\n",
    "    corr = train[[c for c in train if c[-3:-1]!='_c' and \n",
    "                  c[:6]!='clust_' and \n",
    "                  'clustering_label' not in c and \n",
    "                  c not in ['stock_id','time_id','row_id','target']]].corr()\n",
    "    all_features = corr.columns.tolist()\n",
    "    print(f'Original feats is {len(all_features)}')\n",
    "    correlated_features = set()\n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(i):\n",
    "            if corr.iloc[i, j] >= 0.99:\n",
    "                correlated_features.add(corr.columns[i])\n",
    "    print(f'Removed feats is {len(correlated_features)}')\n",
    "    train = train.drop(correlated_features, axis=1)\n",
    "    pickle.dump(correlated_features, open(f'correlated_features.p', 'wb'))\n",
    "else:\n",
    "    correlated_features = pickle.load(open(os.path.join(FE_PATH, f'correlated_features.p'), 'rb'))\n",
    "    \n",
    "test = test.drop(correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524dba28",
   "metadata": {
    "_cell_guid": "ffda9df2-bbb4-4a5e-a7d8-faf12211251b",
    "_uuid": "9eec675e-e2b0-4f30-8483-77ff8b900d33",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:20:28.617075Z",
     "iopub.status.busy": "2021-09-27T05:20:28.615056Z",
     "iopub.status.idle": "2021-09-27T05:21:19.810006Z",
     "shell.execute_reply": "2021-09-27T05:21:19.809372Z",
     "shell.execute_reply.started": "2021-09-27T03:43:34.080486Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 51.236301,
     "end_time": "2021-09-27T05:21:19.810215",
     "exception": false,
     "start_time": "2021-09-27T05:20:28.573914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final fillna\n",
    "train = fill_inf_nan(train)\n",
    "test = fill_inf_nan(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18476db2",
   "metadata": {
    "_cell_guid": "3746f6f3-8686-4ca4-a950-ba626220f2ca",
    "_uuid": "52320cfd-80cc-4c80-939f-3c3c258244e1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:21:19.885654Z",
     "iopub.status.busy": "2021-09-27T05:21:19.884921Z",
     "iopub.status.idle": "2021-09-27T05:21:19.888949Z",
     "shell.execute_reply": "2021-09-27T05:21:19.888272Z",
     "shell.execute_reply.started": "2021-09-27T03:44:24.173566Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.04413,
     "end_time": "2021-09-27T05:21:19.889089",
     "exception": false,
     "start_time": "2021-09-27T05:21:19.844959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features is 250\n"
     ]
    }
   ],
   "source": [
    "# Print number of features\n",
    "colNames = [col for col in list(train.columns) if col not in [\"stock_id\", \"time_id\", \"target\", \"row_id\"]]\n",
    "print(f'Number of features is {len(colNames)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a064224",
   "metadata": {
    "_cell_guid": "af87217e-32a5-4f15-9e81-d5e6b7838554",
    "_uuid": "df19bbb6-b5d2-4732-9b51-523f5ea1bc47",
    "papermill": {
     "duration": 0.035357,
     "end_time": "2021-09-27T05:21:19.959507",
     "exception": false,
     "start_time": "2021-09-27T05:21:19.924150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### NN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f9c5340",
   "metadata": {
    "_cell_guid": "325f53ac-ca38-4f77-b1f8-b3efd75beb00",
    "_uuid": "723785b8-b7d3-4824-9254-8e3ef47a1566",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:21:20.209022Z",
     "iopub.status.busy": "2021-09-27T05:21:20.038238Z",
     "iopub.status.idle": "2021-09-27T05:23:44.511737Z",
     "shell.execute_reply": "2021-09-27T05:23:44.510939Z",
     "shell.execute_reply.started": "2021-09-27T03:44:24.181129Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 144.517166,
     "end_time": "2021-09-27T05:23:44.511918",
     "exception": false,
     "start_time": "2021-09-27T05:21:19.994752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:43: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:47: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n"
     ]
    }
   ],
   "source": [
    "# prepare NN dataset\n",
    "\n",
    "# features that requires quantile transformation for NN model\n",
    "nn_qt_feats = [col for col in train.columns if \\\n",
    "               col[-3:-1]!='_c' and \\\n",
    "               col[:6]!='clust_' and \\\n",
    "               'clustering_label' not in col and \\\n",
    "               col not in ['stock_id','time_id','row_id','target']]\n",
    "\n",
    "if TRAIN==True:\n",
    "    train_nn=train[nn_qt_feats].copy()\n",
    "else:\n",
    "    train_nn = pd.read_feather(os.path.join(FE_PATH, 'train_nn.f'))\n",
    "test_nn=test[nn_qt_feats].copy()\n",
    "\n",
    "# quantile transformation\n",
    "if TRAIN==True:\n",
    "    qt = QuantileTransformer(random_state=SEED, n_quantiles=2000, output_distribution='normal')\n",
    "    qt.fit(train_nn[nn_qt_feats])\n",
    "    train_nn[nn_qt_feats] = qt.transform(train_nn[nn_qt_feats])\n",
    "    test_nn[nn_qt_feats] = qt.transform(test_nn[nn_qt_feats])\n",
    "    pickle.dump(qt, open(f'quantile_transformer.p', 'wb'))\n",
    "else:\n",
    "    qt = pickle.load(open(os.path.join(FE_PATH, 'quantile_transformer.p'), 'rb'))\n",
    "    test_nn[nn_qt_feats] = qt.transform(test_nn[nn_qt_feats])\n",
    "\n",
    "# reset key columns\n",
    "train_nn[['stock_id','time_id','row_id','target']] = train[['stock_id','time_id','row_id','target']].reset_index(drop=True)\n",
    "test_nn[['stock_id','time_id','row_id']] = test[['stock_id','time_id','row_id']].reset_index(drop=True)\n",
    "\n",
    "# add back stock and time clustering labels\n",
    "stock_time_clust_label_feats = [c for c in train if 'clustering_label' in c]\n",
    "train_nn[stock_time_clust_label_feats] = train[stock_time_clust_label_feats]\n",
    "test_nn[stock_time_clust_label_feats] = test[stock_time_clust_label_feats]\n",
    "\n",
    "# generate stock cluster features\n",
    "if TRAIN==True:\n",
    "    train_nn, test_nn = gen_clust_feats(stock_groups=stock_groups, train=train_nn, test=test_nn)\n",
    "else:\n",
    "    _ , test_nn = gen_clust_feats(stock_groups=stock_groups, train=train_nn, test=test_nn)\n",
    "\n",
    "\n",
    "# fillna with mean\n",
    "train_nn = fill_inf_nan(train_nn)\n",
    "test_nn = fill_inf_nan(test_nn)\n",
    "\n",
    "# stock_id label encoding (required for NN embedding)\n",
    "nn_cat_cols = ['stock_id'] + stock_time_clust_label_feats\n",
    "for col in nn_cat_cols:\n",
    "    if TRAIN==True:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train_nn[col].values)\n",
    "        train_nn[col] = encoder.transform(train_nn[col].values)\n",
    "        test_nn[col] = encoder.transform(test_nn[col].values)\n",
    "        pickle.dump(encoder, open(f'nn_label_encoder_{col}.p', 'wb'))\n",
    "    else:\n",
    "        encoder = pickle.load(open(os.path.join(FE_PATH, f'nn_label_encoder_{col}.p'), 'rb'))\n",
    "        test_nn[col] = encoder.transform(test_nn[col].values)\n",
    "        \n",
    "# min max scaling\n",
    "nn_num_feats = [c for c in train_nn if c not in ['stock_id','time_id','row_id','target'] and 'clustering_label' not in c]\n",
    "if TRAIN==True:\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(train_nn[nn_num_feats])\n",
    "    train_nn[nn_num_feats] = scaler.transform(train_nn[nn_num_feats])\n",
    "    pickle.dump(scaler, open(f'nn_min_max_scaler.p', 'wb'))\n",
    "    test_nn[nn_num_feats] = scaler.transform(test_nn[nn_num_feats])\n",
    "else:\n",
    "    scaler = pickle.load(open(os.path.join(FE_PATH, 'nn_min_max_scaler.p'), 'rb'))\n",
    "    test_nn[nn_num_feats] = scaler.transform(test_nn[nn_num_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68b5ad",
   "metadata": {
    "_cell_guid": "c8bffefc-b828-4623-83eb-2df95fc2566b",
    "_uuid": "7954affd-6fd3-4dd6-ad12-7e507c30187f",
    "papermill": {
     "duration": 0.03439,
     "end_time": "2021-09-27T05:23:44.582025",
     "exception": false,
     "start_time": "2021-09-27T05:23:44.547635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TabNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5577a66",
   "metadata": {
    "_cell_guid": "37dddb3e-cb40-4856-99e6-0a06de81ee16",
    "_uuid": "50a0ce96-28f1-4034-8072-88adcbaa7de9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:23:44.662670Z",
     "iopub.status.busy": "2021-09-27T05:23:44.661956Z",
     "iopub.status.idle": "2021-09-27T05:24:17.122333Z",
     "shell.execute_reply": "2021-09-27T05:24:17.119813Z",
     "shell.execute_reply.started": "2021-09-27T03:45:21.722907Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 32.506044,
     "end_time": "2021-09-27T05:24:17.122693",
     "exception": false,
     "start_time": "2021-09-27T05:23:44.616649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define dataset for TabNet\n",
    "train_tbn = train_nn.copy()\n",
    "test_tbn = test_nn.copy()\n",
    "\n",
    "# identify categorical and numerical columns\n",
    "cat_cols = ['stock_id'] + stock_time_clust_label_feats\n",
    "num_cols = [c for c in train_tbn if c not in ['stock_id','time_id','row_id','target'] and c not in stock_time_clust_label_feats]\n",
    "\n",
    "# label encoding catergorical cols\n",
    "for col in cat_cols:\n",
    "    if TRAIN==True:\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(train_tbn[col].values)\n",
    "        train_tbn[col] = encoder.transform(train_tbn[col].values)\n",
    "        test_tbn[col] = encoder.transform(test_tbn[col].values)\n",
    "        pickle.dump(encoder, open(f'tabnet_label_encoder_{col}.p', 'wb'))\n",
    "    else:\n",
    "        encoder = pickle.load(open(os.path.join(FE_PATH, f'tabnet_label_encoder_{col}.p'), 'rb'))\n",
    "        test_tbn[col] = encoder.transform(test_tbn[col].values)\n",
    "    \n",
    "# standard scaling numerical cols\n",
    "if TRAIN==True:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_tbn[num_cols])\n",
    "    train_tbn[num_cols] = scaler.transform(train_tbn[num_cols])\n",
    "    test_tbn[num_cols] = scaler.transform(test_tbn[num_cols])\n",
    "    pickle.dump(scaler, open(f'tabnet_std_scaler.p', 'wb'))\n",
    "else:\n",
    "    scaler = pickle.load(open(os.path.join(FE_PATH, 'tabnet_std_scaler.p'), 'rb'))\n",
    "    test_tbn[num_cols] = scaler.transform(test_tbn[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d734f553",
   "metadata": {
    "_cell_guid": "a8679aec-1d44-4553-a949-76fac44f9f82",
    "_uuid": "d59a5879-f145-47ce-8123-b6d586e48d24",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:24:17.273542Z",
     "iopub.status.busy": "2021-09-27T05:24:17.271466Z",
     "iopub.status.idle": "2021-09-27T05:24:22.793833Z",
     "shell.execute_reply": "2021-09-27T05:24:22.792907Z",
     "shell.execute_reply.started": "2021-09-27T03:45:22.211651Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.596689,
     "end_time": "2021-09-27T05:24:22.793986",
     "exception": false,
     "start_time": "2021-09-27T05:24:17.197297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "if TRAIN==True:\n",
    "    train.reset_index(drop=True).to_feather('train.f')\n",
    "    test.reset_index(drop=True).to_feather('test.f')\n",
    "    train_nn.reset_index(drop=True).to_feather('train_nn.f')\n",
    "    test_nn.reset_index(drop=True).to_feather('test_nn.f')\n",
    "    train_tbn.reset_index(drop=True).to_feather('train_tbn.f')\n",
    "    test_tbn.reset_index(drop=True).to_feather('test_tbn.f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de77e1b2",
   "metadata": {
    "_cell_guid": "af552b2b-1ddf-444e-bc8c-73b28b0e3c4a",
    "_uuid": "42729a84-fade-4f9b-ab47-63053a2dd9f4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-09-27T05:24:22.873252Z",
     "iopub.status.busy": "2021-09-27T05:24:22.872604Z",
     "iopub.status.idle": "2021-09-27T05:24:22.878958Z",
     "shell.execute_reply": "2021-09-27T05:24:22.879694Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.051196,
     "end_time": "2021-09-27T05:24:22.879930",
     "exception": false,
     "start_time": "2021-09-27T05:24:22.828734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428932, 254)\n",
      "(3, 253)\n",
      "(428932, 250)\n",
      "(3, 249)\n",
      "(428932, 250)\n",
      "(3, 249)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train_nn.shape)\n",
    "print(test_nn.shape)\n",
    "print(train_tbn.shape)\n",
    "print(test_tbn.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5156.349135,
   "end_time": "2021-09-27T05:24:26.011656",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-27T03:58:29.662521",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
